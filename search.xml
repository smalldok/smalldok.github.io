<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[3-策略模式Strategy]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F3-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8FStrategy%2F</url>
    <content type="text"><![CDATA[作用与场景 避免代码臃肿难以阅读，如非常多的if..else，参照单一原则，使每种事情得以抽象出来，代码结构更加清晰（曾经参加面试时，有面试官问，如果if..else过多怎么解决，当时一脸懵逼）； 常用于负载均衡策略的选择、商品税率、商品活动等场景，在很多开源中间件中也都有策略模式的影子（多看源码有好处）； 策略模式的几个角色： 策略接口Strategy 具体策略对象xxxxStrategy 执行策略对象invoker，内部持有一个策略类的引用 客户端对象Client 策略模式关系图 举例：负载均衡策略的选择，包括随机、轮询、权重等 策略接口Strategy123public class Strategy&#123; Server select(List&lt;Server&gt; serverList);&#125; 具体策略对象xxxxStrategy1234567891011/*** 随机策略*/public class RandomStrategy implements Strategy&#123; @Override public Server select(List&lt;Server&gt; serverList)&#123; //TODO随机算法实现 //Server server= ... return server; &#125;&#125; 1234567891011/*** 轮询策略*/public class RoundRobinStrategy implements Strategy&#123; @Override public Server select(List&lt;Server&gt; serverList)&#123; //TODO轮询算法实现 //Server server= ... return server; &#125;&#125; 执行策略对象invoker12345678910111213public class ServerDiscovery&#123; private Strategy strategy; public ServerDiscovery(Strategy strategy)&#123; this.strategy=strategy; &#125; public String discovery(String serverName)&#123; //获取所有服务节点 List&lt;Server&gt; serverList= zkClient.lookup(serverName); //利用策略，选择一台节点 Server server=strategy.select(serverList); return server.host+":"+server.port; &#125;&#125; 客户端对象Client12345678public class Client&#123; static void main(String[] args) &#123; //使用随机策略选择一台节点 ServerDiscovery sd=new ServerDiscovery(new RandomStrategy()); String hostAndPort=sd.discovery("SERVER_NAME"); System.out.println(hostAndPort); &#125;&#125; 总结：这里只是抛砖引玉，更好的实现见spring cloud中的Ribbon负载均衡策略； 优点： 避免代码臃肿难以阅读，如非常多的if..else，参照单一原则，使每种事情得以抽象出来，代码结构更加清晰； 缺点： 客户端必须知道所有的策略类，并自行决定使用哪一个策略类。这就意味着客户端必须理解这些算法的区别，以便适时选择恰当的算法类。换言之，策略模式只适用于客户端知道算法或行为的情况。 由于策略模式把每个具体的策略实现都单独封装成为类，如果备选的策略很多的话，那么对象的数目就会很多(说缺点有些牵强，能多出来多少呢！)； 设计模式系列文章: 1-命令模式Command 2-责任链模式Filter 3-策略模式Strategy 写得不好，仅供参考！]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2-责任链模式Filter]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F2-%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8FFilter%2F</url>
    <content type="text"><![CDATA[作用与场景过滤或者拦截数据；servlet中的filter、mvc中的拦截器、dubbo中的filter、消费者对重复消息/垃圾消息的过滤等 责任链模式的几个角色 入口数据对象inputObj：传入到责任链中流转的数据 出口数据对象outputObj：责任链处理完之后的对象 责任链对象xxxxFilter：不同的责任链对象，负责不同的责任 责任链对象集合FilterChain：存储所有责任链对象，存储数据处理对象 客户端对象Client：执行具体的责任链 责任链模式关系图 举例：MQ消费者对重复消息/垃圾消息的过滤(其实是拦截器)；正常流程：如果消息没被过滤掉，则做保存处理；异常流程：如果碰到的是重复消息或者垃圾消息，则直接跳过后面的Filter，然后做丢弃处理; 入口数据对象inputObj123456789class Request &#123; private String name = "123"; String getName() &#123; return name; &#125; void setName(String name) &#123; this.name = name; &#125;&#125; 出口数据对象outputObj123456789class Response &#123; private String name; String getName() &#123; return name; &#125; void setName(String name) &#123; this.name = name; &#125;&#125; 责任链对象xxxxFilter12345678910111213141516171819202122232425262728293031323334353637383940/*** Filter接口*/interface Filter&lt;Input,Output&gt; &#123; Output doFilter(Input input, FilterChain filterChain);&#125;/*** 过滤重复消息*/public class RepeatFilter implements Filter&lt;Request,Response&gt;&#123; @Override Response doFilter(Request req, FilterChain filterChain) &#123; //可以附加一些信息 req.setName(req.getName() + "aaa"); //重复消息判定 Boolean isRepeat= xxxService.isRepeat(); if(isRepeat)&#123; return null; &#125; return filterChain.doFilter(req); &#125;&#125;/*** 过滤垃圾消息*/public class RubbishFilter implements Filter&lt;Request,Response&gt;&#123; @Override Response doFilter(Request req, FilterChain filterChain) &#123; //可以附加一些信息 req.setName(req.getName() + "bbb"); //垃圾消息判定 Boolean isRubbish= xxxService.isRubbish(); if(isRubbish)&#123; return null; &#125; return filterChain.doFilter(req); &#125;&#125; 责任链对象集合FilterChain12345678910111213class FilterChain &#123; private static List&lt;Filter&gt; filters = new ArrayList&lt;&gt;(); static void addFilter(Filter filter) &#123; filters.add(filter); &#125; private int pos = 0; Response doFilter(Request request) &#123; if (pos &lt; filters.size()) &#123; Filter filter = filters.get(pos++); return filter.doFilter(request, this); &#125; &#125;&#125; 客户端对象Client12345678910111213public class Client &#123; static void main(String[] args) &#123; FilterChain filterChain = new FilterChain(); FilterChain.addFilter(new RepeatFilter()); FilterChain.addFilter(new RubbishFilter()); Response response = filterChain.doFilter(new Request()); if(response == null)&#123; System.out.println("丢弃消息...."); &#125;else&#123; System.out.println("保存消息...."); &#125; &#125;&#125; 责任链模式总结：优点： 可以扩展多种过滤业务，方便后续维护； 缺点： 责任链太长或者每条链判断处理的时间太长会影响性能； 每次都是从链头开始：这也正是链表的缺点。 设计模式系列文章: 1-命令模式Command 2-责任链模式Filter 3-策略模式Strategy 写得不好，仅供参考！]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1-命令模式Command]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F1-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8FCommand%2F</url>
    <content type="text"><![CDATA[作用与场景 调用者与执行者之间解耦； 用于需要用不同的命令，做不同的业务逻辑处理，如网络通信client与server之间互相传输的不同命令(登录、登出、握手、发消息等，见开源实现SOFA-Bolt)； 命令模式的几个角色 抽象命令接口Command：定义命令的接口，声明执行的方法。 具体的命令对象ConcreteCommand：持有具体的接受者对象，完成具体的具体的命令。 接受者对象Receiver：接受者对象，真正执行命令的对象。 传递命令对象Invoker：持有命令对象，要求命令对象执行请求。 客户端对象Client：创建具体命令的对象并且设置命令对象的接受者。 命令模式关系图 示例：假设我现在拿个玩具遥控器(invoker)，当我按唱歌按钮(ConcreteCommand)，玩具(receiver)就唱歌，当我按跳舞按钮(ConcreteCommand)，玩具(receiver)就跳舞。 抽象命令接口Command：123456789/** * 命令接口 */public interface Command &#123; /** * 执行命令 */ void execute();&#125; 具体的命令对象ConcreteCommand：（唱歌命令）123456789101112131415161718192021222324/** * 唱歌命令 */public class SingCommand implements Command&#123; //玩具接口类 private Toys toys; /** * 创建唱歌命令的时候，传入具体的玩具对象 * @param toys */ public SingCommand(Toys toys) &#123; this.toys = toys; &#125; /** * 调用玩具对象的唱歌接口 */ @Override public void execute() &#123; toys.sing(); &#125;&#125; 具体的命令对象ConcreteCommand：（跳舞命令）123456789101112131415161718192021222324/** * 跳舞命令 */public class DanceCommand implements Command&#123; //玩具接口类 private Toys toys; /** * 创建唱歌命令的时候，传入具体的玩具对象 * @param toys */ public DanceCommand(Toys toys) &#123; this.toys = toys; &#125; /** * 调用玩具对象的跳舞接口 */ @Override public void execute() &#123; toys.dance(); &#125;&#125; 接受者对象Receiver：123456789101112131415161718192021222324252627282930/** * 玩具接口(真正的命令执行者接口) */public interface Toys&#123; void sing(); void dance();&#125;/** * 凹凸曼 */public class Ultraman implements Toys&#123; /** * 唱歌方法 */ @Override public void sing() &#123; System.out.println("泰罗之光，粉碎吧！！"); &#125; /** * 跳舞方法 */ @Override public void dance() &#123; System.out.println("谈恋爱不如跳舞！！"); &#125;&#125; 传递命令对象Invoker：12345678910111213141516171819202122/** * 遥控器 */public class Controller &#123; private Command command; /** * 设置具体的命令 * @param command */ public void setCommand(Command command) &#123; this.command = command; &#125; /** * 执行命令 */ public void doCommand() &#123; command.execute(); &#125;&#125; 客户端对象：12345678910111213141516171819202122232425262728/** * 小孩子 */public class Client &#123; public static void main(String[] args) &#123; // 需要有个遥控器（Invoker） Controller controller = new Controller(); // 需要有个玩具（Receiver） Toys toys = new Ultraman(); // 需要有个唱歌按钮(ConcreteCommand) SingCommand singCommand = new SingCommand(toys); System.out.println("按下唱歌按钮！"); controller.setCommand(singCommand); System.out.println("开始唱歌。。。"); controller.doCommand(); System.out.println("-------------------------------------------------"); // 需要有个跳舞按钮(ConcreteCommand) DanceCommand danceCommand = new DanceCommand(toys); System.out.println("按下跳舞按钮！"); controller.setCommand(danceCommand); System.out.println("开始跳舞。。。"); controller.doCommand(); &#125;&#125; 测试结果：12345按下唱歌按钮！开始唱歌。。。-------------------------------------------------按下跳舞按钮！开始跳舞。。。 总结：上面的例子仅仅是实现单个命令的的命令模式，而命令模式是可以相当复杂的，就比如说，你让机器人同时唱歌、跳舞、画画等等，这时候我们可以将多个命令存储起来，然后一次性执行。 命令模式优缺点：优点： 实现客户端和接受者之间的解耦(由遥控器代执行我的命令) 可以动态的添加新的命令。 (遥控器)只需要调用同一个方法（doCommand方法）便可以实现不同的功能。 缺点： 每一个具体命令都是一个类，可能要创建很多的命令类，注意类的膨胀扩张。 设计模式系列文章: 1-命令模式Command 2-责任链模式Filter 3-策略模式Strategy 写得不好，仅供参考！]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker run --restart and --rm]]></title>
    <url>%2Fdocker%2Fdocker%20run%20--restart%20and%20--rm%2F</url>
    <content type="text"><![CDATA[Docker容器的重启策略Docker容器的重启策略是面向生产环境的一个启动策略，在开发过程中可以忽略该策略。Docker容器的重启都是由Docker守护进程完成的，因此与守护进程息息相关。 Docker容器的重启策略如下： no，默认策略，在容器退出时不重启容器 on-failure，在容器非正常退出时（退出状态非0），才会重启容器on-failure:3，在容器非正常退出时重启容器，最多重启3次 always，在容器退出时总是重启容器 unless-stopped，在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 docker run的–restart选项通过--restart选项，可以设置容器的重启策略，以决定在容器退出时Docker守护进程是否重启刚刚退出的容器。--restart选项通常只用于detached模式的容器。--restart选项不能与--rm选项同时使用。显然，--restart选项适用于detached模式的容器，而--rm选项适用于foreground模式的容器。 在docker ps查看容器时，对于使用了--restart选项的容器，其可能的状态只有Up或Restarting两种状态。 示例： 12docker run -d --restart=always bba-208docker run -d --restart=on-failure:10 bba-208 补充： 查看容器重启次数 1docker inspect -f &quot;&#123;&#123; .RestartCount &#125;&#125;&quot; bba-208 查看容器最后一次的启动时间1docker inspect -f &quot;&#123;&#123; .State.StartedAt &#125;&#125;&quot; bba-208 docker run的–rm选项在Docker容器退出时，默认容器内部的文件系统仍然被保留，以方便调试并保留用户数据。但是，对于foreground容器，由于其只是在开发调试过程中短期运行，其用户数据并无保留的必要，因而可以在容器启动时设置--rm选项，这样在容器退出时就能够自动清理容器内部的文件系统。示例如下：1docker run --rm bba-208 等价于1docker run --rm=true bba-208 显然，--rm选项不能与-d同时使用，即只能自动清理foreground容器，不能自动清理detached容器注意，--rm选项也会清理容器的匿名data volumes。 所以，执行docker run命令带--rm命令选项，等价于在容器退出后，执行docker rm -v。]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker日志收集方案]]></title>
    <url>%2Fdocker%2Fdocker%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[docker 日志收集方案容器日志采集： 基于docker api的日志流采集(stdout)； 日志文件采集（log file） 传统应用日志采集： 日志文件采集 工具选型： 支持文件日志和stdout 只支持文件日志 只支持stdout 工具名称 支持文件日志 支持stdout 使用感受 log-pilot yes yes 不错 filebeat yes no 6.3.0版本总是读不到数据 logspout no yes 他的stdout实现与log-pilot一样,调用的docker api docker log dirver no yes dirver支持none/json-file/syslog/fluentd/.. 日志采集工具要点： 尽量不要影响主机上的应用、占用资源越小越好，不过如果是容器应用的话倒是可以限制其使用的资源； 是否支持灵活的日志标记，这样可以方便后续归类和数据分析，目前看下来log-pilot还不错； 案例： tomcat既有stdout输出，也有文件日志输出(access.log)，这时你会选择哪个采集工具？ java应用既有stdout输出(console)，也有文件日志输出(xxxx.log)，这时你会选择哪个采集工具？ xxx应用，只有文件日志输出(xxxx.log)，这时你会选择哪个采集工具？ log-pilot限定容器资源、用docker-compose启动准备环境 阿里开源：https://github.com/AliyunContainerService/log-pilot log-pilot 具有如下特性： 一个单独的 log 进程收集机器上所有容器的日志。不需要为每个容器启动一个 log 进程。 支持文件日志和 stdout。docker log dirver 亦或 logspout 只能处理 stdout，log-pilot 不仅支持收集 stdout 日志，还可以收集文件日志。 声明式配置。当您的容器有日志要收集，只要通过 label 声明要收集的日志文件的路径，无需改动其他任何配置，log-pilot 就会自动收集新容器的日志。 支持多种日志存储方式。无论是强大的阿里云日志服务，还是比较流行的 elasticsearch 组合，甚至是 graylog，log-pilot 都能把日志投递到正确的地点。 采集输出到console环境准备：一台主机步骤： 先启动一个 log-pilot，再启动一个 tomcat 容器 12345678910111213# 打开一个终端，启动log-pilot容器（不要关闭终端, 观察控制台日志）docker run --name mypilot --rm -it \ -v /var/run/docker.sock:/var/run/docker.sock \ -v /:/host \ --privileged \ registry.cn-hangzhou.aliyuncs.com/acs-sample/log-pilot:latest# 打开一个终端，启动tomcat容器docker run --name mytomcat -it --rm -p 10080:8080 \-v /usr/local/tomcat/logs \--label aliyun.logs.catalina=stdout \--label aliyun.logs.access=/usr/local/tomcat/logs/localhost_access_log.*.txt \tomcat 说明： aliyun.logs.catalina=stdout 告诉 log-pilot 这个容器要收集 stdout 日志。 aliyun.logs.access=/usr/local/tomcat/logs/localhost_access_log.*.txt 则表示要收集容器内 /usr/local/tomcat/logs/ 目录下所有名字匹配 localhost_access_log.*.txt 的文件日志。后面会详细介绍 label 的用法。 log-pilot 会监控 Docker 容器事件，当发现带有 aliyun.logs.xxx 容器时，自动解析容器配置，并且开始收集对应的日志。启动 tomcat 之后，会发现 log-pilot 的终端立即输出了一大堆的内容，其中包含 tomcat 启动时输出的 stdout 日志，也包括 log-pilot 自己输出的一些调试信息。 可以打开浏览器访问刚刚部署的 tomcat，您会发现每次刷新浏览器，在 log-pilot 的终端里都能看到类似的记录。其中 message 后面的内容就是从 /usr/local/tomcat/logs/localhost_access_log.XXX.txt 里收集到的日志。 1234# 打开一个终端，进入tomcat容器docker exec -it -u root mytomcat /bin/bash# 模拟浏览器访问tomcatcurl 127.0.0.1:8080 采集输出到ES+kibana环境准备：一台主机(可以跟上面同一台主机) 部署ELK 123456789# 先在宿主机上执行如下命令sudo sysctl -w vm.max_map_count=262144# 运行ELK容器docker run -d -p 5601:5601 -p 9200:9200 -p 5044:5044 -p 9300:9300 -it --name elk sebp/elk:630# 检查浏览器访问kibana控制台 http://主机IP:5601浏览器访问es http://主机IP:9200 重启log-pilot 12345678910111213# 关闭前面运行的log-pilotdocker stop mypilotdocker rm mypilot# 启动log-pilot，注意替换ES所在的主机IP地址docker run --rm -it \ -v /var/run/docker.sock:/var/run/docker.sock \ -v /:/host \ --privileged \ -e FLUENTD_OUTPUT=elasticsearch \ -e ELASTICSEARCH_HOST=172.17.0.3 \ -e ELASTICSEARCH_PORT=9200 \ registry.cn-hangzhou.aliyuncs.com/acs-sample/log-pilot:latest 相比前面启动 log-pilot 的方式，这里增加了三个环境变量： FLUENTD_OUTPUT=elasticsearch：把日志发送到 ElasticSearch。 ELASTICSEARCH_HOST=${ELASTICSEARCH_HOST}：ElasticSearch 的域名。 ELASTICSEARCH_PORT=${ELASTICSEARCH_PORT}：ElasticSearch 的端口号。 继续运行前面的 tomcat，再次访问，让 tomcat 产生一些日志，所有这些新产生的日志都将发送到 ElasticSearch 里。 打开 kibana，此时您还看不到新日志，需要先创建 index。log-pilot 会把日志写到 ElasticSearch 特定的 index下，规则如下： 如果应用上使用了标签 aliyun.logs.tags，并且 tags 里包含 target，使用 target 作为 ElasticSearch 里的 index。否则，使用标签 aliyun.logs.XXX 里的 XXX 作为 index。 在前面 tomcat 里的例子里，没有使用 liyun.logs.tags 标签，所以默认使用了 access 和 catalina 作为 index。我们先创建 index access。 创建好 index 就可以查看日志了。 使用编排文件运行log-pilotlog-pilot docker-compose.yml 12345678910111213141516171819202122232425262728version: "2"services: pilot: image: registry.cn-hangzhou.aliyuncs.com/acs-sample/log-pilot:latest volumes: - /var/run/docker.sock:/var/run/docker.sock - /:/host privileged: true environment: FLUENTD_OUTPUT: elasticsearch #按照您的需要替换 ELASTICSEARCH_HOST: $&#123;elasticsearch&#125; #按照您的需要替换 ELASTICSEARCH_PORT: 9200 logging: driver: json-file options: max-size: "10m" max-file: "5" cpu_shares: 512 #限制CPU使用份额权重,默认1024 mem_limit: 600m #内存限制600M memswap_limit: 800m #内存+swap限制800M blkio_config: weight: 300 #限制读写磁盘带宽,默认500,取值0-1000 networks: - webnetnetworks: webnet: driver: bridge 查看限制的CPU、内存是否生效： 123456789# 查看CPUcat /sys/fs/cgroup/cpu/docker/&lt;容器长ID&gt;/cpu.shares# 查看内存cat /sys/fs/cgroup/memory/docker/&lt;容器长ID&gt;/memory.limit_in_bytescat /sys/fs/cgroup/memory/docker/&lt;容器长ID&gt;/memory.memsw.limit_in_bytes# 或者直接查看容器详细信息docker inspect &lt;容器长ID&gt; tomcat docker-compose.yml 12345678910111213141516171819202122version: "2"services: mytomcat: image: tomcat:latest ports: - 8080:8080 volumes: - /usr/local/tomcat/logs labels: - "aliyun.logs.catalina=stdout" - "aliyun.logs.access=/usr/local/tomcat/logs/localhost_access_log.*.txt" logging: driver: json-file options: max-size: "10m" max-file: "5" networks: - webnetnetworks: webnet: driver: bridge 运行容器 12docker-compose -f mylogpilot.yml updocker-compose -f mytomcat.yml up label 说明启动 tomcat 时，声明了下面两个 label 来告诉 log-pilot 这个容器的日志位置。 12--label aliyun.logs.catalina=stdout--label aliyun.logs.access=/usr/local/tomcat/logs/localhost_access_log.*.txt 您还可以在应用容器上添加更多的标签。 aliyun.logs.$name = $path 变量 name 是日志名称，只能包含 0~9、a~z、A~Z 和连字符（-）。 变量 path 是要收集的日志路径，必须具体到文件，不能只写目录。文件名部分可以使用通配符，例如，/var/log/he.log 和 /var/log/*.log 都是正确的值，但 /var/log 不行，不能只写到目录。stdout 是一个特殊值，表示标准输出。 aliyun.logs.$name.format：日志格式，目前支持以下格式。 none：无格式纯文本。 json：json 格式，每行一个完整的 json 字符串。 csv：csv 格式。 aliyun.logs.$name.tags：上报日志时，额外增加的字段，格式为 k1=v1,k2=v2，每个 key-value 之间使用逗号分隔，例如 aliyun.logs.access.tags=&quot;name=hello,stage=test&quot;，上报到存储的日志里就会出现 name 字段和 stage 字段。 如果使用 ElasticSearch 作为日志存储，target 这个 tag 具有特殊含义，表示 ElasticSearch 里对应的 index。 最后的思考 采集到重复数据问题，比如采集程序重启之后，读到重复数据，是否有排重？根据什么规则排重？ 采集数据是否会发生遗漏，比如采集程序重启之后，是否有标记最后读取的位置 采集输出到后端，是否有过滤，或者可以自定义过滤 采集输出到后端，是否尽量保证不丢失，采用的策略是什么 如果log-pilot做不到上述事情,或者扩展起来麻烦，则考虑自己写个agent, 起码自己写的程序，知道如何驾驭; 参考 log-pilot官方https://github.com/AliyunContainerService/log-pilothttps://helpcdn.aliyun.com/document_detail/50441.html Docker日志收集最佳实践https://yq.aliyun.com/articles/72700 logspouthttps://github.com/gliderlabs/logspout logspout 网友扩展https://www.jianshu.com/p/bd482c13ea66 几种采集架构图 业务埋点数据、应用日志数据log-pilot-&gt;kafka-&gt;logstash-&gt;es-&gt;kibanalog-pilot-&gt;kafka-&gt;storm/spark 应用日志数据log-pilot-&gt;logstash-&gt;es-&gt;kibana 采集文件日志采集工具可以用log-pilot、filebeat、fluentd、flume、….]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker swarm mode]]></title>
    <url>%2Fdocker%2Fdocker%20swarm%2F</url>
    <content type="text"><![CDATA[准备环境 三台linux服务器或者linux容器,并且安装docker环境 这里我采用容器, 需docker run docker,也就是docker上运行centos容器，centos容器中再运行docker环境;构建容器环境，参考：https://github.com/smalldok/docker-based-tools/tree/master/dind_centos 本地构建dind_centos镜像之后,分别创建如下三个容器（对应好Ip和hostname） role ip hostname desc manager node 172.17.0.2 docker-0 - work node 172.17.0.3 docker-1 - work node 172.17.0.4 docker-2 - 主机间开放端口在各个节点进行通信的时候,必须开放相关的防火墙策略,其中包括：tcp port 2377为集群管理通信tcp and udp port 7946为节点通信udp port 4789为网络间流量 分别在各主机上执行如下命令： 1234567# 开放防火墙端口firewall-cmd --permanent --add-port=2377/tcpfirewall-cmd --permanent --add-port=7946/tcpfirewall-cmd --permanent --add-port=7946/udpfirewall-cmd --permanent --add-port=4789/udpfirewall-cmd --reload # 永久打开端口需要reload一下firewall-cmd --list-all # 查看防火墙，添加的端口也可以看到 centos7镜像需要安装的工具 123456# ifconfig命令支持yum install net-tools# 安装firewalldyum install firewalld firewall-configsystemctl start firewalld # 启动systemctl status firewalld # 或者 firewall-cmd --state 查看状态 创建swarm集群初始化集群，节点之间相互通信的IP地址为172.17.0.2，默认端口2377 12345678[root@docker-0 swarm]# docker swarm init --advertise-addr 172.17.0.2Swarm initialized: current node (dlr9h76ov43r7239sq3iksjyw) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-3d7ynvjvs9kqdqaut2sir2ky42uoiv54fmwef5s7odei33a031-3rcwr1styfzw8f8j3k9mvbz9c 172.17.0.2:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. swarm集群模式是否激活 12[root@docker-0 swarm]# docker info|grep -i swarmSwarm: active 默认监听两个TCP端口 1234# 2377：集群的管理端口、7946：节点之间的通信端口[root@docker-0 swarm]# netstat -tnlp|grep dockertcp6 0 0 :::2377 :::* LISTEN 78/dockerd tcp6 0 0 :::7946 :::* LISTEN 78/dockerd 查看网络 12345678# 默认会创建一个overlay的网络ingress、一个bridge网络docker_gwbridge[root@docker-0 swarm]# docker network lsNETWORK ID NAME DRIVER SCOPE301fd3e9c939 bridge bridge local5ab7c9aec44f docker_gwbridge bridge local9faeade5067d host host localk0m6xm3c1pgr ingress overlay swarm74eb81b4e335 none null local 查看集群中的节点,当有多个manager节点的时候,是通过raft协议来选取主节点,也就是leader节点 123[root@docker-0 swarm]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONdlr9h76ov43r7239sq3iksjyw * docker-0 Ready Active Leader 18.03.1-ce swarm配置文件 12345678# swarm配置文件都在/var/lib/docker/swarm目录中,有相关的证书和manager的配置文件,使用的是raft协议[root@docker-0 swarm]# ls -ltotal 20drwxr-xr-x 2 root root 4096 Jun 26 03:23 certificates # 使用tls来进行安全通信-rw------- 1 root root 145 Jun 26 03:23 docker-state.json # 用来记录通信的地址和端口,也会记录本地的地址和端口drwx------ 4 root root 4096 Jun 26 03:23 raft # raft协议-rw------- 1 root root 66 Jun 26 03:23 state.json # manager的ip和端口drwxr-xr-x 2 root root 4096 Jun 26 03:23 worker # 记录工作节点下发的任务信息 登陆docker-1, 将docker-1节点加入集群 12[root@docker-1 /]# docker swarm join --token SWMTKN-1-3d7ynvjvs9kqdqaut2sir2ky42uoiv54fmwef5s7odei33a031-3rcwr1styfzw8f8j3k9mvbz9c 172.17.0.2:2377This node joined a swarm as a worker. 登陆docker-2, 将docker-2节点加入集群 12[root@docker-2 /]# docker swarm join --token SWMTKN-1-3d7ynvjvs9kqdqaut2sir2ky42uoiv54fmwef5s7odei33a031-3rcwr1styfzw8f8j3k9mvbz9c 172.17.0.2:2377This node joined a swarm as a worker. 当忘记了加入集群的token,可以使用如下命令找到,然后在node节点上直接执行,就可以加入worker节点或者manager节点 1234[root@docker-0 swarm]# docker swarm join-token workerTo add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-3d7ynvjvs9kqdqaut2sir2ky42uoiv54fmwef5s7odei33a031-3rcwr1styfzw8f8j3k9mvbz9c 172.17.0.2:2377 查看集群节点信息 12345[root@docker-0 swarm]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONdlr9h76ov43r7239sq3iksjyw * docker-0 Ready Active Leader 18.03.1-cewscczkpfb15c5knm6cr6lj712 docker-1 Ready Active 18.03.1-cepgpnatt19yjti3eqy0gc3ebwg docker-2 Ready Active 18.03.1-ce 节点之间的角色可以随时进行转换(使用update进行更新) 1234567891011121314[root@docker-0 swarm]# docker node update --role manager docker-1docker-1[root@docker-0 swarm]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONdlr9h76ov43r7239sq3iksjyw * docker-0 Ready Active Leader 18.03.1-cewscczkpfb15c5knm6cr6lj712 docker-1 Ready Active Reachable 18.03.1-cepgpnatt19yjti3eqy0gc3ebwg docker-2 Ready Active 18.03.1-ce[root@docker-0 swarm]# docker node update --role worker docker-1docker-1[root@docker-0 swarm]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONdlr9h76ov43r7239sq3iksjyw * docker-0 Ready Active Leader 18.03.1-cewscczkpfb15c5knm6cr6lj712 docker-1 Ready Active 18.03.1-cepgpnatt19yjti3eqy0gc3ebwg docker-2 Ready Active 18.03.1-ce 运行服务service是一组task集合，包括多个task，一个task为一个容器;例如运行nginx服务,从而拆解为几个nginx的容器在各个节点上进行运行； 创建名称为web的服务，基于nginx镜像 12345[root@docker-0 swarm]# docker service create --name web nginxplwzhwijh15j6tpg7yz7wshdpoverall progress: 1 out of 1 tasks1/1: runningverify: Service converged 创建名称为frontweb的服务，基于nginx镜像，模式为global 1234567[root@docker-0 swarm]# docker service create --name frontweb --mode global nginxp2geazk5hdckzzb9rdd1m5n9loverall progress: 3 out of 3 tasksdlr9h76ov43r: runningwscczkpfb15c: runningpgpnatt19yjt: runningverify: Service converged 查看运行的服务 1234[root@docker-0 swarm]# docker service lsID NAME MODE REPLICAS IMAGE PORTSp2geazk5hdck frontweb global 3/3 nginx:latest plwzhwijh15j web replicated 1/1 nginx:latest 查看web运行信息,默认情况下manager节点也可以运行容器 123[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSqjrl2qg04a3b web.1 nginx:latest docker-0 Running Running 15 minutes ago 查看frontweb运行信息,默认情况下manager节点也可以运行容器 12345[root@docker-0 swarm]# docker service ps frontwebID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSn7r3e85ukc2m frontweb.pgpnatt19yjti3eqy0gc3ebwg nginx:latest docker-2 Running Running 5 minutes agopaf09ihxyr5q frontweb.wscczkpfb15c5knm6cr6lj712 nginx:latest docker-1 Running Running 5 minutes agoo7yrjm0jbll1 frontweb.dlr9h76ov43r7239sq3iksjyw nginx:latest docker-0 Running Running 6 minutes ago 创建服务时，服务的几种状态prepared 表示准备,主要是从仓库拉取镜像；starting 启动容器,进行验证容器状态；running 正常运行 服务模式replicated 表示副本，默认使用的mode，并且默认情况只会创建一个副本，主要使用目的是为了高可用;global 必须在每个节点上运行一个task(也就是容器),可以看到上面创建frontweb服务时，使用global创建了三个容器; 查看frontweb服务详细信息 12345678910111213141516171819202122[root@docker-0 swarm]# docker service inspect frontweb --prettyID: p2geazk5hdckzzb9rdd1m5n9lName: frontwebService Mode: GlobalPlacement:UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-firstRollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-firstContainerSpec: Image: nginx:latest@sha256:3e2ffcf0edca2a4e9b24ca442d227baea7b7f0e33ad654ef1eb806fbd9bedcf0Resources:Endpoint Mode: vip 服务扩容缩容集群必然涉及服务高可用，从而会有服务的扩容缩容 将web服务扩容为3个task 1234567[root@docker-0 swarm]# docker service scale web=3web scaled to 3overall progress: 3 out of 3 tasks1/3: running [==================================================&gt;]2/3: running [==================================================&gt;]3/3: running [==================================================&gt;]verify: Service converged 查看服务，可看到web服务的replicas副本数量为3个 1234[root@docker-0 swarm]# docker service lsID NAME MODE REPLICAS IMAGE PORTSp2geazk5hdck frontweb global 3/3 nginx:latestplwzhwijh15j web replicated 3/3 nginx:latest 可以看到三个节点上都运行了一个web服务(task) 12345[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSqjrl2qg04a3b web.1 nginx:latest docker-0 Running Running 41 minutes agos6ejxq4xb1qo web.2 nginx:latest docker-2 Running Running 3 minutes ago5xim2g6fmom8 web.3 nginx:latest docker-1 Running Running 3 minutes ago 在默认情况下，管理节点也可以运行task，所以上面可以看到Manager节点上也运行了web服务 将web服务缩容为2个 123456[root@docker-0 swarm]# docker service scale web=2web scaled to 2overall progress: 2 out of 2 tasks1/2: running [==================================================&gt;]2/2: running [==================================================&gt;]verify: Service converged 查看运行的web容器 1234[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSqjrl2qg04a3b web.1 nginx:latest docker-0 Running Running about an hour agos6ejxq4xb1qo web.2 nginx:latest docker-2 Running Running 9 minutes ago 当要让swarm的manager节点不运行容器的时候，只要更改节点的状态，从Active变成Drain即可；如果在manager上运行容器，那么当manager宕机的时候，如果不是多节点的manager，会导致服务无法进行调度； 将manager节点的状态修改为Drain状态，使得此节点不会运行相关的task任务 123456789[root@docker-0 swarm]# docker node update --availability drain docker-0docker-0# 查看节点的状态从active变成drain[root@docker-0 swarm]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONdlr9h76ov43r7239sq3iksjyw * docker-0 Ready Drain Leader 18.03.1-cewscczkpfb15c5knm6cr6lj712 docker-1 Ready Active 18.03.1-cepgpnatt19yjti3eqy0gc3ebwg docker-2 Ready Active 18.03.1-ce 本来运行在docker-0节点上的所有task会被关闭，然后mode=replicated的服务会自动迁移到其他的worker节点上(这一点有利也有弊)，mode=global的服务不会自动迁移 12345[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSq57mgwxfdxl0 web.1 nginx:latest docker-1 Running Running 2 minutes agoqjrl2qg04a3b \_ web.1 nginx:latest docker-0 Shutdown Shutdown 2 minutes agos6ejxq4xb1qo web.2 nginx:latest docker-2 Running Running 18 minutes ago 自动故障转移当运行在某节点上的task被关闭、或者节点宕机、或者网络抖动，mode=replicated的服务会自动迁移到其他的worker节点上(这一点有利也有弊)，mode=global的服务不会自动迁移；mode=replicated时的自动迁移，在生产环境中需要考虑的一个问题就是，如果出现了自动failover，那么其他的机器是否有足够的资源来创建这些容器，所以需要考虑node节点剩余资源的问题，例如cpu和内存； 场景模拟：1、mode=replicated的web服务，在docker-1和docker-2这2个节点上分别运行着一个task；2、模拟将docker-2节点进行宕机(关闭docker服务)，观察docker-1节点上将会有2个task运行；3、恢复docker-2节点服务，观察docker-1节点上还是会有2个task运行，docker-2节点不会有task运行，可以看到并不会把之前迁移到docker-1上的服务自动迁移回来，所以这里也比较坑； 模拟宕机前： 服务名 节点主机名 运行状态 web.1 docker-1 Running web.2 docker-2 Running 模拟宕机后(docker-2宕机)： 服务名 节点主机名 运行状态 web.1 docker-1 Running web.2 docker-1 Running web.2 docker-2 Shutdown 模拟恢复后： 服务名 节点主机名 运行状态 web.1 docker-1 Running web.2 docker-1 Running web.2 docker-2 Shutdown 查看服务分布 1234[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSq57mgwxfdxl0 web.1 nginx:latest docker-1 Running Running 25 minutes agos6ejxq4xb1qo web.2 nginx:latest docker-2 Running Running 42 minutes ago 模拟docker-2宕机 12# 登陆docker-2节点，关闭docker服务[root@docker-2 /]# systemctl stop docker 查看节点信息，可以看到docker-2已经为down，也就是主机宕机 12345[root@docker-0 swarm]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONdlr9h76ov43r7239sq3iksjyw * docker-0 Ready Drain Leader 18.03.1-cewscczkpfb15c5knm6cr6lj712 docker-1 Ready Active 18.03.1-cepgpnatt19yjti3eqy0gc3ebwg docker-2 Down Active 18.03.1-ce 观察docker-1节点上将会有2个task运行，宕机的docker-2标记为shutdown 12345[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSq57mgwxfdxl0 web.1 nginx:latest docker-1 Running Running 42 minutes agouqpzuj4tkwpd web.2 nginx:latest docker-1 Running Running 2 minutes agos6ejxq4xb1qo \_ web.2 nginx:latest docker-2 Shutdown Running 2 minutes ago 将docker-2节点进行上线 1[root@docker-2 /]# systemctl start docker 查看服务节点信息，docker-2状态变为ready,表示可以运行task任务 12345[root@docker-0 swarm]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONdlr9h76ov43r7239sq3iksjyw * docker-0 Ready Drain Leader 18.03.1-cewscczkpfb15c5knm6cr6lj712 docker-1 Ready Active 18.03.1-cepgpnatt19yjti3eqy0gc3ebwg docker-2 Ready Active 18.03.1-ce 再次查看web服务的分布，服务没有再迁移回docker-2节点 12345[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSq57mgwxfdxl0 web.1 nginx:latest docker-1 Running Running about an hour agouqpzuj4tkwpd web.2 nginx:latest docker-1 Running Running 16 minutes agos6ejxq4xb1qo \_ web.2 nginx:latest docker-2 Shutdown Shutdown 2 minutes ago 访问服务访问服务分为两种，对内、对外；对内：内部访问的服务，不对外开放端口对外：外部节点可访问，对外开放端口，也就是主机映射端口 创建2个副本的httpd服务 123456[root@docker-0 swarm]# docker service create --name web --replicas=2 httpd6mxfzdhrqeqgexmbyq5u7j6dsoverall progress: 2 out of 2 tasks1/2: running [==================================================&gt;]2/2: running [==================================================&gt;]verify: Service converged 或者 1[root@docker-0 swarm]# docker service create --name web --publish 8000:80 --replicas=2 httpd 查看运行容器的主机 1234[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS5fwbadj1s8gx web.1 httpd:latest docker-2 Running Running about a minute agoklab0nnwke22 web.2 httpd:latest docker-1 Running Running about a minute ago 登陆节点服务，查看运行的容器 1234567[root@docker-1 /]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES74429db851d3 httpd:latest "httpd-foreground" 33 seconds ago Up 13 seconds 80/tcp web.2.klab0nnwke223tzztixiii1uq[root@docker-2 /]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES577c242824f0 httpd:latest "httpd-foreground" About a minute ago Up About a minute 80/tcp web.1.5fwbadj1s8gxx4smkils5lmu9 查看运行容器的IP地址 12345678910111213[root@docker-1 /]# docker exec 74429db851d3 ip addr show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever18: eth0@if19: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever# 根据IP地址访问，只能在本节点上进行访问，属于内部网络，也就是docker_gwbrige网络[root@docker-1 /]# curl 172.18.0.2&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 添加主机映射端口，将服务对外开放，使外部能访问此服务 12345678910111213141516171819202122232425[root@docker-0 swarm]# docker service update --publish-add 8000:80 webweboverall progress: 2 out of 2 tasks1/2: running [==================================================&gt;]2/2: running [==================================================&gt;]verify: Service converged# 可以观察到，原来的容器被关闭，重新创建了新的容器[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSsad9u6d1gf1v web.1 httpd:latest docker-2 Running Running 2 minutes ago5fwbadj1s8gx \_ web.1 httpd:latest docker-2 Shutdown Shutdown 2 minutes agonbxubcnx7dpy web.2 httpd:latest docker-1 Running Running 2 minutes agoklab0nnwke22 \_ web.2 httpd:latest docker-1 Shutdown Shutdown 2 minutes ago# 无论是manager节点还是worker节点，均监听了8000端口，均可以访问[root@docker-0 swarm]# curl 172.17.0.3:8000&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;[root@docker-0 swarm]# curl 172.17.0.4:8000&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;[root@docker-0 swarm]# curl 172.17.0.3:8000# 集群中每个机器都会监听8000端口，无论是否运行了这个容器[root@docker-0 swarm]# netstat -ntlp|grep 8000tcp6 0 0 :::8000 :::* LISTEN 66487/dockerd 以上使用的就是routing mesh功能，在swarm内部实现了负载均衡，使用的网络为swarm自动创建的overlay网络。当使用publish端口时，最大坏处是对外暴露了端口号，而且在使用的时候，如果进行了故障转移，那么多个服务运行在同一个主机上面，会造成端口占用冲突； 服务发现docker-swarm原生提供了服务发现功能，也可以使用外部中间件，如consul/etcd/zookeeper; 创建overlay网络要使用服务发现，需要相互通信的service必须属于同一个overlay网络,所以我们先创建一个新的overlay网络； 12345678910[root@docker-0 swarm]# docker network create --driver overlay webs9jlkeot6iua2pzldff16dz8k[root@docker-0 swarm]# docker network lsNETWORK ID NAME DRIVER SCOPE301fd3e9c939 bridge bridge local5ab7c9aec44f docker_gwbridge bridge local9faeade5067d host host localk0m6xm3c1pgr ingress overlay swarm74eb81b4e335 none null locals9jlkeot6iua web overlay swarm 默认的ingress没有提供服务发现，必须创建自己的overlay网络； 部署service到新建的overlay部署一个web服务，并将其挂载到新创建的overlay网络 123456[root@docker-0 swarm]# docker service create --name my-web --replicas=2 --network web httpdqvws1y4kvkkiii9ny1tzmfriooverall progress: 2 out of 2 tasks1/2: running2/2: runningverify: Service converged 部署一个test服务用于测试，挂载到同一个overlay网络 12345678910[root@docker-0 swarm]# docker service create --name test --network web busybox sleep 100000000o216j6g3cny2ulrs9ncacr4roverall progress: 1 out of 1 tasks1/1: runningverify: Service converged# sleep 10000000的作用是保持busybox容器处于运行状态，我们才能进入到容器访问my-web[root@docker-0 swarm]# docker service ps testID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSz6ozex5qo747 test.1 busybox:latest docker-1 Running Running 29 seconds ago 验证：通过docker service ps test确认test所在节点在docker-1;登陆到docker-1节点，并进入test容器，然后ping my-web 12345678910111213[root@docker-1 /]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4f23e905ab2b busybox:latest "sleep 10000000" 10 seconds ago Up 7 seconds test.1.z6ozex5qo74781icswy4245pd0af60046e58c httpd:latest "httpd-foreground" 3 minutes ago Up 2 minutes 80/tcp my-web.1.llob2wie0nx1x98m1vw780hcv[root@docker-1 /]# docker exec test.1.z6ozex5qo74781icswy4245pd ping -c 2 my-webPING my-web (10.0.0.5): 56 data bytes64 bytes from 10.0.0.5: seq=0 ttl=64 time=0.112 ms64 bytes from 10.0.0.5: seq=1 ttl=64 time=0.167 ms--- my-web ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.112/0.139/0.167 ms 10.0.0.5是虚拟IP 查看各个主机的IP地址 1234567[root@docker-1 /]# docker exec test.1.z6ozex5qo74781icswy4245pd nslookup tasks.my-webServer: 127.0.0.11Address 1: 127.0.0.11Name: tasks.my-webAddress 1: 10.0.0.6 my-web.2.zrj1b3nqaqfgmm3ipaj4grk33.webAddress 2: 10.0.0.7 my-web.1.llob2wie0nx1x98m1vw780hcv.web 查看VIP地址 123456[root@docker-1 /]# docker exec test.1.z6ozex5qo74781icswy4245pd nslookup my-webServer: 127.0.0.11Address 1: 127.0.0.11Name: my-webAddress 1: 10.0.0.5 对于服务使用者，test服务根本就不需要知道my-web副本的IP、VIP，只需直接用service的名字就能访问服务； 滚动更新service当需要进行更新的时候，swarm可以设定相应的策略来进行更新，例如并行更新多少个容器，更新时间间隔等；滚动更新降低了应用更新的风险，如果某个副本更新失败，整个更新将暂停，其他副本则可以继续提供服务，同时，在更新的过程中，总是有副本在运行的，因此也保证了业务连续性； swarm安装如下步骤进行滚动更新： 停止第一个副本 调度任务。选择worker node 在worker上用新的镜像启动副本 如果更新成功则继续更新下一个，如果失败，暂停整个更新的过程 测试：创建2个副本的service，镜像使用httpd:2.2.31,然后将其更新到httpd:2.2.32 1234567891011121314151617181920212223242526[root@docker-0 swarm]# docker service create --name web-update --replicas=2 httpd:2.2.31yhjzc1lmkygvzk7zma6v7rwfyoverall progress: 2 out of 2 tasks1/2: running [==================================================&gt;]2/2: running [==================================================&gt;]verify: Service converged[root@docker-0 swarm]# docker service ps web-updateID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSpnss5ryum1f9 web-update.1 httpd:2.2.31 docker-1 Running Running about a minute agoo7fkw7jl78j2 web-update.2 httpd:2.2.31 docker-2 Running Running about a minute ago# 将web-update更新到httpd:2.2.32 (默认一次只更新一个副本，且副本间没有等待时间)[root@docker-0 swarm]# docker service update --image httpd:2.2.32 web-updateweb-updateoverall progress: 2 out of 2 tasks1/2: running [==================================================&gt;]2/2: running [==================================================&gt;]verify: Service converged[root@docker-0 swarm]# docker service ps web-updateID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSri7pp1gwfpof web-update.1 httpd:2.2.32 docker-1 Running Running 8 seconds agopnss5ryum1f9 \_ web-update.1 httpd:2.2.31 docker-1 Shutdown Shutdown about a minute ago974r7xpufhry web-update.2 httpd:2.2.32 docker-2 Running Running about a minute agoo7fkw7jl78j2 \_ web-update.2 httpd:2.2.31 docker-2 Shutdown Shutdown 2 minutes ago 更新前,先设置更新的并发数和更新的时间间隔，--update-parallelism设置并行更新的副本数，通过--update-delay指定滚动更新的时间间隔 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 副本增加到6个，并且设置更新并发数为2个，时间间隔为1分5秒[root@docker-0 swarm]# docker service update --replicas 6 --update-parallelism 2 --update-delay 1m05s web-updateweb-updateoverall progress: 6 out of 6 tasks1/6: running [==================================================&gt;]2/6: running [==================================================&gt;]3/6: running [==================================================&gt;]4/6: running [==================================================&gt;]5/6: running [==================================================&gt;]6/6: running [==================================================&gt;]verify: Service converged[root@docker-0 swarm]# docker service ps web-updateID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSri7pp1gwfpof web-update.1 httpd:2.2.32 docker-1 Running Running 10 minutes agopnss5ryum1f9 \_ web-update.1 httpd:2.2.31 docker-1 Shutdown Shutdown 11 minutes ago974r7xpufhry web-update.2 httpd:2.2.32 docker-2 Running Running 11 minutes agoo7fkw7jl78j2 \_ web-update.2 httpd:2.2.31 docker-2 Shutdown Shutdown 12 minutes agol5gt07q396dw web-update.3 httpd:2.2.32 docker-2 Running Running about a minute agotj4fookdmlko web-update.4 httpd:2.2.32 docker-1 Running Running 49 seconds agoncyji5qju1co web-update.5 httpd:2.2.32 docker-2 Running Running 49 seconds ago1x58y4zqqyq6 web-update.6 httpd:2.2.32 docker-1 Running Running about a minute ago#查看具体的信息，可以看到设置更新的参数[root@docker-0 swarm]# docker service inspect web-update --prettyID: yhjzc1lmkygvzk7zma6v7rwfyName: web-updateService Mode: Replicated Replicas: 6Placement:UpdateConfig: Parallelism: 2 Delay: 1m5s On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-firstRollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-firstContainerSpec: Image: httpd:2.2.32@sha256:a28a1a54ee7c4e03249b5eb3fed0c387399ffa5bb422ab50cbb19ffde76e58e7Resources:Endpoint Mode: vip 将目前的6个副本从httpd:2.2.32, 更新到httpd:2.4.16 1234567891011#可以观察到，每次2个副本同时更新，1分05秒后进行下一组的2个副本更新[root@docker-0 swarm]# docker service update --image httpd:2.4.16 web-updateweb-updateoverall progress: 6 out of 6 tasks1/6: running2/6: running3/6: running4/6: running5/6: running6/6: runningverify: Service converged 如果在中途更新失败，则暂停整个集群的更新；那么将会导致整个集群的副本版本不一致，这时需要rollback回滚； 更新回滚swarm有个方便的回滚功能，如果更新后的效果不理想，可以通过–rollback快速恢复到更新之前的状态；但是它只能回滚到上一次执行docker service update之前的状态，并不能无限制回滚； 1[root@docker-0 swarm]# docker service update --rollback web-update 测试下来，这个回滚并不是很稳定，会有回滚失败导致容器副本数减少、有些副本回滚没成功问题； 节点标签给node节点加label标签，用于控制task运行在哪些节点；生产环境上有时需要手动将容器分布在不同的节点； 更新节点属性，为节点添加标签 1234[root@docker-0 swarm]# docker node update --label-add ncname=docker-1-label docker-1docker-1[root@docker-0 swarm]# docker node update --label-add ncname=docker-2-label docker-2docker-2 查看设置的标签 1234[root@docker-0 swarm]# docker node inspect docker-1 --prettyID: wscczkpfb15c5knm6cr6lj712Labels: - ncname=docker-1-label 根据label , 指定节点来运行相关的任务 123456789101112[root@docker-0 swarm]# docker service create --name web --constraint node.labels.ncname==docker-1-label --replicas 2 nginx5xqg712i4ugcli6myq0u4pa75overall progress: 2 out of 2 tasks1/2: running [==================================================&gt;]2/2: running [==================================================&gt;]verify: Service converged# 可以看到，运行在指定的机器上[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSg0n5bz0zrml1 web.1 nginx:latest docker-1 Running Running 13 seconds agojdaah28caxa9 web.2 nginx:latest docker-1 Running Running 13 seconds ago 去掉指定的标签 123456[root@docker-0 swarm]# docker service update --constraint-rm node.labels.ncname==docker-1-label webweboverall progress: 2 out of 2 tasks1/2: running2/2: runningverify: Service converged 服务迁移到另一台主机上 12345678910111213[root@docker-0 swarm]# docker service update --constraint-add node.labels.ncname==docker-2-label webweboverall progress: 2 out of 2 tasks1/2: running2/2: runningverify: Service converged[root@docker-0 swarm]# docker service ps webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSvhc4kvdlbvhd web.1 nginx:latest docker-2 Running Running about a minute agog0n5bz0zrml1 \_ web.1 nginx:latest docker-1 Shutdown Shutdown about a minute ago3uc0w4ozudfm web.2 nginx:latest docker-2 Running Running 53 seconds agojdaah28caxa9 \_ web.2 nginx:latest docker-1 Shutdown Shutdown 55 seconds ago 这种修改也可以使用rollback进行回滚；但我使用回滚命令，然而并没有什么卵用，留待以后再做验证1[root@docker-0 swarm]# docker service update --rollback web 健康检查在企业生产环境中，合理的健康检查设置可以保证应用的可用性。现在很多应用框架已经内置了监控检查能力，比如Spring Boot Actuator。配合Docker内置的健康检测机制，可以非常简洁实现应用可用性监控，自动故障处理，和零宕机更新。 有两种方式： dockerfile 中加指令方式 docker service create 参数指令方式 dockerfile 中加指令方式HEALTHCHECK 指令格式： HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉 注：在Dockerfile中 HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。 使用包含 HEALTHCHECK 指令的dockerfile构建出来的镜像，在实例化Docker容器的时候，就具备了健康状态检查的功能。启动容器后会自动进行健康检查。 HEALTHCHECK 支持下列选项： –interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒； –timeout=&lt;间隔&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； –retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。 –start-period=&lt;间隔&gt;: 应用的启动的初始化时间，在启动过程中的健康检查失效不会计入，默认 0 秒； (从17.05)引入 在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否： 0：成功； 1：失败； 2：保留值，不要使用 容器启动之后，初始状态会为 starting (启动中)。Docker Engine会等待 interval 时间，开始执行健康检查命令，并周期性执行。如果单次检查返回值非0或者运行需要比指定 timeout 时间还长，则本次检查被认为失败。如果健康检查连续失败超过了 retries 重试次数，状态就会变为 unhealthy (不健康)。 注： 一旦有一次健康检查成功，Docker会将容器置回 healthy (健康)状态 当容器的健康状态发生变化时，Docker Engine会发出一个 health_status 事件。 假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：1234FROM elasticsearch:5.5 HEALTHCHECK --interval=5s --timeout=2s --retries=12 \ CMD curl --silent --fail localhost:9200/_cluster/health || exit 1 123456789[root@docker-1 es]# lltotal 4-rw-r--r-- 1 root root 148 Jun 28 07:48 Dockerfile# 构建镜像[root@docker-1 es]# docker build -t test/elasticsearch:5.5 .# 运行镜像[root@docker-1 es]# docker run --rm -d --name=elasticsearch test/elasticsearch:5.5 查看Elasticsearch容器从 starting 状态进入了 healthy 状态 123456[root@docker-1 es]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf129ea7ebc6e test/elasticsearch:5.5 "/docker-entrypoint.…" 47 seconds ago Up 10 seconds (health: starting) 9200/tcp, 9300/tcp elasticsearch[root@docker-1 es]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf129ea7ebc6e test/elasticsearch:5.5 "/docker-entrypoint.…" About a minute ago Up 36 seconds (healthy) 9200/tcp, 9300/tcp elasticsearch 另外一种方法是在 docker run 命令中，直接指明healthcheck相关策略。 1234567$ docker run --rm -d \ --name=elasticsearch \ --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \ --health-interval=5s \ --health-retries=12 \ --health-timeout=2s \ elasticsearch:5.5 为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里，可以用 docker inspect 来查看。我们可以通过如下命令，来获取过去5个容器的健康检查结果 1234567docker inspect --format='&#123;&#123;json .State.Health&#125;&#125;' elasticsearch# 或者docker inspect elasticsearch | jq ".[].State.Health"# 或者docker inspect elasticsearch 返回123456789101112&#123; "Status": "healthy", "FailingStreak": 0, "Log": [ &#123; "Start": "2018-06-28T09:04:45.612342812Z", "End": "2018-06-28T09:04:45.736592584Z", "ExitCode": 0, "Output": "..." &#125;, ...&#125; 建议在Dockerfile中声明相应的健康检查策略，这样可以方便镜像的使用 Docker社区为提供了一些包含健康检查的实例镜像，我们可以在如下项目中获取 https://github.com/docker-library/healthcheck docker service create 参数指令方式在Docker 1.13之后，在Docker Swarm mode中提供了对健康检查策略的支持 可以在 docker service create 命令中指明健康检查策略 1234567$ docker service create -d \ --name=elasticsearch \ --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \ --health-interval=5s \ --health-retries=12 \ --health-timeout=2s \ elasticsearch 在Swarm模式下，Swarm manager会监控服务task的健康状态，如果容器进入 unhealthy 状态，它会停止容器并且重新启动一个新容器来取代它。这个过程中会自动更新服务的 load balancer (routing mesh) 后端或者 DNS记录，可以保障服务的可用性。 在1.13版本之后，在服务更新阶段也增加了对健康检查的支持，这样在新容器完全启动成功并进入健康状态之前，load balancer/DNS解析不会将请求发送给它。这样可以保证应用在更新过程中请求不会中断。 常见问题空间不足如果哪天发现镜像无法构建，那一般都是空间问题； 方法一：暴力清除12345678# 查看哪个目录占用空间最大du -sh /*# 停止docker服务systemctl stop docker# 删除/var/lib/docker目录下的所有文件rm -rf /var/lib/docker# 启动docker服务systemctl start docker 注意：docker 服务重启后，需要重新加入集群节点1docker swarm join --token SWMTKN-1-3d7ynvjvs9kqdqaut2sir2ky42uoiv54fmwef5s7odei33a031-3rcwr1styfzw8f8j3k9mvbz9c 172.17.0.2:2377 方法二： 转移/var/lib/docker目录略…]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker save-load and export-import]]></title>
    <url>%2Fdocker%2Fdocker%20save-load%20and%20export-import%2F</url>
    <content type="text"><![CDATA[docker 镜像导入导出两组导入导出命令： docker save/load docker export/import 两种使用场景(两者可以结合使用)： 命令 场景 save/load 如果部署的节点服务器不能连外部或者内部镜像仓库，则可以docker save镜像打包，然后拷贝上传到要部署的节点服务其上，使用docker load载入 export/import 用于制作基础镜像，比如容器启动后，在容器中安装一些软件或者环境的设置，使用docker export导出修改后的容器，然后分发给其他人使用(比如作为基础的开发环境) 也可以使用docker commit命令，提交修改后的容器，并上传至镜像仓库;注意：用docker commit命令打包的镜像，比Dockerfile方式打出的镜像大(镜像分层问题) 测试环境准备步骤： 构建一个centos镜像 进入容器安装软件 配置防火墙 提交容器修改 构建镜像、运行容器、进入容器后台 12345$ git clone https://github.com/smalldok/docker-based-tools.git$ cd dind_centos$ docker build -t dind_centos .$ docker run --name docker-0 --privileged -d dind_centos$ docker exec -it -u root docker-0 /bin/bash 安装软件 123456# 安装支持ifconfig命令的工具yum install net-tools# 安装firewalldyum install firewalld firewall-configsystemctl start firewalld # 启动 开放docker-swarm需要的端口 1234567# 开放防火墙端口firewall-cmd --permanent --add-port=2377/tcpfirewall-cmd --permanent --add-port=7946/tcpfirewall-cmd --permanent --add-port=7946/udpfirewall-cmd --permanent --add-port=4789/udpfirewall-cmd --reload # 永久打开端口需要reload一下firewall-cmd --list-all # 查看防火墙，添加的端口也可以看到 docker export/import docker exportdocker export是将container的文件系统打包; docker importdocker import将container导入后成为一个Image,而不是恢复为一个container;可指定image[:tag]，为镜像指定新名称；如果名称相同，则会覆盖老的镜像； docker export 导出容器 12345$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1eb256b1d4b1 dind_centos "wrapdocker /usr/sbi…" About an hour ago Up About an hour docker-0$ docker export 1eb256b1d4b1 &gt; dind_centos.tar docker import 导入 12345678$ docker import dind_centos.tar dind_centos:1.0sha256:4e8612dad5ba4230e618a424727a679e0f52f7a1fcfb16bbfe5626338784d404$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdind_centos 1.0 4e8612dad5ba 20 seconds ago 539MBdind_centos latest 9031f35e0ec5 4 days ago 550MBcentos latest 49f7960eb7e4 3 weeks ago 200MB 查看之前对容器的修改是否还在 123456# 运行容器$ docker run --name docker-test --privileged -d dind_centos:1.0 wrapdocker /usr/sbin/init960ecfc2f531533ba36c3cefb00fa9f784cdafd14a898577c52262f03d3faede# 查看ifconfig是否生效$ ifconfig docker save/load docker savedocker save 可以对image或者container打包；对container打包，其实打的是容器背后的image;docker save 可以用来将一个或者多个Image打包，如： 12# 打包之后的test.tar包含nginx:1.0 httpd:1.4这两个镜像docker save -o test.tar nginx:1.0 httpd:1.4 docker load 12# 该命令会把nginx:1.0 httpd:1.4载入进来，如果本地已经存在这两个镜像，会被覆盖docker load -i test.tar]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker构建Jenkins image]]></title>
    <url>%2Fdocker%2Fdocker%E6%9E%84%E5%BB%BAjenkins%20image%2F</url>
    <content type="text"><![CDATA[引言基于官方Jenkins镜像，构建打包环境，包括Maven/git/nodejs;预安装jenkins各种插件；具体见： https://github.com/smalldok/jenkins-maven-git-nodejs-docker 以下是构建jenkins image步骤 拉取官方镜像 启动docker服务 拉取jenkins镜像(docker hub 官方)官方镜像搜索：https://hub.docker.comdocker pull jenkins:2.60.3 或者docker pull jenkins 查看镜像docker images 删除镜像docker rmi 7b210b6c238a 或者 docker rmi -f 7b210b6c238a 运行镜像 1docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v /Users/smalldok/work/idea-work/docker/volume/jenkins_home:/var/jenkins_home jenkins:2.60.3 或者以交互方式运行(这种适合更新修改镜像内部的东西)123docker run --name myjenkins -i -t -p 8080:8080 -p 50000:50000 -v /Users/smalldok/work/idea-work/docker/volume/jenkins_home:/var/jenkins_home jenkins:2.60.3 /bin/bash然后exit退出、commit来重新创建镜像副本 或者后台方式运行(推荐)1docker run --name myjenkins -d --env JAVA_OPTS=&quot;-Xmx8192m&quot; -p 8080:8080 -p 50000:50000 -v /Users/smalldok/work/idea-work/docker/volume/jenkins_home:/var/jenkins_home jenkins:2.60.3 1docker run --name myjenkins -d --env JAVA_OPTS=&quot;-Xmx8192m&quot; -p 8080:8080 -p 50000:50000 -v /Users/smalldok/work/idea-work/docker/volume/jenkins_home:/var/jenkins_home -e ROOT_PASS=&quot;jenkins&quot; jenkins:2.60.3 jenkins解锁启动后访问http://127.0.0.1:8080/ 出现Unlock Jenkins页面，按提示在jenkins容器中获取密码；命令行进入容器:123docker ps -ldocker exec -it 91e4b117c7f0 /bin/bashcat /var/jenkins_home/secrets/initialAdminPassword 以root用户进入容器1docker exec -it -u root myjenkins /bin/bash 制作Dockerfile验证jenkins plugin安装jenkins plugin list Organization and Administrationdashboard-viewcloudbees-folderantisamy-markup-formatterbackup Build Featurestoken-macrobuild-timeoutcredentials-bindingws-cleanuptimestamper Build Toolsnodejsbuild-name-setterscm-api 源码管理工具 Build Analysis and Reporting无 Pipelines and Continuous Deliverybuild-pipeline-pluginparameterized-triggercopyartifact Source Code Managementgit-clientgitgitlab-plugingithub-pullrequest User Management and Securitymatrix-authpam-authldaprole-strategy Notifications and PublishingPublish Over SSHssh-credentialsSSHMaileremail-ext project and viewnested-viewsectioned-view dockerdocker-commonsdocker-build-publishdocker-pluginkubernetes jenkins plugin.txt查找或下载插件 https://plugins.jenkins.io/123456789101112131415161718192021222324252627282930313233343536373839404142dashboard-view:latestcloudbees-folder:latestantisamy-markup-formatter:latestbackup:latesttoken-macro:latestbuild-timeout:latestcredentials-binding:latestws-cleanup:latesttimestamper:latestnodejs:latestbuild-name-setter:latestscm-api:latestbuild-pipeline-plugin:latestparameterized-trigger:latestcopyartifact:latestgit-client:latestgit:latestgitlab-plugin:latestgithub-pullrequest:latestmatrix-auth:latestpam-auth:latestldap:latestrole-strategy:latestpublish-over-ssh:latestssh-credentials:latestssh:latestmailer:latestemail-ext:latestnested-view:latestsectioned-view:latestdocker-commons:latestdocker-build-publish:latestdocker-plugin:latestkubernetes:latest 验证插件安装 进入容器命令行 1docker exec -it -u root myjenkins /bin/bash 准备plugins.txt 12cd /usr/share/jenkins/vi plugins.txt 执行插件安装脚本 1/usr/local/bin/install-plugins.sh $(cat /usr/share/jenkins/plugins.txt | tr &apos;\n&apos; &apos; &apos;) 单个失败，则重新安装,如：/usr/local/bin/install-plugins.sh jira-trigger:latest安装完成后，查看/usr/share/jenkins/ref/plugins目录； 登录jenkins 控制台页面查看http://127.0.0.1:8080/pluginManager/installed验证JDK、GIT、SSH安装JDK，默认在jenkins基础镜像中已经安装，见官方Dockerfile: https://github.com/jenkinsci/docker/blob/master/DockerfileGIT，默认在上面安装jenkins插件git时已经安装;SSH，默认在上面安装jenkins插件ssh时已经安装;123java -versiongit --versionssh 验证maven安装1234567891011121314151617181920212223242526apt-get update &amp;&amp; apt-get install -y wget# get maven 3.5.3wget --no-verbose -O /tmp/apache-maven-3.5.3.tar.gz http://archive.apache.org/dist/maven/maven-3/3.5.3/binaries/apache-maven-3.5.3-bin.tar.gz# verify checksumecho &quot;51025855d5a7456fc1a67666fbef29de /tmp/apache-maven-3.5.3.tar.gz&quot; | md5sum -c# install maventar xzf /tmp/apache-maven-3.5.3.tar.gz -C /opt/ln -s /opt/apache-maven-3.5.3 /opt/mavenln -s /opt/maven/bin/mvn /usr/local/binrm -f /tmp/apache-maven-3.5.3.tar.gz# envvi ~/.bashrcMAVEN_HOME=&quot;/opt/maven&quot;PATH=&quot;$PATH:$MAVEN_HOME&quot;export MAVEN_HOMEexport PATH保存wq! 然后生效source ~/.bashrc#check MAVENmvn -vecho $MAVEN_HOME 验证nodejs安装12345678910111213# get nodejs 9.9.0wget --no-verbose -O /tmp/node-v9.9.0-linux-x64.tar.gz https://nodejs.org/dist/v9.9.0/node-v9.9.0-linux-x64.tar.gz# install nodejstar xzf /tmp/node-v9.9.0-linux-x64.tar.gz -C /opt/ln -s /opt/node-v9.9.0-linux-x64 /opt/nodejsln -s /opt/nodejs/bin/node /usr/local/bin/nodeln -s /opt/nodejs/bin/npm /usr/local/bin/npmrm -f /tmp/node-v9.9.0-linux-x64.tar.gz#check nodejsnode -vnpm -v Dockerfile1234567891011121314151617181920212223242526272829303132333435# 基础镜像,基于官方jenkins2.60.3,包括openJDK8# 见 https://github.com/jenkinsci/docker/blob/master/Dockerfile## 预安装jenkins各种插件maven/git/nodejs/docker plugin等等,详见: resources/plugins.txt# 预安装maven、git、nodejs、SSH环境## github: https://github.com/smalldok/jenkins+maven-git-nodejs-dockerFROM jenkins:2.60.3MAINTAINER smalldok yaojialing5566@gmail.com# Install Jenkins PluginsCOPY resources/plugins.txt /usr/share/jenkins/plugins.txtRUN /usr/local/bin/install-plugins.sh $(cat /usr/share/jenkins/plugins.txt | tr &apos;\n&apos; &apos; &apos;)# Install mavenUSER rootRUN apt-get update &amp;&amp; apt-get install -y wget# get maven 3.5.3RUN wget --no-verbose -O /tmp/apache-maven-3.5.3.tar.gz http://archive.apache.org/dist/maven/maven-3/3.5.3/binaries/apache-maven-3.5.3-bin.tar.gz# verify checksumRUN echo &quot;51025855d5a7456fc1a67666fbef29de /tmp/apache-maven-3.5.3.tar.gz&quot; | md5sum -c# install mavenRUN tar xzf /tmp/apache-maven-3.5.3.tar.gz -C /opt/ \ &amp;&amp; ln -s /opt/apache-maven-3.5.3 /opt/maven \ &amp;&amp; ln -s /opt/maven/bin/mvn /usr/local/bin \ &amp;&amp; rm -f /tmp/apache-maven-3.5.3.tar.gzENV MAVEN_HOME /opt/maven# Switch back to Jenkins userUSER jenkins Build image1234567891011121314151617# builddocker build -t smalldok/jenkins:1.0 .# check imagedocker images# rundocker run --name myjenkins -d --env JAVA_OPTS=&quot;-Xmx8192m&quot; -p 8080:8080 -p 50000:50000 -v /Users/smalldok/work/idea-work/docker/volume/jenkins_home:/var/jenkins_home -e ROOT_PASS=&quot;jenkins&quot; smalldok/jenkins:1.0# open consolehttp://127.0.0.1:8080/# stop containerdocker stop myjenkins# delete containerdocker rm myjenkins Push image镜像上传阿里云： 注册阿里云docker registry https://cr.console.aliyun.com 在账户中创建命名空间、镜像仓库 登录阿里云docker registry 1docker login --username=[用户名] registry.cn-hangzhou.aliyuncs.com 登录registry的用户名是您的阿里云账号全名，密码是您开通服务时设置的密码。你可以在镜像管理首页点击右上角按钮修改docker login密码。 将镜像推送到registry123$ docker login --username=[用户名] registry.cn-hangzhou.aliyuncs.com$ docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/smalldok/jenkins:[镜像版本号]$ docker push registry.cn-hangzhou.aliyuncs.com/smalldok/jenkins:[镜像版本号] 镜像上传docker官方(被墙) 注册Hub账号：https://hub.docker.com/ 在账户中创建仓库：Create Repository login hub 1docker login upload 1docker push smalldok/jenkins:1.0 其他命令 12# 修改镜像tagdocker tag &lt;imageID&gt; &lt;namespace&gt;/&lt;image name&gt;:&lt;version tag eg latest&gt; 常见问题问题1删除容器，会删除-v /your/home:/var/jenkins_home指定挂载目录下的数据？不会，例如：执行docker rm myjenkins,并不会删除宿主机/your/home下的数据 问题2构建Image Size过大问题？一条指令构建一层layer，应该减少层即可减少体积;123RUN aaaaaRUN bbbbbRUN ccccc 应尽量改为：123RUN aaaaa \ &amp;&amp; bbbbb \ &amp;&amp; ccccc 问题3docker: Error response from daemon: error creating overlay mount to /Users/smalldok/install/docker/docker-root-dir/overlay2/517ba0e3473b5196356d2c970322371964dff4c8803acc88421e71a34aabe783-init/merged: invalid argument.不要修改docker root dir，还原为默认的/var/lib/docker下 问题4：ocker: Error response from daemon: Conflict. The container name “/myjenkins” is already in use by container “e4c7f088c941063dfbea65c65ec683978dab2d309fa252a9f735594f6bb481d9”. You have to remove (or rename) that container to be able to reuse that name.主机上运行docker rm xxxx 删除容器 问题5：在容器中执行vi xxx.txt 报错 vi: command not found；在使用docker容器时，有时候里边没有安装vim，敲vim命令时提示说：vim: command not found，这个时候就需要安装vim，可是当你敲apt-get install vim命令时，提示：1234Reading package lists... DoneBuilding dependency tree Reading state information... DoneE: Unable to locate package vim 这时候需要敲：apt-get update，这个命令的作用是：同步 /etc/apt/sources.list 和 /etc/apt/sources.list.d 中列出的源的索引，这样才能获取到最新的软件包。等更新完毕以后再敲命令：apt-get install vim命令即可。]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-retry 破坑]]></title>
    <url>%2Fspring-retry%20%E7%A0%B4%E5%9D%91%2F</url>
    <content type="text"><![CDATA[使用场景A接口 -&gt; B接口 调用失败，重试、重试失败降级处理；如：实现第三方的app Push推送；如：消息系统推送消息给订阅方； pom.xml项目基于springboot，在根pom.xml中引用，所以这里没有&lt;version&gt;，以springboot中的版本为准;项目根pom.xml123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.0.RELEASE&lt;/version&gt; &lt;relativePath/&gt;&lt;/parent&gt; 项目modepom.xml123456789&lt;!-- spring-retry --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;&lt;/dependency&gt; spring-retry知识123456@Retryable(value= &#123;RemoteAccessException.class&#125;,maxAttempts = 3,backoff = @Backoff(delay = 1000l,multiplier = 1))public void call(String param)&#123;&#125;@Recoverpublic void recover(RemoteAccessException e) &#123;&#125; @Retryablevalue选择需要重试的异常，可指定多个；maxAttempts 最大重试次数,超过则调用@Recover指定的方法(根据异常类型)delay 每次重试的间隔时间 @Recover当超过重试次数，则调用此注解的方法(根据异常类型) 使用原则：不要全部异常都进行重试，要有选择性的根据异常重试； 两种使用方式我目前使用的第一种方式，因为业务比较简单； 1、@Retryable和@Recover在同一个类中@EnableRetry启动配置1234@Configuration@EnableRetryclass RetryConfig&#123;&#125; 业务类RemoteService.java123456789101112@Servicepublic class RemoteService &#123; @Retryable(value= &#123;RemoteAccessException.class&#125;,maxAttempts = 3,backoff = @Backoff(delay = 1000l,multiplier = 1)) public void call(String param)&#123; System.out.println("do something..."); throw new RemoteAccessException("RPC调用异常"); &#125; @Recover public void recover(RemoteAccessException e) &#123; System.out.println("recover====&gt;"+e.getMessage()); &#125;&#125; 约束：1、@Retryable 和 @Recover必须在同一个类中2、这个类必须是受spring管理的bean3、方法必须是public 2、拦截器@EnableRetry启动配置,且加上拦截器123456789101112131415161718192021222324@Configuration@ConditionalOnProperty(prefix = "mq.consumer.callback", name = &#123;"retry-times","retry-delay-inMilliseconds"&#125; ,matchIfMissing = false)@EnableRetrypublic class RetryConfig&#123; @Autowired private ConsumerProperties consumerProperties; //每一个业务方法对应一个拦截器和自定义recover @Bean @ConditionalOnMissingBean(name = "retryInterceptor") public RetryOperationsInterceptor retryInterceptor() &#123; return RetryInterceptorBuilder .stateless().recoverer(new CustomMessageRecover()) .backOffOptions(0L,1D, consumerProperties.getCallback().getRetryDelayInMilliseconds()) .maxAttempts(consumerProperties.getCallback().getRetryTimes()).build(); &#125; static class CustomMessageRecover implements MethodInvocationRecoverer&lt;Void&gt; &#123; @Override public Void recover(Object[] args, Throwable cause) &#123; System.out.println("IN THE RECOVER ZONE!!!"); return null; &#125; &#125;&#125; 业务类RemoteService.java12345678@Servicepublic class RemoteService &#123; @Retryable(value= &#123;RemoteAccessException.class&#125;,interceptor = "retryInterceptor") public void call(String param)&#123; System.out.println("do something..."); throw new RemoteAccessException("RPC调用异常"); &#125;&#125; 注意到没？@Recover注解的方法不在里面了；已经由拦截器代理到CustomMessageRecover类]]></content>
      <categories>
        <category>服务治理</category>
      </categories>
      <tags>
        <tag>重试</tag>
        <tag>降级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[h2客户端工具DbVisualizer]]></title>
    <url>%2Fh2%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B7%A5%E5%85%B7DbVisualizer%2F</url>
    <content type="text"><![CDATA[引言 一般springboot中启动h2，即可通过网页访问控制台，来操作H2；如，http://localhost:9088/xxxserver/h2-console但是要启动项目才能访问，极大的不方便；因此需要第三方工具直接打开mychannel.mv.db文件，像在console上操作一样； 准备 工具版本：DbVisualizer 9.0.7网上有破解的，它还可连接所有主流关系型数据库(我日常就用的它) 项目中使用的h2版本：h2-1.4.193 使用 假设mychannel.mv.db文件在/aaa/bbb目录=&gt; /aaa/bbb/mychannel.mv.db 打开DbVisualizer向导,创建一个H2连接：Database Type: H2Driver(JDBC)： H2 embeddedDatabase filename: /aaa/bbb/mychannelDatabase Userid: 项目里面设置的用户名Database Password: 项目里面设置的密码 注意事项 DbVisualizer 的H2驱动问题DbVisualizer 自己会下载数据库驱动，但自带的h2驱动为 1.3 的版本，目前最新h2版本为 1.4 （因为1.3版本默认是 dbname.h2.db 形式的，用1.3驱动无法正确打开最新的 dbname.mv.db 形式的数据库文件，因为默认后缀不同，连接URL带上数据库名，dbvisualizer 会自动认为是 dbname.h2.db 由于文件不存在，所以变成是新建一个 dbname.h2.db 数据库文件了） 解决办法把最新版本的h2的jar包(本地maven仓库中拿)，复制到 DbVisualizer 安装目录下的 DbVisualizer\jdbc\h2 中，把原有目录中的h2.jar 删掉，把最新版本的h2 jar 包命名为 h2.jar 替换原有的。这样就能识别新版本的h2，打开已经创建了的 *.mv.db 了]]></content>
      <categories>
        <category>嵌入式数据库</category>
      </categories>
      <tags>
        <tag>H2</tag>
        <tag>嵌入式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot-webjars集成前端UI]]></title>
    <url>%2Fspringboot-webjars%E9%9B%86%E6%88%90%E5%89%8D%E7%AB%AFUI%2F</url>
    <content type="text"><![CDATA[什么是WebJarsWebJars是将客户端（浏览器）资源（JavaScript，Css等）打成jar包文件，以对资源进行统一依赖管理。WebJars的jar包部署在Maven中央仓库上。 为什么使用我们在开发Java web项目的时候会使用像Maven，Gradle等构建工具以实现对jar包版本依赖管理，以及项目的自动化管理，但是对于JavaScript，Css等前端资源包，我们只能采用拷贝到webapp下的方式，这样做就无法对这些资源进行依赖管理。如果我们将这些前端资源打成jar包，我们就可以进行依赖管理。 我们经常看到类似如下这种带管理后台的第三方工具，在springboot中这么使用： springboot 处理静态资源知识Spring Boot 默认将 /** 所有访问映射到以下目录：1234classpath:/META-INF/resourcesclasspath:/resourcesclasspath:/staticclasspath:/public 优先级顺序为：META/resources &gt; resources &gt; static &gt; public 如果UI页面不放在这些默认目录，则需要在springboot webconfiguration中配置资源映射(本文就是采用资源映射方式)； 参考：http://www.cnblogs.com/magicalSam/p/7189476.htmlhttps://github.com/codecentric/spring-boot-admin/blob/master/spring-boot-admin-server/src/main/java/de/codecentric/boot/admin/config/AdminServerWebConfiguration.java 找个web前端工程 一个管理后台前端项目http://panjiachen.github.io/vueAdmin-templatefork到我自己的githubhttps://github.com/smalldok/vueAdmin-template.git 在本地git clone https://github.com/smalldok/vueAdmin-template.git 进入vueAdmin-template目录，新建pom.xml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.github&lt;/groupId&gt; &lt;artifactId&gt;xxx-admin-server-ui&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.7&lt;/java.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;yarn-install&lt;/id&gt; &lt;phase&gt;validate&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;exec&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;executable&gt;yarn&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;install&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;yarn-build&lt;/id&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;exec&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;executable&gt;yarn&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;run&lt;/argument&gt; &lt;argument&gt;build&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- &lt;execution&gt; &lt;id&gt;yarn-test&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;exec&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;executable&gt;yarn&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;run&lt;/argument&gt; &lt;argument&gt;test&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; --&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;nonFilteredFileExtensions&gt; &lt;nonFilteredFileExtension&gt;woff&lt;/nonFilteredFileExtension&gt; &lt;nonFilteredFileExtension&gt;ttf&lt;/nonFilteredFileExtension&gt; &lt;nonFilteredFileExtension&gt;woff2&lt;/nonFilteredFileExtension&gt; &lt;nonFilteredFileExtension&gt;eot&lt;/nonFilteredFileExtension&gt; &lt;nonFilteredFileExtension&gt;swf&lt;/nonFilteredFileExtension&gt; &lt;nonFilteredFileExtension&gt;ico&lt;/nonFilteredFileExtension&gt; &lt;/nonFilteredFileExtensions&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;dist&lt;/directory&gt; &lt;targetPath&gt;META-INF/xxx-admin-server-ui&lt;/targetPath&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;/project&gt; 安装jdk1.8、maven3（略） 安装nodejs（略） 安装yarn1.执行命令npm -i -g yarn2.查看当前的源，默认是：官网 ,yarn config get registry3.改成taobao的源,yarn config set registry https://registry.npm.taobao.org 执行mvn install不出意外，会在本地maven仓库中生成xxx-admin-server-ui-1.0.0-SNAPSHOT.jar; 以上步骤，最好都在mac下操作 springboot 集成前端http://start.spring.io创建一个springboot项目，名字叫xxx-admin-server pom.xml配置 web config 启动项目，访问http://127.0.0.1:8080/，最终效果]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>springboot</tag>
        <tag>后台管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端开发工具之vscode安装配置]]></title>
    <url>%2F%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%B9%8Bvscode%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前端的一些开发工具vscode atom sublime text webstorm mac vscode 安装官网下载安装即可，略….我安装后的效果 插件 vscode-icon项目工程文件加上漂亮图标，方便识别，必备良品！ Debugger for Chrome映射 chrome 的 debug功能，静态页面都可以用 vscode 来打断点调试 vscode-fileheader添加头部注释：作者、创建时间、最新修改时间;1、设置头注释，打开.vscode/settings.json，查找fileheader2、插入头注释快捷键ctrl+alt+i beautify格式化html，css，js HTML Snippets可以在不输入 &lt; 的情况下提示 Git History显示git log和line history 更多插件参考：vscode 插件推荐 - 献给所有前端工程师 http://www.cnblogs.com/xwwin/p/6247765.html vscode 配置备份这么多配置，不备份怎么行 方式一：备份到github gist github 上生成token，然后复制token 上传备份vscode中按shift+alt +u 在弹出窗里输入你的token，然后会生成syncSummary.txt文件在窗口中打开； 下载备份打开syncSummary.txt文件，找到sync.gist值；vscode中按shift+alt +d，在弹出窗里输入这个值； 参考文章：vscode同步设置&amp;扩展插件 方式二：备份github或者git oschina(分支形式) 在github或者git oschina创建一个项目https://xxxx/smalldok/mybackup.git 命令行，进入vscode安装目录 备份git initgit add .git commit -m “xxxxxxx”git checkout -b mac-vscode1.14.2git remote add origin https://xxxx/smalldok/mybackup.gitgit push origin mac-vscode1.14.2 恢复git pull –rebase origin mac-vscode1.14.2 方式二也是我自己备份电脑上各种配置的做法]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins+springboot]]></title>
    <url>%2Fjenkins-springboot%2F</url>
    <content type="text"><![CDATA[1、环境2、jenkins服务器与部署服务器SSH打通3、jenkins配置4、部署服务器配置 附：服务端小组协作流程图 环境 服务器 安装 描述 jenkins服务器 jenkins2.60.1 - - jdk1.8 - - git2.1.4 - - maven3 - 部署服务器 centos6.6 7有BUG - jdk1.8 - - openssh-server - github项目例子 https://github.com/smalldok/springboot-jenkins - jenkins服务器与部署服务器SSH打通主要是jenkins中可以远程执行部署服务器的SHELL脚本； 部署服务器 启动SSH服务1234567rpm -qa |grep ssh #检查是否装了SSH包#没有的话yum install openssh-serverchkconfig --list sshd #检查SSHD是否在本运行级别下设置为开机启动chkconfig --level 2345 sshd on #如果没设置启动就设置下.service sshd restart #重新启动netstat -antp |grep sshd #看是否启动了22端口.确认下.iptables -nL #看看是否放行了22口. jenkins服务器 测试远程执行SHELL1234567#jenkins服务器上生产RSA公钥私钥对#jenkens_home目录中的.ssh目录中运行命令ssh-keygen -t rsa #一直回车ssh-copy-id -i smalldok@172.17.0.2 #传输公钥给需要访问的服务器#jenkins服务器上测试：ssh smalldok@172.17.0.2 "/home/smalldok/test.sh" test.sh12#!/bin/shecho "&gt;&gt;&gt;&gt;&gt;&gt;&gt;" jenkins 配置安装插件jenkins 系统管理-&gt;插件管理安装maven-intestrating-plugin(新建maven构建项目用)；安装Role-based Authorization Strategy(用户角色权限设置)； jdk/git/maven设置jenkins 系统管理-&gt;Global Tool Configuration分别设置jdk、git、maven 项目构建配置见https://github.com/smalldok/springboot-jenkins.git 创建用户、分配权限、视图jenkins权限管理，实现不同用户组显示对应视图views中不同的jobs ；场景：隔离开发环境、测试环境、生产环境；开发人员构建开发环境的项目；测试人员构建测试环境的项目；运维人员构建生产环境的项目； 视图 All dev-A项目 dev-B项目 test-B项目 - dev-MyDemo - - - - dev-xxServer - - - - test-xxServer 即：开发人员登录jenkins只能看到dev-A项目、dev-B项目视图；测试人员登录jenkins只能看到test-B项目视图； admin All视图dev-A项目视图这个视图下，存放A项目的开发构建环境dev-B项目视图这个视图下，存放B项目的开发构建环境test-B项目视图这个视图下，存放B项目的测试构建环境 分别创建如上三个视图把项目加入到相关的视图中；开发的项目前缀，以dev-开头；开发的项目前缀，以test-开头； 创建用户（乔治、佩奇） jenkins 系统管理-&gt;Manage and Assign Roles-&gt;Manage Rolesjenkins 系统管理-&gt;Manage and Assign Roles-&gt;Assign Roles 佩奇登录jenkins的效果乔治登录jenkins的效果 参考：http://blog.sina.com.cn/s/blog_8d8f99440101hpzb.html 配置备份配置目录（jenkins_home）加入git版本库 部署服务器配置目标服务器上，提前建好几个目录/home/smalldok/build/home/smalldok/backcup/home/smalldok/release 还需要一个deploy的shell见-&gt; https://github.com/smalldok/springboot-jenkins.git 服务端小组协作流程图]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IaaS-PaaS-SaaS区别]]></title>
    <url>%2FIaaS-PaaS-SaaS%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Infrastructure（基础设施）-as-a- Service，Platform（平台）-as-a-Service，Software（软件）-as-a-Service。基础设施在最下端，平台在 中间，软件在顶端; 如果你是一个网站站长，想要建立一个网站。不采用云服务，你所需要的投入大概是：买服务器，安装服务器软件，编写网站程序。现在你追随潮流，采用流行的云计算，如果你采用IaaS服务，那么意味着你就不用自己买服务器了，随便在哪家购买虚拟机，但是还是需要自己装服务器软件而如果你采用PaaS的服务，那么意味着你既不需要买服务器，也不需要自己装服务器软件，只需要自己开发网站程序如果你再进一步，购买某些在线论坛或者在线网店的服务,这意味着你也不用自己开发网站程序，只需要使用它们开发好的程序，而且他们会负责程序的升级、维护、增加服务器等，而你只需要专心运营即可，此即为SaaS。 IaaS：阿里云 亚马逊云 腾讯云PaaS：新浪云 百度云]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Iaas</tag>
        <tag>PaaS</tag>
        <tag>SaaS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2Ftest%2F</url>
    <content type="text"><![CDATA[test…. MPush开发手册]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[SPI定制化]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%2FSPI%E5%AE%9A%E5%88%B6%E5%8C%96%2F</url>
    <content type="text"><![CDATA[什么是SPI建议直接百度 java spi, 官方介绍: http://docs.oracle.com/javase/tutorial/sound/SPI-intro.html mpush 使用SPI的目的肯定是为了扩展性，在不想修改源码的情况下，去替换系统原有的实现，代价最小也最灵活。 mpush目前支持的SPI列表所有受支持的SPI都在mpuhs-api模块的com.mpush.api.spi包下面。 ServerEventListenerFactory 监听Server 产生的相关事件 BindValidatorFactory 绑定用户身份是校验用户身份是否合法 ClientClassifierFactory 同一个用户多设备登录互踢策略，默认Android和ios会互踢 PushHandlerFactory 接收客户端发送到服务端的上行推送消息 ExecutorFactory 整个系统的线程池工厂，可以按自己的策略创建线程池 CacheManagerFactory 系统使用的缓存实现，默认是redis，支持自定义 MQClientFactory 系统使用的MQ实现，默认是redis的pub/sub，支持自定义 ServiceRegistryFactory 服务注册组件，默认实现是zookeeper，支持自定义 ServiceDiscoveryFactory 服务发现组件，默认实现是zookeeper，支持自定义 PushListenerFactory 消息来源自定义，默认使用Gateway模块，支持自定义，比如使用MQ 开发流程 新建一个maven工程，并加入mpush-api模块的依赖 实现要定制的接口和其对应的Factory 在resources目录下创建META-INF.services目录 创建名字为com.mpush.api.spi.xxx.XXXFactory的文件，即对应的接口名 文件的内容为实现类的全名(packageName+className) 通过mvn打成jar包，并把其放到mpush/lib目录 重启mpush server 就会优先加载用户提供的实现来覆盖原有的默认实现]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>架构分析</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息类型和消息协议]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%2F%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B%E5%92%8C%E6%B6%88%E6%81%AF%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[消息类型1234567891011121314151617181920212223242526public enum Command &#123; HEARTBEAT(1), // 心跳 HANDSHAKE(2), // 握手 LOGIN(3), LOGOUT(4), BIND(5), // 绑定用户 UNBIND(6), // 解绑用户 FAST_CONNECT(7), // 快速重连 PAUSE(8), RESUME(9), ERROR(10), // 错误消息 OK(11), // 成功消息 HTTP_PROXY(12), // HTTP代理 KICK(13), // 踢人 GATEWAY_KICK(14), PUSH(15), // 推送 GATEWAY_PUSH(16), NOTIFICATION(17), GATEWAY_NOTIFICATION(18), CHAT(19), GATEWAY_CHAT(20), GROUP(21), GATEWAY_GROUP(22), ACK(23), UNKNOWN(-1);&#125; 消息协议 mpush使用的为自定义私有协议，定长Header + body, 其中header部分固定13个字节。 心跳固定为一个字节，值为 -33。 名称 类型 长度 说明 length int 4 表示body的长度 cmd byte 1 表示消息协议类型 checkcode short 2 是根据body生成的一个校验码 flags byte 1 表示当前包启用的特性，比如是否启用加密，是否启用压缩 sessionId int 4 消息会话标识用于消息响应 lrc byte 1 纵向冗余校验，用于校验header body byte 不固定 消息内容]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>架构分析</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模块依赖关系]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%2F%E6%A8%A1%E5%9D%97%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[mpush-client：服务端SDK,主要提供发送Push的接口给其他业务使用，比如MPNS mpush-boot：是服务端启动入口模块，主要控制server启动、停止流程 mpush-api：定义了mpush相关核心接口及协议，还包括对外暴露的SPI接口 mpush-netty：主要提供netty相关的一些基础类，像NettyServer,NettyClient mpush-tools：mpush用到的一些工具类，包括线程池，加密，配置文件解析等等 mpush-zk：zookeeper的client, 包括path的定义，节点定义，数据监听等 mpush-cache：redis缓存模块，支持单机模式和3.x集群模式，包括用户路由，上下线消息等 mpush-common：定义了mpush-client模块和mpush-core模块都会用到的类，主要是消息、路由等 mpush-core：sever核心模块，包括接入服务，网关服务，路由中心，推送中心等等 mpush-monitor：服务监控模块主要监控JVM，线程池，JMX，服务状态统计，性能统计等]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>架构分析</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度进阶-加解密]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%2F%E6%B7%B1%E5%BA%A6%E8%BF%9B%E9%98%B6-%E5%8A%A0%E8%A7%A3%E5%AF%86%2F</url>
    <content type="text"><![CDATA[深度进阶-加解密1、数据交换过程2、RSA 加密解密3、AES 加密解密4、客户端-服务端-MPNS加密交互5、客户端-服务端-握手交互 数据交换过程Mpush方案1、算法上同时使用了非对称加密算法(RSA)和对称加密算法(AES)。2、在客户端预埋好由服务端生成好的公钥。3、客户端生成随机数R1，并通过RSA公钥加密后发送给服务端。4、服务端用RSA私钥解密客户端发送的数据取得R1,同时生成随机数R2,并以R1为Key使用AES加密R2然后把加密后的数据发送到客户端。5、客户端以R1为Key使用AES解密服务发送的数据取得R26、此时客户端和服务的同时都拥有随机数R1、R2，然后使用相同的混淆算法生成会话密钥(sessionKey), 之后传输的数据都以此值作为AES加密的密钥。参考：http://mpush.mydoc.io/?t=134350 密钥交换章节 其他方案针对日志的加密方案，我们充分考虑了对称加密和非对称加密各自的优缺点：对称加密加密与解密使用的是同样的密钥，所以速度快，但由于需要将密钥在网络传输，所以安全性不高；非对称加密使用了一对密钥，公钥与私钥，所以安全性高，但加密与解密速度慢。虽然非对称加密很安全，但是和对称加密比起来，它非常的慢，所以还是要用对称加密来传送报文，但对称加密所使用的密钥可以通过非对称加密的方式发送出去。流程如下： APP 端首先生成了一个随机数作为对称密钥。 APP 端向服务端请求公钥。 服务端将带版本号的公钥发送给 APP 端。 APP 端使用该版本的公钥将自己的对称密钥加密。 APP 端将加密后的对称密钥（带上公钥版本号）发送给服务端。 服务端使用对应版本号的私钥解密得到 APP 端的对称密钥。 APP 端与服务端可以使用对称密钥来对沟通的内容进行加密与解密了。 参考：https://mp.weixin.qq.com/s/iUwqQxx87qQeRecjGxwTtg 《苏宁用户行为采集体系的演变》 RSA 加密解密mpush工程 -&gt; mpush-tools模块RSAUtilsTest.java mpush工程 -&gt; mpush-test模块RsaTest.java AES 加密解密mpush工程 -&gt; mpush-tools模块AESUtilsTest.java 客户端-服务端-MPNS加密交互1、客户端向服务端下发消息，会加密；（参考工程mpush-client-java的MPushClient类）2、服务端向客户端返回消息，不会加密；（参考mpush工程，mpush-client模块的ConnClientChannelHandler）（参考mpush工程，mpush-core模块的ConnectionServer中各个*handler#handle方法）3、MPNS向服务端下发消息，会加密；4、服务端向MPNS返回消息，不会加密；5、服务端向客户端转发MPNS的消息，会加密； 总结：请求消息加密，响应消息不加密 客户端-服务端-握手交互 服务端生成好RSA公钥私钥，并将公钥串给客户端的开发人员1、客户端生成随机数R1，并通过RSA公钥加密后发送给服务端。消息发送前，会经过编码、然后判断SessionContext中是否设置加解密类，如果设置了加解密类，则对消息体加密；2和3、服务端用RSA私钥解密客户端发送的数据取得R1,同时生成随机数R2,并以R1为Key使用AES加密R2然后把加密后的数据发送到客户端。2 取得R1服务端ConnectionServer与客户端建立连接时(channelActive)，会设置SessionContext上下文中的加解密类为RSA； 服务端ConnectionServer接收到客户端的握手消息，找到HandshakeHandler处理类，然后调用其父类中BaseMessageHandler#handle()方法 上面t.decodeBody()方法，是调用其父类BaseMessage#decodeBody方法；然后decodeBody方法会调用到decodeBinaryBody0方法，进行解码操作；解密、解压之后，开始解码得到客户端传过来的R13 生成随机数R2,并以R1为Key使用AES加密R2然后把加密后的数据发送到客户端。 4、客户端以R1为Key使用AES解密服务端发送的数据，取得R25、此时客户端和服务的同时都拥有随机数R1、R2，然后使用相同的混淆算法生成会话密钥(sessionKey), 之后传输的数据都以此值作为AES加密的密钥。6、绑定用户请求的消息体(BindUserMessage)用AES加密，密钥为客户端生成的sessionKey如上图所示，在调用binUser(clientConfig)前，已经设置上下文中加解密类为AES，且密钥为sessionKey;先对消息进行编码，然后压缩、加密消息体 7、服务端接收到消息，用服务端生成的sessionKey解密消息体在服务端处理握手响应时(HandshakeHandler#handle)，已将sessionKey作为密钥的AES实例设置到conn中的sessionContext中； 故，服务端处理后续的绑定用户请求时，session中的加解密器key是sessionKey，所以在最后decode解码时会拿到sessionKey作为密钥的AES实例，然后进行解密； 8、绑定用户请求的响应消息体(OkMessage)用AES加密，使用SessionContext中的sessionKey作为密钥的AES实例加密;此步骤有错误，返回给客户端的消息体(OkMessage)，并没有加密； 貌似OKMessage消息没有经过加密，直接是编码然后返回给客户端了； ConnectServer接收客户端PUSH时，返回给客户端的AckMessage消息也是没有经过加密；没有加密的消息，还有ErrorMessage、FastConnectOkMessage、HttpResponseMessage； GatewayServer接收MPNS的PUSH时，然后转给客户端的PushMessage消息会加密； GatewayServer返回给MPNS的ErrorMessage、OKMessage消息没有经过加密；]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>架构分析</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
        <tag>加解密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mpush源码阅读总结]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总结 通盘阅读其源码，可以了解端到云、云到端的消息推送流程和实现细节； 有客户端DEMO(安卓/IOS/WEB端) 、服务端DEMO、业务端DEMO 实现了TCP服务、websocket服务、HTTP代理服务(客户端通过http下发消息) 定义了统一的生命周期组件，统一组件的初始化、启动、关闭例如：NettyTCPServer extends BaseService SPI扩展，支持指定名称的单个加载、多个加载时排序取第一个 流控 GlobalFlowControl #单进程全局流控，对MPNS发送给网关的流控(单进程) RedisFlowControl #redis 流控(多进程)，用于限制广播消息到客户端的数量（这里代码实现不安全） FastFlowControl #快速流控(单进程)，用于限制广播消息到客户端的数量（这里代码实现不安全） ExactFlowControl #分段流控，分段统计，把1s 分成 100份，10ms一份，限制10ms内允许的最大数量 配置集中管理 监控，实现了JVM、各个线程池指标的监控 性能监控，利用Profiler做方法耗时的埋点打印 时间线，通过TimeLine，可以清晰知道msg流转到哪个组件，有点像trace，只不过它是单进程的 线程池，定义了5种线程池，包括MQ/event-bus/push-client-timer/push-task-timer/ack-timer 事件总线，使用Guava的EventBus，方便其他组件订阅建连、断连、异常事件 不足网络组件不够抽象，与消息推送的实现逻辑混合在一起，使得结构不清晰；关于网络组件抽象这一块，可以参考下SOFA-Bolt的实现； SOFABolt 是蚂蚁金融服务集团开发的一套基于 Netty 实现的网络通信框架。 为了让 Java 程序员能将更多的精力放在基于网络通信的业务逻辑实现上，而不是过多的纠结于网络底层 NIO 的实现以及处理难以调试的网络问题，Netty 应运而生。 为了让中间件开发者能将更多的精力放在产品功能特性实现上，而不是重复地一遍遍制造通信框架的轮子，SOFABolt 应运而生。 该产品已经运用在了蚂蚁中间件的微服务 (SOFARPC)、消息中心、分布式事务、分布式开关、以及配置中心等众多产品上https://github.com/alipay/sofa-bolt 通信模型与超时控制通信模型没有抽象出来(oneway/sync/future/callback) 超时控制用了netty的HashedWheelTimer和SchedulerExecutor调度线程池这2种实现；目测之所以采用SchedulerExecutor调度线程池，是因为在客户端代码中只想依赖JDK实现； 可以在站内搜索文章《通信模型与超时控制》 配置集中在一个类里面，虽然集中管理比较方便，但始终感觉不太优雅；不支持扩展通过配置中心动态修改配置； 流控代码实现不太安全]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统架构说明]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%2F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[系统架构说明1、逻辑架构2、服务调用关系3、Mpush server启动模块4、消息传递流程(接入服务/Websocket服务)5、云到端消息推送流程 逻辑架构 1、最左侧三大组件分别是日志系统、监控系统、控制台治理服务 Log System 主要负责业务日志大输出，主要有链接相关日志、推送链路日志、心跳日志、监控日志等 Monitor 主要用作系统状态监控，可用于系统调优，包括jvm内存，线程，线程池，系统堆栈，垃圾回收情况，内存泄漏情况等。 AdminServer主要用于在控制台对单台机器进行控制与探查，比如参看连接数量，在线用户数，取消本级ZK注册，关闭服务等 2、右侧三个分别是ZK服务，配置中心和安全工具箱 ZK Client 主要负责注册长链接ip:port,网关ip:port以及监听各个节点变化，同时增加了缓存 ConfigCenter 是MPUSH Server 配置化的关键，贯穿到各个模块，非常重要 Sercutity Box 主要实现了RSA加密，DES加密，会话密钥生成及Session复用(用于快速重连) 3、Core模块分别是长链接服务，网关服务，Packet编解码及分发模块，Message序列化及处理模块 ConnectServer用于维持和客户端之间的TCP通道，主要负责和客户端交互 GatewayServer用于处理Mpush Server之间的消息交互比如踢人，发送PUSH Packet主要是协议部分的编解码和包的完整性校验，最大长度校验等 PacketReceiver主要负责消息的分发，分发是根据Command来的 Connection/ConnectionManager主要负责链接管理，定时检查链接空闲情况，是否读写超时，如果链接断开发出相应的事件给路由中心去处理 Message部分是整个的业务核心处理了处理消息的序列化，还有压缩、加密等，MessageHandler会根据不同消息独立处理自己所属的业务，主要有：心跳响应、握手及密钥交换、快速重连、绑定/解绑用户、http代理、消息推送等 4、路由中心主要包括：本地路由，远程路由，用户在线管理三大块 LocalRouterManager负责维护用户＋设备与链接(connection)之间的关系 RemoteRouterManager负责维护用户＋设备与链接所在机器IP之间的关系 UserManager主要处理用户上下线事件的广播，以及单台机器的在线用户及数量的维护和查询5、MPUSH的缓存部分，目前只支持Redis,支持双写，主备，hash 等特性 见其官网说明：http://mpush.mydoc.io/?t=134339 服务依赖关系 业务系统是要发送业务消息的服务，所有要推送的消息直接转给MPNS MPNS是我们的业务推送系统，负责消息推送，长链接的检查，离线消息存储，用户打标等 APNS、JPUSH、MPUSH等分别是我们的客户端已经接入的推送系统 MPNS主要是为了隔离业务系统和各种推送系统，用户使用哪个长链接服务，业务系统不需要感知，统一有MPNS去选择、去切换 Alloc负责调度维护MPushServer集群，提供查询可用机器列表的接口，详细参见[Alloc实现] Mpush server启动模块 消息传递流程(接入服务/Websocket服务)接入服务 Websocket服务 云到端消息推送流程]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>架构分析</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度进阶]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%2F%E6%B7%B1%E5%BA%A6%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[深度进阶1、握手及快速重连2、绑定用户3、MPNS 消息推送流程4、HTTP代理5、端到云-发消息6、广播推送与流控7、广播推送条件过滤8、消息漫游(未实现) 握手及快速重连 tcp连接建立后，第一个消息就是握手或快速重连消息。 Handshake的目的是为了生成会话密钥，同时会把生成好的密钥存储到redis，并把key返回给客户端，为快速重连使用 FastConnect是基于Handshake生成的sessionId来配合使用的，目的是为了减少RSA加密的使用次数，特别是网络较差的情况，毕竟RSA加密想对还是比较耗时的，客户端只需把sessionId传给服务端，其就能从redis中取出上次会话信息，恢复到上次握手成功之后的会话状态，这个过程不需要任何加密和密钥交换，相对会比较快速。 绑定用户 握手成功后表示安全通道已经建立，但同时还要给连接设置用户甚至标签，只有这样业务才能更好的去识别用户（没有用户的业务是另外一回事）。 设置用户非常简单，只需把其存储到session即可，但因为要支持集群的，就必须把用户的位置信息（或者叫路由信息）发布出去，让集群里的其他机器能够通过useId来查找用户的位置（在哪台机器），因为客户端的TCP连接在哪台机器，那么和这个客户端的所有数据传输都必须经过这台机器的这个连接！(很多同学会问为什么不把connection存储到redis)。 路由中心有两部分组成：本地路由和远程路由，本地路由数据结构为userId-&gt;connection的map，数据存储在本机内存；远程路由数据结构为userId-&gt;connServer机器ip，数据存储在redis;所以要给一个用户发信息必须先查远程路由，找到用户在哪台机器，然后把消息发给这台机器，让其去本地路由查找connection并通过查找到的TCP连接把消息发给用户。 MessageCenter之前使用的redis提供的pub/sub实现的，也可以自己搭建MQ来实现，最新版踢人已经不再使用pub/sub，而是直接使用udp网关实现。 踢人：之所以会有踢人的情况是根据业务需要来的，有些业务系统是不允许同一个用户在多个设备同时在线的，或者只允许不同类型的终端同时在线，比如QQ,手机和PC可以同时在线，但同一个帐号在两台PC登录时其中一个肯定会被踢下线，mpush的踢人要表达的也是同一个意思。 顺便提一下关于同时在线的策略，或者说端的类型的定义mpush已经支持SPI定制化。 MPNS 消息推送流程1、PushSender是消息推送的入口，它的实现在mpush-client模块属于服务端SDK，主要包含有GatewayClient, RemoteRouterManager; RemoteRouterManager用于定位用户在哪台机器，有没有在线等，而GatewayClient用于把要发送的的业务消息发给用户TCP连接所在的机器。 2、GatewayServer负责接收GatewayClient发送过来的消息，然后到LocalRouterManager查找用户的Connection，然后把消息交由其下发。 3、ConnectionServer 负责维持所有连接到当前机器的客户端连接，所以消息最终还是尤其下发（图比较简单，但能表达核心流程）。 HTTP代理使用场景问题前面有提到过http代理这个东西，但很多人不知道这个东西该怎么用，或者说有什么用？以及在什么场景下使用？ 移动APP通信场景分析从使用的链接情况来看，一般可以分为两大类：TCP长链接，HTTP短链接；长链接用于消息推送或IM等场景，HTTP用于业务数据的查询或修改。虽然不是所有的APP都需要IM功能，但大多应用都需要消息推送功能。为了推送消息，APP必须维持一根长链接，但大部分时间除了心跳这根链接上是没多少消息在传输的，特别是非IM类的APP，因为这类应用并没大量的消息要不停的推送，维持长链接只是为了消息的及时到达，这势必造成了很大的资源浪费！ 解决方案针对上述情况MPUSH提供了Http代理方案，目的一是充分利用push通道，而是提高数据传输效率节省电量，节省流量，提供比http更高的安全性。 实现原理MPushClient 提供了一个叫sendHttp的方法，该方法用于把客户端原本要通过HTTP方式发送的请求，全部通过PUSH通道转发，实现整个链路的长链接化；通过这种方式应用大大减少Http短链接频繁的创建，不仅仅节省电量，经过测试证明请求时间比原来至少缩短一倍，而且MPush提供的还有数据压缩功能，对于比较大的数据还能大大节省流量(压缩率4-10倍)，更重要的是所有通过代理的数据都是加密后传输的，大大提高了安全性！ 使用方式服务端1、修改mpush.conf增加mp.http.proxy-enabled=true启用http代理2、修改mpush.conf增加dns-mapping配置，示例如下123mp.http.dns-mapping=&#123;//域名映射外网地址转内部IP &quot;api.jituancaiyun.com&quot;:[&quot;10.0.10.1:8080&quot;, &quot;10.0.10.2:8080&quot;]&#125; 说明：因为mpush server要做http代理转发，而客户端传过来的一般是域名比如http://api.jituancaiyun.com/get/userInfo.json为了不到公网上再绕一圈建议把mpush server 和业务服务(api.jituancaiyun.com)部署到同一个局域网，并增域名api.jituancaiyun.com到提供该服务的集群机器内网ip之间的一个映射，这样mpush server就可以通过局域网把请求转发到具体到业务服务，效率更高！ 客户端1、设置ClientConfig.setEnableHttpProxy(true)来启用客户端代理。2、通过Client.sendHttp(HttpRequest request)方法来发送请求。AndroidSDK通过com.mpush.android.MPush#sendHttpProxy(HttpRequest request)来发送比较合适。 流程分析 Client代表App业务比如查询用户信息的接口 MPushApiProxy是一个工具类用于负责处理当前请求是使用普通的HTTP还是使用MPush长链接通道，这个类在SDK中说不存在的，是我们公司内部的业务，实现起来也很简单，建议Android工程中增加这么一个角色，而不是到处直接去依赖Mpush的代码，方便以后解耦。 MPushClient这个SDK已经提供，用于把Http协议打包成mpush协议。 HttpProxyHandler包括后面的几个组件都是服务端业务组件。用于接收客户端传过来的请求并反解为Http协议，然后通过DNSMapping找到域名对应的局域网IP，再通过内置的HttpClient，把请求转发给业务WEB服务，并把业务服务的返回值(HttpResponse)打包成MPush协议发送到客户端。 DNSMapping负责通过域名解析成局域网IP，并具有负载均衡以及简单的健康检查功能(针对所配置的WEB服务) HttpClient目前使用的是用Netty实现的全异步的一个HttpClient，负责通过http的方式请求业务服务。 Nginx是业务服务，也可以是Tomcat，特别需要建议的是链接超时时间配置长一些。 为什么要这样实现？为什么要这样实现？因为这样做对原有的业务系统侵入特别低，如果MPushApiProxy这个组件设计的好，对于最两边的业务组件/服务(Client,Nginx)，对请求方式应该是无感知的，这个角色是无法区分到底请求是通过普通的Http方式发送出去的还是通过长链接代理的方式发送的！！！另附上通过Http Proxy 实现双向通信交互图 端到云-发消息接口产生背景作为一个纯粹推送系统，最初是没有发送上行消息接口的只有下行的消息，毕竟MPush不是IM系统。后来在很多同学的要求下，就增加了此接口用于cleint上报消息。 接口的使用此接口和服务端下行Push保持相同的协议，可以互发消息，默认服务端没有对消息做任何业务上的处理，收到后直接丢弃。同时MPush提供了SPI的方式来接管客户端发过来的消息，具体请参考第三章 系统架构 - SPI定制化。如果没有特殊需求客户端上行消息建议使用Http Proxy模式。 广播推送与流控什么是广播推送？按推送用户范围来划分，MPush目前支持三种方式的推送： 单用户推送，推送消息给指定的某个用户。 批量推送，业务自己圈定一批用户，推送同一条消息给圈定的用户。 全网推送，推送消息给所有的在线用户。 这里所说的广播推送指的就是第三种用户范围的推送。 为什么广播推送要控制流量？因为要推送消息给全网在线用户，用户量可能非常大，为了防止瞬时流量过大，所有加了入了防过载保护：流量控制。 流量控制的使用 单任务流量控制 全局流量控制 广播推送条件过滤为什么要对用户进行过滤？因为广播是针对所有在线用户，为了更精准的推送，必须对目标用户进行筛选，才能满足个性化的业务需求。 目前支持的筛选纬度 tag：业务自己打的标签 userId：用户登录ID clientVersion：客户端版本 osName：客户端系统平台 osVersion：客户端系统版本 目前支持的表达式目前只支持jvm内置的Nashorn脚本引擎，语法为javascript标准语法。 使用用例具体请参照com.mpush.api.push.PushContext.java 灰度20%的在线用户：userId % 100 &lt; 20 包含test标签的用户：tags!=null &amp;&amp; tags.indexOf(&quot;test&quot;)!=-1 客户端版本号大于2.0的安卓用户：clientVersion.indexOf(&quot;android&quot;)!=-1 &amp;&amp; clientVersion.replace(/[^\d]/g,&quot;&quot;) &gt; 20 消息漫游(未实现)如果不需要支持“消息漫游”，对于在线消息，如果用户接收到，是不需要存储到数据库的。但如果要支持“换一台机器也能看到历史的聊天消息”，就需要对所有消息进行存储。 消息投递如上图，用户A发送消息给用户B，虽然B在线，仍然要增加一个步骤2.5，在投递之前进行存储，以备B的其他端登陆时，可以拉取到历史消息。 消息拉取如上图，原本不在线的B(phone端)重新登录了，怎么拉取历史消息呢？只需要在客户端本地存储一个上一次拉取到的msg_id(time)，到服务端重新拉取即可。 这里还有个问题，由于服务端存储所有消息成本是非常高的，所以一般“消息漫游”是有时间（或者消息数）限制，不能拉取所有所有几年前的历史消息，只能拉取最近的云端消息。 参考 架构师之路：https://mp.weixin.qq.com/s/gHnq-VqRSBX-UiGd306NDw 《微信多点登录，消息漫游，假如让你来实现？》]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>架构分析</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0-Alloc说明]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FAlloc%E6%9C%8D%E5%8A%A1%2F0-Alloc%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Alloc服务文章目录: 0-Alloc说明 1-启动服务 2-处理拉取服务列表请求 3-处理index.html请求 4-处理PUSH请求 打包部署说明]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Alloc服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2-处理拉取服务列表请求]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FAlloc%E6%9C%8D%E5%8A%A1%2F2-%E5%A4%84%E7%90%86%E6%8B%89%E5%8F%96%E6%9C%8D%E5%8A%A1%E5%88%97%E8%A1%A8%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[Alloc服务文章目录: 0-Alloc说明 1-启动服务 2-处理拉取服务列表请求 3-处理index.html请求 4-处理PUSH请求 打包部署说明]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Alloc服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1-启动服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FAlloc%E6%9C%8D%E5%8A%A1%2F1-%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Alloc服务文章目录: 0-Alloc说明 1-启动服务 2-处理拉取服务列表请求 3-处理index.html请求 4-处理PUSH请求 打包部署说明]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Alloc服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4-处理PUSH请求]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FAlloc%E6%9C%8D%E5%8A%A1%2F4-%E5%A4%84%E7%90%86PUSH%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[Alloc服务文章目录: 0-Alloc说明 1-启动服务 2-处理拉取服务列表请求 3-处理index.html请求 4-处理PUSH请求 打包部署说明]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Alloc服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[打包部署说明]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FAlloc%E6%9C%8D%E5%8A%A1%2F%E6%89%93%E5%8C%85%E9%83%A8%E7%BD%B2%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Alloc服务文章目录: 0-Alloc说明 1-启动服务 2-处理拉取服务列表请求 3-处理index.html请求 4-处理PUSH请求 打包部署说明]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Alloc服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-处理index.html请求]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FAlloc%E6%9C%8D%E5%8A%A1%2F3-%E5%A4%84%E7%90%86index.html%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[Alloc服务文章目录: 0-Alloc说明 1-启动服务 2-处理拉取服务列表请求 3-处理index.html请求 4-处理PUSH请求 打包部署说明]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Alloc服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MPNS发送消息流程]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2FMPNS%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启动-初始化]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2F%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息发送-常规]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2F%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81-%E5%B8%B8%E8%A7%84%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务发现]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2F%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流量整形]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2F%E6%B5%81%E9%87%8F%E6%95%B4%E5%BD%A2%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[连接初始化]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2F%E8%BF%9E%E6%8E%A5%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码解码]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2F%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[channelPipeline处理]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMPNS%2FchannelPipeline%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>MPNS</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FlowControl-流控]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FFlowControl-%E6%B5%81%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EventBus-事件总线]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FEventBus-%E4%BA%8B%E4%BB%B6%E6%80%BB%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[介绍一个内存级别的异步事件总线服务，实现了简单的生产-消费者模式；EventBus是Guava框架对观察者模式的一种实现，使用EventBus可以很简洁的实现事件注册监听和消费。 使用场景： MPUSH中各事件的发布、订阅； Elastic-Job任务执行和任务轨迹记录 new EventBus();new AsyncEventBus(); post(new xxxEvent()) 发布事件 register(this) 注册、订阅事件@Subscribe@AllowConcurrentEvents unregister(this) 取消订阅 第一步：哪个类需要订阅，首先的注册，然后订阅123456789101112131415// 注册EventBus.register(this);// 订阅事件@Subscribe@AllowConcurrentEventsvoid on(ConnectionCloseEvent event) &#123; ....&#125;@Subscribe@AllowConcurrentEventsvoid on(xxxxEvent event) &#123; ....&#125; 第二步：发布事件1EventBus.post(new UserOnlineEvent(message.getConnection(), message.userId)); 参考：https://blog.csdn.net/fanhenghui/article/details/51459273 MPUSH源码实现MPushClient.java 创建EventBus1234public MPushClient() &#123; monitorService = new MonitorService(); EventBus.create(monitorService.getThreadPoolManager().getEventBusExecutor());&#125; EventBus.java123456789101112131415161718192021222324252627282930313233343536package com.mpush.tools.event;import com.google.common.eventbus.AsyncEventBus;import com.mpush.api.event.Event;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.concurrent.Executor;/** * Created by ohun on 2015/12/29. * * @author ohun@live.cn */public class EventBus &#123; private static final Logger LOGGER = LoggerFactory.getLogger(EventBus.class); private static com.google.common.eventbus.EventBus eventBus; public static void create(Executor executor) &#123; eventBus = new AsyncEventBus(executor, (exception, context) -&gt; LOGGER.error("event bus subscriber ex", exception)); &#125; public static void post(Event event) &#123; eventBus.post(event); &#125; public static void register(Object bean) &#123; eventBus.register(bean); &#125; public static void unregister(Object bean) &#123; eventBus.unregister(bean); &#125;&#125; EventConsumer.java 父类中注册，子类中可以订阅事件123456package com.mpush.tools.event;public abstract class EventConsumer &#123; public EventConsumer() &#123; EventBus.register(this); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public final class LocalRouterManager extends EventConsumer implements RouterManager&lt;LocalRouter&gt; &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LocalRouterManager.class); private static final Map&lt;Integer, LocalRouter&gt; EMPTY = new HashMap&lt;&gt;(0); /** * 本地路由表 */ private final Map&lt;String, Map&lt;Integer, LocalRouter&gt;&gt; routers = new ConcurrentHashMap&lt;&gt;(); @Override public LocalRouter register(String userId, LocalRouter router) &#123; LOGGER.info("register local router success userId=&#123;&#125;, router=&#123;&#125;", userId, router); return routers.computeIfAbsent(userId, s -&gt; new HashMap&lt;&gt;(1)).put(router.getClientType(), router); &#125; @Override public boolean unRegister(String userId, int clientType) &#123; LocalRouter router = routers.getOrDefault(userId, EMPTY).remove(clientType); LOGGER.info("unRegister local router success userId=&#123;&#125;, router=&#123;&#125;", userId, router); return true; &#125; @Override public Set&lt;LocalRouter&gt; lookupAll(String userId) &#123; return new HashSet&lt;&gt;(routers.getOrDefault(userId, EMPTY).values()); &#125; @Override public LocalRouter lookup(String userId, int clientType) &#123; LocalRouter router = routers.getOrDefault(userId, EMPTY).get(clientType); LOGGER.info("lookup local router userId=&#123;&#125;, router=&#123;&#125;", userId, router); return router; &#125; public Map&lt;String, Map&lt;Integer, LocalRouter&gt;&gt; routers() &#123; return routers; &#125; /** * 监听链接关闭事件，清理失效的路由 * * @param event */ @Subscribe @AllowConcurrentEvents void on(ConnectionCloseEvent event) &#123; Connection connection = event.connection; if (connection == null) return; SessionContext context = connection.getSessionContext(); String userId = context.userId; if (userId == null) return; int clientType = context.getClientType(); LocalRouter localRouter = routers.getOrDefault(userId, EMPTY).get(clientType); if (localRouter == null) return; String connId = connection.getId(); //2.检测下，是否是同一个链接, 如果客户端重连，老的路由会被新的链接覆盖 if (connId.equals(localRouter.getRouteValue().getId())) &#123; //3. 删除路由 routers.getOrDefault(userId, EMPTY).remove(clientType); //4. 发送用户下线事件, 只有老的路由存在的情况下才发送，因为有可能又用户重连了，而老的链接又是在新连接之后才断开的 //这个时候就会有问题，会导致用户变成下线状态，实际用户应该是在线的。 EventBus.post(new UserOfflineEvent(event.connection, userId)); LOGGER.info("clean disconnected local route, userId=&#123;&#125;, route=&#123;&#125;", userId, localRouter); &#125; else &#123; //如果不相等，则log一下 LOGGER.info("clean disconnected local route, not clean:userId=&#123;&#125;, route=&#123;&#125;", userId, localRouter); &#125; &#125;&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVMUtil]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FJVMUtil%2F</url>
    <content type="text"><![CDATA[利用JMX的MBEAN,输出线程栈、堆栈到本地； 使用时机1（线程池发生reject，打印线程栈）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.mpush.tools.thread.pool;import com.mpush.tools.Utils;import com.mpush.tools.common.JVMUtil;import com.mpush.tools.config.CC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.concurrent.RejectedExecutionException;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;import static com.mpush.tools.thread.pool.ThreadPoolConfig.REJECTED_POLICY_ABORT;import static com.mpush.tools.thread.pool.ThreadPoolConfig.REJECTED_POLICY_CALLER_RUNS;public final class DumpThreadRejectedHandler implements RejectedExecutionHandler &#123; private final static Logger LOGGER = LoggerFactory.getLogger(DumpThreadRejectedHandler.class); private volatile boolean dumping = false; private static final String DUMP_DIR = CC.mp.monitor.dump_dir; private final ThreadPoolConfig poolConfig; private final int rejectedPolicy; public DumpThreadRejectedHandler(ThreadPoolConfig poolConfig) &#123; this.poolConfig = poolConfig; this.rejectedPolicy = poolConfig.getRejectedPolicy(); &#125; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; LOGGER.warn("one task rejected, poolConfig=&#123;&#125;, poolInfo=&#123;&#125;", poolConfig, Utils.getPoolInfo(e)); if (!dumping) &#123; dumping = true; dumpJVMInfo(); &#125; if (rejectedPolicy == REJECTED_POLICY_ABORT) &#123; throw new RejectedExecutionException("one task rejected, pool=" + poolConfig.getName()); &#125; else if (rejectedPolicy == REJECTED_POLICY_CALLER_RUNS) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125; &#125; private void dumpJVMInfo() &#123; LOGGER.info("start dump jvm info"); JVMUtil.dumpJstack(DUMP_DIR + "/" + poolConfig.getName()); LOGGER.info("end dump jvm info"); &#125;&#125; 使用时机2（MPushServer监控，打印线程栈、堆栈）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class MonitorService extends BaseService implements Monitor, Runnable &#123; private static final int FIRST_DUMP_JSTACK_LOAD_AVG = 2, SECOND_DUMP_JSTACK_LOAD_AVG = 4, THIRD_DUMP_JSTACK_LOAD_AVG = 6, FIRST_DUMP_JMAP_LOAD_AVG = 4; private static final String dumpLogDir = CC.mp.monitor.dump_dir; private static final boolean dumpEnabled = CC.mp.monitor.dump_stack; private static final boolean printLog = CC.mp.monitor.print_log; private static final long dumpPeriod = CC.mp.monitor.dump_period.getSeconds(); private volatile boolean dumpFirstJstack = false; private volatile boolean dumpSecondJstack = false; private volatile boolean dumpThirdJstack = false; private volatile boolean dumpJmap = false; private final ResultCollector collector; private final ThreadPoolManager threadPoolManager; public MonitorService() &#123; threadPoolManager = new ThreadPoolManager(); collector = new ResultCollector(threadPoolManager); &#125; private Thread thread; @Override public void run() &#123; while (isRunning()) &#123; MonitorResult result = collector.collect(); if (printLog) &#123; Logs.MONITOR.info(result.toJson()); &#125; if (dumpEnabled) &#123; dump(); &#125; try &#123; TimeUnit.SECONDS.sleep(dumpPeriod); &#125; catch (InterruptedException e) &#123; if (isRunning()) stop(); &#125; &#125; &#125; @Override protected void doStart(Listener listener) throws Throwable &#123; if (printLog || dumpEnabled) &#123; thread = Utils.newThread(ThreadNames.T_MONITOR, this); thread.setDaemon(true); thread.start(); &#125; listener.onSuccess(); &#125; @Override protected void doStop(Listener listener) throws Throwable &#123; if (thread != null &amp;&amp; thread.isAlive()) thread.interrupt(); listener.onSuccess(); &#125; private void dump() &#123; double load = collector.getJvmInfo().load(); if (load &gt; FIRST_DUMP_JSTACK_LOAD_AVG) &#123; if (!dumpFirstJstack) &#123; dumpFirstJstack = true; JVMUtil.dumpJstack(dumpLogDir); &#125; &#125; if (load &gt; SECOND_DUMP_JSTACK_LOAD_AVG) &#123; if (!dumpSecondJstack) &#123; dumpSecondJstack = true; JVMUtil.dumpJmap(dumpLogDir); &#125; &#125; if (load &gt; THIRD_DUMP_JSTACK_LOAD_AVG) &#123; if (!dumpThirdJstack) &#123; dumpThirdJstack = true; JVMUtil.dumpJmap(dumpLogDir); &#125; &#125; if (load &gt; FIRST_DUMP_JMAP_LOAD_AVG) &#123; if (!dumpJmap) &#123; dumpJmap = true; JVMUtil.dumpJmap(dumpLogDir); &#125; &#125; &#125; @Override public void monitor(String name, Thread thread) &#123; &#125; @Override public void monitor(String name, Executor executor) &#123; threadPoolManager.register(name, executor); &#125; public ThreadPoolManager getThreadPoolManager() &#123; return threadPoolManager; &#125;&#125; 源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177/* * (C) Copyright 2015-2016 the original author or authors. * * Licensed under the Apache License, Version 2.0 (the "License"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. * * Contributors: * ohun@live.cn (夜色) */package com.mpush.tools.common;import com.mpush.tools.Utils;import com.sun.management.HotSpotDiagnosticMXBean;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import javax.management.MBeanServer;import javax.management.ObjectName;import java.io.File;import java.io.FileOutputStream;import java.io.OutputStream;import java.io.PrintStream;import java.lang.management.*;import java.security.AccessController;import java.security.PrivilegedExceptionAction;import java.util.Iterator;import java.util.Map;import java.util.Set;public class JVMUtil &#123; private static final String HOT_SPOT_BEAN_NAME = "com.sun.management:type=HotSpotDiagnostic"; private static final Logger LOGGER = LoggerFactory.getLogger(JVMUtil.class); private static HotSpotDiagnosticMXBean hotSpotMXBean; private static ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); public static void jstack(OutputStream stream) throws Exception &#123; PrintStream out = new PrintStream(stream); boolean cpuTimeEnabled = threadMXBean.isThreadCpuTimeSupported() &amp;&amp; threadMXBean.isThreadCpuTimeEnabled(); Map&lt;Thread, StackTraceElement[]&gt; map = Thread.getAllStackTraces(); for (Map.Entry&lt;Thread, StackTraceElement[]&gt; entry : map.entrySet()) &#123; Thread t = entry.getKey(); StackTraceElement[] elements = entry.getValue(); ThreadInfo tt = threadMXBean.getThreadInfo(t.getId()); long tid = t.getId(); Thread.State state = t.getState(); long cpuTimeMillis = cpuTimeEnabled ? threadMXBean.getThreadCpuTime(tid) / 1000000 : -1; long userTimeMillis = cpuTimeEnabled ? threadMXBean.getThreadUserTime(tid) / 1000000 : -1; out.printf("%s id=%d state=%s deamon=%s priority=%s cpu[total=%sms,user=%sms]", t.getName(), tid, t.getState(), t.isDaemon(), t.getPriority(), cpuTimeMillis, userTimeMillis); final LockInfo lock = tt.getLockInfo(); if (lock != null &amp;&amp; state != Thread.State.BLOCKED) &#123; out.printf("%n - waiting on &lt;0x%08x&gt; (a %s)", lock.getIdentityHashCode(), lock.getClassName()); out.printf("%n - locked &lt;0x%08x&gt; (a %s)", lock.getIdentityHashCode(), lock.getClassName()); &#125; else if (lock != null &amp;&amp; state == Thread.State.BLOCKED) &#123; out.printf("%n - waiting to lock &lt;0x%08x&gt; (a %s)", lock.getIdentityHashCode(), lock.getClassName()); &#125; if (tt.isSuspended()) &#123; out.print(" (suspended)"); &#125; if (tt.isInNative()) &#123; out.print(" (running in native)"); &#125; out.println(); if (tt.getLockOwnerName() != null) &#123; out.printf(" owned by %s id=%d%n", tt.getLockOwnerName(), tt.getLockOwnerId()); &#125; final MonitorInfo[] monitors = tt.getLockedMonitors(); for (int i = 0; i &lt; elements.length; i++) &#123; final StackTraceElement element = elements[i]; out.printf(" at %s%n", element); for (int j = 1; j &lt; monitors.length; j++) &#123; final MonitorInfo monitor = monitors[j]; if (monitor.getLockedStackDepth() == i) &#123; out.printf(" - locked %s%n", monitor); &#125; &#125; &#125; out.println(); final LockInfo[] locks = tt.getLockedSynchronizers(); if (locks.length &gt; 0) &#123; out.printf(" Locked synchronizers: count = %d%n", locks.length); for (LockInfo l : locks) &#123; out.printf(" - %s%n", l); &#125; out.println(); &#125; &#125; &#125; public static void dumpJstack(final String jvmPath) &#123; Utils.newThread("dump-jstack-t", (() -&gt; &#123; File path = new File(jvmPath); if (path.exists() || path.mkdirs()) &#123; File file = new File(path, System.currentTimeMillis() + "-jstack.log"); try (FileOutputStream out = new FileOutputStream(file)) &#123; JVMUtil.jstack(out); &#125; catch (Throwable t) &#123; LOGGER.error("Dump JVM cache Error!", t); &#125; &#125; &#125;)).start(); &#125; private static HotSpotDiagnosticMXBean getHotSpotMXBean() &#123; try &#123; return AccessController.doPrivileged(new PrivilegedExceptionAction&lt;HotSpotDiagnosticMXBean&gt;() &#123; public HotSpotDiagnosticMXBean run() throws Exception &#123; MBeanServer server = ManagementFactory.getPlatformMBeanServer(); Set&lt;ObjectName&gt; s = server.queryNames(new ObjectName(HOT_SPOT_BEAN_NAME), null); Iterator&lt;ObjectName&gt; itr = s.iterator(); if (itr.hasNext()) &#123; ObjectName name = itr.next(); HotSpotDiagnosticMXBean bean = ManagementFactory.newPlatformMXBeanProxy(server, name.toString(), HotSpotDiagnosticMXBean.class); return bean; &#125; else &#123; return null; &#125; &#125; &#125;); &#125; catch (Exception e) &#123; LOGGER.error("getHotSpotMXBean Error!", e); return null; &#125; &#125; private static void initHotSpotMBean() throws Exception &#123; if (hotSpotMXBean == null) &#123; synchronized (JVMUtil.class) &#123; if (hotSpotMXBean == null) &#123; hotSpotMXBean = getHotSpotMXBean(); &#125; &#125; &#125; &#125; public static void jMap(String fileName, boolean live) &#123; File f = new File(fileName, System.currentTimeMillis() + "-jmap.log"); String currentFileName = f.getPath(); try &#123; initHotSpotMBean(); if (f.exists()) &#123; f.delete(); &#125; hotSpotMXBean.dumpHeap(currentFileName, live); &#125; catch (Exception e) &#123; LOGGER.error("dumpHeap Error!" + currentFileName, e); &#125; &#125; public static void dumpJmap(final String jvmPath) &#123; Utils.newThread("dump-jmap-t", () -&gt; jMap(jvmPath, false)).start(); &#125;&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Profiler-统计方法或者线程执行时间]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FProfiler-%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95%E6%88%96%E8%80%85%E7%BA%BF%E7%A8%8B%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[测试实现类： Profiler.java这两个类中打断点 AdminHandler.java ServerChannelHandler.java 测试Profiler1234567891011121314151617181920212223242526272829303132public class ProfilerTest &#123; //10ms long profile_slowly_limit = CC.mp.monitor.profile_slowly_duration.toMillis(); @Test public void testProfiler() throws Exception &#123; byte cmd = 1; try &#123; Profiler.enable(true); Profiler.start("time cost on [channel read]: %s", "111111"); Profiler.enter("time cost on [A]"); Thread.sleep(300); Profiler.release(); Profiler.enter("time cost on [B]"); Thread.sleep(500); Profiler.release(); Profiler.enter("time cost on [C]"); Thread.sleep(200); Profiler.release(); Thread.sleep(400); &#125;finally &#123; Profiler.release(); if (Profiler.getDuration() &gt; profile_slowly_limit) &#123; System.out.println(Profiler.dump()); &#125; Profiler.reset(); &#125; &#125;&#125; 输出：12340 [1,400ms (400ms), 100%] - time cost on [channel read]:+---0 [300ms, 21%, 21%] - time cost on [A]+---300 [500ms, 36%, 36%] - time cost on [B]`---800 [200ms, 14%, 14%] - time cost on [C] Profiler入门Profiler入门 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CC-配置中心]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FCC-%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[Typesafe的Config库，纯Java写成、零外部依赖、代码精简、功能灵活、API友好。支持Java properties、JSON、JSON超集格式HOCON以及环境变量。123456789101112public class Configure &#123; private final Config config; public Configure(String confFileName) &#123; config = ConfigFactory.load(confFileName); &#125; public Configure() &#123; config = ConfigFactory.load(); &#125; public String getString(String name) &#123; return config.getString(name); &#125;&#125; ConfigFactory.load()会加载配置文件，默认加载classpath下的application.conf,application.json和application.properties文件。当然也可以调用ConfigFactory.load(confFileName)加载指定的配置文件。 配置内容即可以是层级关系，也可以用”.”号分隔写成一行:123456789101112131415161718mp.log-level=$&#123;log.level&#125;mp.net.gateway-server-net=tcp // 网关服务使用的网络 udp/tcpmp.net.alloc-server-port=9999mp.net.alloc-server-protocol=httpmp.zk.server-address="127.0.0.1:2181"mp.redis &#123;// redis 集群配置 password:"" nodes:["127.0.0.1:6379"] //格式是ip:port cluster-model:single //single, cluster&#125;// 或者host&#123; ip = 127.0.0.1 port = 2282&#125;// 或者host.ip = 127.0.0.1host.port = 2282 即json格式和properties格式。（貌似.json只能是json格式，.properties只能是properties格式，而*.conf可以是两者混合,而且配置文件只能是以上三种后缀名） 代码中如何使用：String zk_address= CC.mp.zk.server-address; CC.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337package com.mpush.tools.config;import com.mpush.api.spi.net.DnsMapping;import com.mpush.tools.config.data.RedisNode;import com.typesafe.config.*;import java.io.File;import java.time.Duration;import java.util.*;import java.util.concurrent.TimeUnit;import static java.util.stream.Collectors.toCollection;/** * mpush 配置中心 * Created by yxx on 2016/5/20. * * @author ohun@live.cn */public interface CC &#123; Config cfg = load(); static Config load() &#123; Config config = ConfigFactory.load();//扫描加载所有可用的配置文件 // String custom_conf = System.getProperty("mp.conf");//加载自定义配置, 值来自jvm启动参数指定-Dmp.conf String custom_conf = "mp.conf"; if (config.hasPath(custom_conf)) &#123; File file = new File(config.getString(custom_conf)); if (file.exists()) &#123; Config custom = ConfigFactory.parseFile(file); config = custom.withFallback(config); &#125; &#125; return config; &#125; interface mp &#123; Config cfg = CC.cfg.getObject("mp").toConfig(); String log_dir = cfg.getString("log-dir"); String log_level = cfg.getString("log-level"); String log_conf_path = cfg.getString("log-conf-path"); interface core &#123; Config cfg = mp.cfg.getObject("core").toConfig(); int session_expired_time = (int) cfg.getDuration("session-expired-time").getSeconds(); int max_heartbeat = (int) cfg.getDuration("max-heartbeat", TimeUnit.MILLISECONDS); int max_packet_size = (int) cfg.getMemorySize("max-packet-size").toBytes(); int min_heartbeat = (int) cfg.getDuration("min-heartbeat", TimeUnit.MILLISECONDS); long compress_threshold = cfg.getBytes("compress-threshold"); int max_hb_timeout_times = cfg.getInt("max-hb-timeout-times"); String epoll_provider = cfg.getString("epoll-provider"); static boolean useNettyEpoll() &#123; if (!"netty".equals(CC.mp.core.epoll_provider)) return false; String name = CC.cfg.getString("os.name").toLowerCase(Locale.UK).trim(); return name.startsWith("linux");//只在linux下使用netty提供的epoll库 &#125; &#125; interface net &#123; Config cfg = mp.cfg.getObject("net").toConfig(); String local_ip = cfg.getString("local-ip"); String public_ip = cfg.getString("public-ip"); int connect_server_port = cfg.getInt("connect-server-port"); String connect_server_bind_ip = cfg.getString("connect-server-bind-ip"); String connect_server_register_ip = cfg.getString("connect-server-register-ip"); Map&lt;String, Object&gt; connect_server_register_attr = cfg.getObject("connect-server-register-attr").unwrapped(); int admin_server_port = cfg.getInt("admin-server-port"); int gateway_server_port = cfg.getInt("gateway-server-port"); String gateway_server_bind_ip = cfg.getString("gateway-server-bind-ip"); String gateway_server_register_ip = cfg.getString("gateway-server-register-ip"); String gateway_server_net = cfg.getString("gateway-server-net"); String gateway_server_multicast = cfg.getString("gateway-server-multicast"); String gateway_client_multicast = cfg.getString("gateway-client-multicast"); int gateway_client_port = cfg.getInt("gateway-client-port"); int ws_server_port = cfg.getInt("ws-server-port"); String ws_path = cfg.getString("ws-path"); int gateway_client_num = cfg.getInt("gateway-client-num"); static boolean tcpGateway() &#123; return "tcp".equals(gateway_server_net); &#125; static boolean udpGateway() &#123; return "udp".equals(gateway_server_net); &#125; static boolean wsEnabled() &#123; return ws_server_port &gt; 0; &#125; static boolean udtGateway() &#123; return "udt".equals(gateway_server_net); &#125; static boolean sctpGateway() &#123; return "sctp".equals(gateway_server_net); &#125; interface public_ip_mapping &#123; Map&lt;String, Object&gt; mappings = net.cfg.getObject("public-host-mapping").unwrapped(); static String getString(String localIp) &#123; return (String) mappings.get(localIp); &#125; &#125; interface snd_buf &#123; Config cfg = net.cfg.getObject("snd_buf").toConfig(); int connect_server = (int) cfg.getMemorySize("connect-server").toBytes(); int gateway_server = (int) cfg.getMemorySize("gateway-server").toBytes(); int gateway_client = (int) cfg.getMemorySize("gateway-client").toBytes(); &#125; interface rcv_buf &#123; Config cfg = net.cfg.getObject("rcv_buf").toConfig(); int connect_server = (int) cfg.getMemorySize("connect-server").toBytes(); int gateway_server = (int) cfg.getMemorySize("gateway-server").toBytes(); int gateway_client = (int) cfg.getMemorySize("gateway-client").toBytes(); &#125; interface write_buffer_water_mark &#123; Config cfg = net.cfg.getObject("write-buffer-water-mark").toConfig(); int connect_server_low = (int) cfg.getMemorySize("connect-server-low").toBytes(); int connect_server_high = (int) cfg.getMemorySize("connect-server-high").toBytes(); int gateway_server_low = (int) cfg.getMemorySize("gateway-server-low").toBytes(); int gateway_server_high = (int) cfg.getMemorySize("gateway-server-high").toBytes(); &#125; interface traffic_shaping &#123; Config cfg = net.cfg.getObject("traffic-shaping").toConfig(); interface gateway_client &#123; Config cfg = traffic_shaping.cfg.getObject("gateway-client").toConfig(); boolean enabled = cfg.getBoolean("enabled"); long check_interval = cfg.getDuration("check-interval", TimeUnit.MILLISECONDS); long write_global_limit = cfg.getBytes("write-global-limit"); long read_global_limit = cfg.getBytes("read-global-limit"); long write_channel_limit = cfg.getBytes("write-channel-limit"); long read_channel_limit = cfg.getBytes("read-channel-limit"); &#125; interface gateway_server &#123; Config cfg = traffic_shaping.cfg.getObject("gateway-server").toConfig(); boolean enabled = cfg.getBoolean("enabled"); long check_interval = cfg.getDuration("check-interval", TimeUnit.MILLISECONDS); long write_global_limit = cfg.getBytes("write-global-limit"); long read_global_limit = cfg.getBytes("read-global-limit"); long write_channel_limit = cfg.getBytes("write-channel-limit"); long read_channel_limit = cfg.getBytes("read-channel-limit"); &#125; interface connect_server &#123; Config cfg = traffic_shaping.cfg.getObject("connect-server").toConfig(); boolean enabled = cfg.getBoolean("enabled"); long check_interval = cfg.getDuration("check-interval", TimeUnit.MILLISECONDS); long write_global_limit = cfg.getBytes("write-global-limit"); long read_global_limit = cfg.getBytes("read-global-limit"); long write_channel_limit = cfg.getBytes("write-channel-limit"); long read_channel_limit = cfg.getBytes("read-channel-limit"); &#125; &#125; &#125; interface security &#123; Config cfg = mp.cfg.getObject("security").toConfig(); int aes_key_length = cfg.getInt("aes-key-length"); String public_key = cfg.getString("public-key"); String private_key = cfg.getString("private-key"); &#125; interface thread &#123; Config cfg = mp.cfg.getObject("thread").toConfig(); interface pool &#123; Config cfg = thread.cfg.getObject("pool").toConfig(); int conn_work = cfg.getInt("conn-work"); int http_work = cfg.getInt("http-work"); int push_task = cfg.getInt("push-task"); int push_client = cfg.getInt("push-client"); int ack_timer = cfg.getInt("ack-timer"); int gateway_server_work = cfg.getInt("gateway-server-work"); int gateway_client_work = cfg.getInt("gateway-client-work"); interface event_bus &#123; Config cfg = pool.cfg.getObject("event-bus").toConfig(); int min = cfg.getInt("min"); int max = cfg.getInt("max"); int queue_size = cfg.getInt("queue-size"); &#125; interface mq &#123; Config cfg = pool.cfg.getObject("mq").toConfig(); int min = cfg.getInt("min"); int max = cfg.getInt("max"); int queue_size = cfg.getInt("queue-size"); &#125; &#125; &#125; interface zk &#123; Config cfg = mp.cfg.getObject("zk").toConfig(); int sessionTimeoutMs = (int) cfg.getDuration("sessionTimeoutMs", TimeUnit.MILLISECONDS); String watch_path = cfg.getString("watch-path"); int connectionTimeoutMs = (int) cfg.getDuration("connectionTimeoutMs", TimeUnit.MILLISECONDS); String namespace = cfg.getString("namespace"); String digest = cfg.getString("digest"); String server_address = cfg.getString("server-address"); interface retry &#123; Config cfg = zk.cfg.getObject("retry").toConfig(); int maxRetries = cfg.getInt("maxRetries"); int baseSleepTimeMs = (int) cfg.getDuration("baseSleepTimeMs", TimeUnit.MILLISECONDS); int maxSleepMs = (int) cfg.getDuration("maxSleepMs", TimeUnit.MILLISECONDS); &#125; &#125; interface redis &#123; Config cfg = mp.cfg.getObject("redis").toConfig(); String password = cfg.getString("password"); String clusterModel = cfg.getString("cluster-model"); String sentinelMaster = cfg.getString("sentinel-master"); List&lt;RedisNode&gt; nodes = cfg.getList("nodes") .stream()//第一纬度数组 .map(v -&gt; RedisNode.from(v.unwrapped().toString())) .collect(toCollection(ArrayList::new)); static boolean isCluster() &#123; return "cluster".equals(clusterModel); &#125; static boolean isSentinel() &#123; return "sentinel".equals(clusterModel); &#125; static &lt;T&gt; T getPoolConfig(Class&lt;T&gt; clazz) &#123; return ConfigBeanImpl.createInternal(cfg.getObject("config").toConfig(), clazz); &#125; &#125; interface http &#123; Config cfg = mp.cfg.getObject("http").toConfig(); boolean proxy_enabled = cfg.getBoolean("proxy-enabled"); int default_read_timeout = (int) cfg.getDuration("default-read-timeout", TimeUnit.MILLISECONDS); int max_conn_per_host = cfg.getInt("max-conn-per-host"); long max_content_length = cfg.getBytes("max-content-length"); Map&lt;String, List&lt;DnsMapping&gt;&gt; dns_mapping = loadMapping(); static Map&lt;String, List&lt;DnsMapping&gt;&gt; loadMapping() &#123; Map&lt;String, List&lt;DnsMapping&gt;&gt; map = new HashMap&lt;&gt;(); cfg.getObject("dns-mapping").forEach((s, v) -&gt; map.put(s, ConfigList.class.cast(v) .stream() .map(cv -&gt; DnsMapping.parse((String) cv.unwrapped())) .collect(toCollection(ArrayList::new)) ) ); return map; &#125; &#125; interface push &#123; Config cfg = mp.cfg.getObject("push").toConfig(); interface flow_control &#123; Config cfg = push.cfg.getObject("flow-control").toConfig(); interface global &#123; Config cfg = flow_control.cfg.getObject("global").toConfig(); int limit = cfg.getNumber("limit").intValue(); int max = cfg.getInt("max"); int duration = (int) cfg.getDuration("duration").toMillis(); &#125; interface broadcast &#123; Config cfg = flow_control.cfg.getObject("broadcast").toConfig(); int limit = cfg.getInt("limit"); int max = cfg.getInt("max"); int duration = (int) cfg.getDuration("duration").toMillis(); &#125; &#125; &#125; interface monitor &#123; Config cfg = mp.cfg.getObject("monitor").toConfig(); String dump_dir = cfg.getString("dump-dir"); boolean dump_stack = cfg.getBoolean("dump-stack"); boolean print_log = cfg.getBoolean("print-log"); Duration dump_period = cfg.getDuration("dump-period"); boolean profile_enabled = cfg.getBoolean("profile-enabled"); Duration profile_slowly_duration = cfg.getDuration("profile-slowly-duration"); &#125; &#125;&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPI机制]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FSPI%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Profiler入门]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FProfiler%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、 概述Profiler是一个时间统计程序，他通过在程序中埋点，将埋点时间记录入线程变量中以实现革离，最后dump出结果，得出埋点时间树。 二、 Profiler常用方法(Profiler类的静态方法)1、Start当前线程埋点计时的开始，标识当前埋点的开始时间，每个埋点产生一个Entry，被压入线程变量中的栈中，当start方法被调用时，就是一个时间点栈的开始，如果start方法被重复调用，则覆盖前面被放入线程变量的栈，每个线程只允许一个栈的存在。该方法的参数标识该Entry信息。 2、 Enter向线程变量中的栈压入当前埋点信息Profiler.entry(“…”)，标识当前埋点的开始时间，如果此线程此前未调用start方法，则此方法调用不执行任何操作。 3、Release结束栈里当前Entry时间，打上结束时间标识。 4、GetDuration获取耗费的总时间，是从start到当前的中时间，如果start的Entry没有被release，则getDuration的结果为-1，所以只有栈的最底层的Entry被release后getDuration得到的数值才有意义。 5、Dump当线程内的所有Entry被release后，通过dump的到整个线程内埋点的时间树，列出所有Entry，并统计各自所占用的时间。 6、Reset清除当前线程内的计数器栈。当一次调用完成后，在dump出时间树后应该将其清除，并须再次调用start方可重新计时。 三、 Profiler埋点方式在消息中心项目中主要采用两种埋点方式： 一种是业务程序中直接埋入Profiler代码； 一种是利用Spring方法拦截器，将Profiler代码埋入拦截器中。两种方法各有优劣，所以在项目中两种方式综合应用，相互补充。 1、直接埋点优点：在业务代码中埋点可以设置任意粒度，能够显示程序中微小范围内的时间变化。缺点：Profiler耦合进了业务代码，埋点分布于业务程序的各处，不便于阅读和修改，同时由于其要带try{}finally{}块，将业务代码分割的支离破碎，造成业务代码的断层。用例如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public void sendWithChannels(final MessageTaskDO messageTask, final MessageTypeDO messageType,final UserInfoDO userInfo, final int chs) throws MessageException&#123; int channelInt = chs; UserInfoFuture userInfoFuture = null; Profiler.start("TagTask isFinish"); try&#123; if (messageTask.getTaskPreStatus() == MessageTaskDO.FALURE_NOTSEND_RETRY &amp;&amp; messageStatusDAO.isFinish(messageTask.getTaskID(), userInfo.getTargetID()))&#123; return; &#125; Profiler.enter("Task Filter"); try&#123; channelInt = getChannelsAfterFilter(messageType, userInfo, channelInt); &#125;finally&#123; Profiler.release(); &#125; Profiler.enter("Task Fill UserInfo"); try&#123; channelInt = getChannelsAfterUInfoFilled(userInfo, userInfoFuture, channelInt); &#125;finally&#123; Profiler.release(); &#125; List&lt;Channel&gt; channels = null; Map&lt;Channel, Integer&gt; channelTemplateId = null; Map&lt;String, String&gt; mapContext = null; Profiler.enter("Task Fill Context"); try&#123; channels = Channel.parseChannels(channelInt); &#125;finally&#123; Profiler.release(); &#125; Profiler.enter("Task Send with Channels"); try&#123; for (final Channel channel : channels)&#123; final boolean couldSend = true; String sendToAddress = ""; if (couldSend)&#123; Profiler.enter("send of Channel choose"); if (channel == Channel.WANGWANG)&#123; sendToAddress = userInfo.getUserNick(); &#125;else&#123; logger.error("MessageTask:" + messageTask.getTaskID() + " is not channel"); continue; &#125; final Integer templetId = channelTemplateId.get(channel); final String replyTo = "taobao@taobao.com"; Profiler.release(); &#125; &#125; &#125;finally&#123; Profiler.release(); &#125; &#125;finally&#123; Profiler.release(); final String detail = Profiler.dump("Detail: ", " "); logger.warn(String.format("调用服务：%s的方法%s耗时：%dms，超过预期\n%s\n", new Object[] &#123;invocation.getThis().getClass().getName(), invocation.getMethod().getName(),duration, detail &#125;)); Profiler.reset(); &#125; &#125; 通过上面代码可以看出，直接埋点虽然可以控制到很小的粒度，但埋点代码混杂于业务代码，将业务代码分割的支离破碎，是一种很不优雅的用法。 2、Spring拦截器埋点优点: 和业务代码解耦，要统计哪个方法的耗时只需要修改拦截器配置文件和拦截器中的埋点代码即可，和业务代码完全无关。缺点: 埋点粒度只能到方法这个级别，方法内部的耗时统计就无法完成，同时只能拦截Spring Bean的方法，Bean内部间方法调用无法拦截，只能拦截外部调用Bean的方法。用例如下：1234567891011121314151617181920&lt;!-- 自动代理所有的advisor --&gt; &lt;bean id="autoProxyCreator" class="org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator"&gt; &lt;property name="proxyTargetClass" value="true"/&gt; &lt;/bean&gt; &lt;bean id="interceptorAdvisor" class="org.springframework.aop.support.RegexpMethodPointcutAdvisor"&gt; &lt;!-- 业务实现方法名匹配 --&gt; &lt;property name="patterns"&gt; &lt;list&gt; &lt;value&gt;com.taobao.messenger.dal.*DAO.*&lt;/value&gt; &lt;value&gt;com.taobao.messenger.service.*&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="advice"&gt; &lt;ref bean="interceptorAdvice" /&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="interceptorAdvice" class="com.taobao.messenger.proxy.ProfilerInterceptor"&gt; &lt;/bean&gt; 123456789101112131415161718192021222324252627282930313233public Object invoke(final MethodInvocation invocation) throws Throwable &#123; final String key = invocation.getThis().getClass().getName()+MessageConstants.DOT_SPLIT+invocation.getMethod().getName(); boolean resetFlag = false; if(Profiler.getEntry() == null || (key.equals(TagTaskDispatcher.class.getName() +MessageConstants.DOT_SPLIT+"sendMessage")))&#123; Profiler.start(key); resetFlag = true; &#125;else&#123; Profiler.enter(key); &#125; Object result = null; try&#123; result = invocation.proceed(); &#125; finally&#123; Profiler.release(); if(resetFlag || (key.equals(TagTaskDispatcher.class.getName() +MessageConstants.DOT_SPLIT+"sendMessage")))&#123; try&#123;if(threshold.containsKey(key))&#123; final Long duration = Profiler.getDuration(); if(duration&gt;threshold.get(key))&#123; final String detail = Profiler.dump("Detail: ", " "); logger.warn(String.format("调用服务：%s的方法%s耗时：%dms，超过预期\n%s\n", new Object[] &#123; invocation.getThis().getClass().getName(), invocation.getMethod().getName(),duration, detail &#125;)); &#125; &#125; &#125; finally&#123; Profiler.reset(); &#125;&#125; &#125; return result; &#125; 通过上面代码可以看出，Profiler代码和业务代码从表象上看完全无关，从结构上和管理上都比较不错。 四、 Profiler使用中的问题1、埋点方式的问题通过上面埋点方式的分析，两种方式都有自己的优缺点，所以我们采用两种方式综合使用的方针，以Spring拦截器埋点方式为主，在需要被埋点的重要关节，程序实现的同时尽量将要监控的点放到不通的类里面，被监控的方法粒度尽可能的小，便于配置Spring进行监控。有些相互间关系比较紧密的不容易分割的点和在debug阶段需要临时监控的点使用直接埋点的方式，进行细粒度的监控，在系统稳定后注释掉临时监控点，保留仍需后续监控的直接埋点，但这种埋点要尽可能的少。 2、内存泄露的问题一定要将Profiler.start()方法和Profiler.reset()方法写入 try{}finally{}块中，确保Profiler.reset()方法一定被调用来释放资源，否则如果中间出现异常跳过资源释放操作将引发内存泄露。同理Profiler.release()方法也应该写入finally块里，跳过虽然不会引发内存泄露，但会导致埋点时间计算错误。这点用 Spring拦截器比较好控制，只要控制住出口和入口就行。 3、时间树输出的问题在多线程环境下一般在在线程的出口输出时间树即可，在线程中间（非start方法对应的栈）不要用getDuration获取总耗时，此时只能得到-1,因为getDuration是求最底层Entry的耗时的，中间状态通常最底层的Entry没被release，没办法获得耗时。 4、方法对Profiler.start() 和 Profiler.reset()要成对出现，reset()及时释放内存，防止内存泄露；Profiler.enter() 和 Profiler.release() 要成对出现，release()是时间段结束埋点，内存中打上此enter结束的时间点。 产生结果树如下:123456789101112131415161718192021222324252627282930313233343536373839[WARN ] 2009-06-30 15:46:06,517 [TaskExecuteThread- 1-priority-1] [proxy.ProfilerInterceptor] - 调用服务：com.taobao.messenger.control.impl.DefaultTaskController的方法send耗时：375ms，超过预期 Detail: 0 [375ms (219ms), 100%] - com.taobao.messenger.control.impl.DefaultTaskController.send +---47 [0ms] - com.taobao.messenger.dal.ibatis.IbatisMessageTypeDAO.getMessageTypeById `---219 [156ms (16ms), 42%, 42%] - com.taobao.messenger.control.impl.DelegateTaskController.sendWithChannels +---235 [0ms] - TagTask isFinish +---235 [62ms, 40%, 17%] - Task Filter | `---235 [62ms, 100%, 17%] - com.taobao.messenger.dal.ibatis.IbatisSubscriptionInfoDAO.getUserSubscriptionInfo +---297 [0ms] - Task Fill UserInfo | `---297 [0ms] - Task fill from UIC +---297 [0ms] - Task Fill Context `---297 [78ms, 50%, 21%] - Task Send with Channels +---297 [0ms] - send of Channel choose +---297 [31ms (16ms), 40%, 8%] - render getTempletId | `---313 [15ms, 48%, 4%] - com.taobao.messenger.dal.ibatis.IbatisMessageTemplateDAO.getMessageTempletByID +---328 [47ms, 60%, 13%] - render content +---375 [0ms] - render subject +---375 [0ms] - render fill message +---375 [0ms] - put message into gateway +---375 [0ms] - send of Channel choose +---375 [0ms] - render getTempletId | `---375 [0ms] - com.taobao.messenger.dal.ibatis.IbatisMessageTemplateDAO.getMessageTempletByID +---375 [0ms] - render content +---375 [0ms] - render subject +---375 [0ms] - render fill message +---375 [0ms] - put message into gateway +---375 [0ms] - send of Channel choose +---375 [0ms] - render getTempletId | `---375 [0ms] - com.taobao.messenger.dal.ibatis.IbatisMessageTemplateDAO.getMessageTempletByID +---375 [0ms] - render content +---375 [0ms] - render subject +---375 [0ms] - render fill message +---375 [0ms] - put message into gateway +---375 [0ms] - send of Channel choose +---375 [0ms] - render getTempletId | `---375 [0ms] - com.taobao.messenger.dal.ibatis.IbatisMessageTemplateDAO.getMessageTempletByID +---375 [0ms] - render content +---375 [0ms] - render subject +---375 [0ms] - render fill message `---375 [0ms] - put message into gateway +— 表示正常 | `— 表示上一个的子集 `— 表示最后一个 参考https://blog.csdn.net/hualusiyu/article/details/8568808https://www.cnblogs.com/shipengzhi/articles/2087517.html 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logs]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FLogs%2F</url>
    <content type="text"><![CDATA[初始化Logs.init(); MPush启动时； 客户端连接时； 使用1Logs.Console.warn("你正在使用的CacheManager只能用于源码测试，生产环境请使用redis 3.x."); 输出110:06:48.366 -[console]- 你正在使用的CacheManager只能用于源码测试，生产环境请使用redis 3.x. 工具类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/* * (C) Copyright 2015-2016 the original author or authors. * * Licensed under the Apache License, Version 2.0 (the "License"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. * * Contributors: * ohun@live.cn (夜色) */package com.mpush.tools.log;import com.mpush.tools.config.CC;import com.typesafe.config.ConfigRenderOptions;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * Created by ohun on 2016/5/16. * * @author ohun@live.cn */public interface Logs &#123; boolean logInit = init(); static boolean init() &#123; if (logInit) return true; System.setProperty("log.home", CC.mp.log_dir); System.setProperty("log.root.level", CC.mp.log_level); System.setProperty("logback.configurationFile", CC.mp.log_conf_path); LoggerFactory .getLogger("console") .info(CC.mp.cfg.root().render(ConfigRenderOptions.concise().setFormatted(true))); return true; &#125; Logger Console = LoggerFactory.getLogger("console"), CONN = LoggerFactory.getLogger("mpush.conn.log"), MONITOR = LoggerFactory.getLogger("mpush.monitor.log"), PUSH = LoggerFactory.getLogger("mpush.push.log"), HB = LoggerFactory.getLogger("mpush.heartbeat.log"), CACHE = LoggerFactory.getLogger("mpush.cache.log"), RSD = LoggerFactory.getLogger("mpush.srd.log"), HTTP = LoggerFactory.getLogger("mpush.http.log"), PROFILE = LoggerFactory.getLogger("mpush.profile.log");&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务启动监听]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%E7%9B%91%E5%90%AC%2F</url>
    <content type="text"><![CDATA[主要用于服务的启动和停止，子类只需要继承BaseService类，然后调用其start、stop方法； 启动服务：123mPushServer.getHttpClient().syncStart();//或者mPushServer.getHttpClient().start(); 停止服务：123mPushServer.getHttpClient().syncStop();//或者mPushServer.getHttpClient().stop(); 子类中的启动、停止方法实现：1234567891011121314@Overrideprotected void doStart(Listener listener) throws Throwable &#123; //....实现相关的启动业务 //通知监听器，表示该操作已经完成 listener.onSuccess();&#125;@Overrideprotected void doStop(Listener listener) throws Throwable &#123; //....实现相关的停止业务 //通知监听器，表示该操作已经完成 listener.onSuccess();&#125; BaseService.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147package com.mpush.api.service;import java.util.concurrent.CompletableFuture;import java.util.concurrent.atomic.AtomicBoolean;/** * Created by yxx on 2016/5/19. * * @author ohun@live.cn */public abstract class BaseService implements Service &#123; protected final AtomicBoolean started = new AtomicBoolean(); @Override public void init() &#123; &#125; @Override public boolean isRunning() &#123; return started.get(); &#125; protected void tryStart(Listener l, FunctionEx function) &#123; FutureListener listener = wrap(l); if (started.compareAndSet(false, true)) &#123; try &#123; init(); function.apply(listener); listener.monitor(this);//主要用于异步，否则应该放置在function.apply(listener)之前 &#125; catch (Throwable e) &#123; listener.onFailure(e); throw new ServiceException(e); &#125; &#125; else &#123; if (throwIfStarted()) &#123; listener.onFailure(new ServiceException("service already started.")); &#125; else &#123; listener.onSuccess(); &#125; &#125; &#125; protected void tryStop(Listener l, FunctionEx function) &#123; FutureListener listener = wrap(l); if (started.compareAndSet(true, false)) &#123; try &#123; function.apply(listener); listener.monitor(this);//主要用于异步，否则应该放置在function.apply(listener)之前 &#125; catch (Throwable e) &#123; listener.onFailure(e); throw new ServiceException(e); &#125; &#125; else &#123; if (throwIfStopped()) &#123; listener.onFailure(new ServiceException("service already stopped.")); &#125; else &#123; listener.onSuccess(); &#125; &#125; &#125; public final CompletableFuture&lt;Boolean&gt; start() &#123; FutureListener listener = new FutureListener(started); start(listener); return listener; &#125; public final CompletableFuture&lt;Boolean&gt; stop() &#123; FutureListener listener = new FutureListener(started); stop(listener); return listener; &#125; @Override public final boolean syncStart() &#123; return start().join(); &#125; @Override public final boolean syncStop() &#123; return stop().join(); &#125; @Override public void start(Listener listener) &#123; tryStart(listener, this::doStart); &#125; @Override public void stop(Listener listener) &#123; tryStop(listener, this::doStop); &#125; protected void doStart(Listener listener) throws Throwable &#123; listener.onSuccess(); &#125; protected void doStop(Listener listener) throws Throwable &#123; listener.onSuccess(); &#125; /** * 控制当服务已经启动后，重复调用start方法，是否抛出服务已经启动异常 * 默认是true * * @return true:抛出异常 */ protected boolean throwIfStarted() &#123; return true; &#125; /** * 控制当服务已经停止后，重复调用stop方法，是否抛出服务已经停止异常 * 默认是true * * @return true:抛出异常 */ protected boolean throwIfStopped() &#123; return true; &#125; /** * 服务启动停止，超时时间, 默认是10s * * @return 超时时间 */ protected int timeoutMillis() &#123; return 1000 * 10; &#125; protected interface FunctionEx &#123; void apply(Listener l) throws Throwable; &#125; /** * 防止Listener被重复执行 * * @param l listener * @return FutureListener */ public FutureListener wrap(Listener l) &#123; if (l == null) return new FutureListener(started); if (l instanceof FutureListener) return (FutureListener) l; return new FutureListener(l, started); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.mpush.api.service;import java.util.concurrent.CompletableFuture;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicBoolean;public class FutureListener extends CompletableFuture&lt;Boolean&gt; implements Listener &#123; private final Listener listener; private final AtomicBoolean started; public FutureListener(AtomicBoolean started) &#123; this.started = started; this.listener = null; &#125; public FutureListener(Listener listener, AtomicBoolean started) &#123; this.listener = listener; this.started = started; &#125; @Override public void onSuccess(Object... args) &#123; if (isDone()) return;// 防止Listener被重复执行 complete(started.get()); if (listener != null) listener.onSuccess(args); &#125; @Override public void onFailure(Throwable cause) &#123; if (isDone()) return;// 防止Listener被重复执行 completeExceptionally(cause); if (listener != null) listener.onFailure(cause); throw cause instanceof ServiceException ? (ServiceException) cause : new ServiceException(cause); &#125; /** * 防止服务长时间卡在某个地方，增加超时监控 * * @param service 服务 */ public void monitor(BaseService service) &#123; if (isDone()) return;// 防止Listener被重复执行 runAsync(() -&gt; &#123; try &#123; this.get(service.timeoutMillis(), TimeUnit.MILLISECONDS); &#125; catch (Exception e) &#123; this.onFailure(new ServiceException(String.format("service %s monitor timeout", service.getClass().getSimpleName()))); &#125; &#125;); &#125; @Override public boolean cancel(boolean mayInterruptIfRunning) &#123; throw new UnsupportedOperationException(); &#125;&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TimeLine-时间线]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2FTimeLine-%E6%97%B6%E9%97%B4%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[TimeLine可以跟踪调用链路点，记录经过的各个环节点，从而知道这条消息经过了哪几个组件； 使用：1234567891011121314151617181920212223242526272829public final class SingleUserPushTask implements PushTask, ChannelFutureListener &#123; //初始化 private final TimeLine timeLine = new TimeLine(); public SingleUserPushTask(MPushServer mPushServer, IPushMessage message, FlowControl flowControl) &#123; ..... //标记开始 this.timeLine.begin("push-center-begin"); &#125; .... mPushServer.getPushCenter().getPushListener().onTimeout(message, timeLine.timeoutEnd().getTimePoints()); mPushServer.getPushCenter().getPushListener().onSuccess(message, timeLine.successEnd().getTimePoints()); mPushServer.getPushCenter().getPushListener().onFailure(message, timeLine.failureEnd().getTimePoints()); mPushServer.getPushCenter().getPushListener().onRedirect(message, timeLine.end("redirect-end").getTimePoints()); mPushServer.getPushCenter().getPushListener().onOffline(message, timeLine.end("offline-end").getTimePoints()); /** * 添加ACK任务到队列, 等待客户端响应 * @param messageId 下发到客户端待ack的消息的sessionId */ private void addAckTask(int messageId) &#123; timeLine.addTimePoint("waiting-ack"); //因为要进队列，可以提前释放一些比较占用内存的字段，便于垃圾回收 message.finalized(); AckTask task = AckTask .from(messageId) .setCallback(new PushAckCallback(message, timeLine, mPushServer.getPushCenter())); mPushServer.getPushCenter().getAckTaskQueue().add(task, message.getTimeoutMills() - (int) (System.currentTimeMillis() - start)); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public final class TimeLine &#123; private final TimePoint root = new TimePoint("root"); private final String name; private int pointCount; private TimePoint current = root; public TimeLine() &#123; name = "TimeLine"; &#125; public TimeLine(String name) &#123; this.name = name; &#125; public void begin(String name) &#123; addTimePoint(name); &#125; public void begin() &#123; addTimePoint("begin"); &#125; public void addTimePoint(String name) &#123; current = current.next = new TimePoint(name); pointCount++; &#125; public void addTimePoints(Object[] points) &#123; if (points != null) &#123; for (int i = 0; i &lt; points.length; i++) &#123; current = current.next = new TimePoint((String) points[i], ((Number) points[++i]).longValue()); pointCount++; &#125; &#125; &#125; public TimeLine end(String name) &#123; addTimePoint(name); return this; &#125; public TimeLine end() &#123; addTimePoint("end"); return this; &#125; public TimeLine successEnd() &#123; addTimePoint("success-end"); return this; &#125; public TimeLine failureEnd() &#123; addTimePoint("failure-end"); return this; &#125; public TimeLine timeoutEnd() &#123; addTimePoint("timeout-end"); return this; &#125; public void clean() &#123; root.next = null; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(name); if (root.next != null) &#123; sb.append('[').append(current.time - root.next.time).append(']').append("(ms)"); &#125; sb.append('&#123;'); TimePoint next = root; while ((next = next.next) != null) &#123; sb.append(next.toString()); &#125; sb.append('&#125;'); return sb.toString(); &#125; public Object[] getTimePoints() &#123; Object[] arrays = new Object[2 * pointCount]; int i = 0; TimePoint next = root; while ((next = next.next) != null) &#123; arrays[i++] = next.name; arrays[i++] = next.time; &#125; return arrays; &#125; private static class TimePoint &#123; private final String name; private final long time; private transient TimePoint next; public TimePoint(String name) &#123; this.name = name; this.time = System.currentTimeMillis(); &#125; public TimePoint(String name, long time) &#123; this.name = name; this.time = time; &#125; public void setNext(TimePoint next) &#123; this.next = next; &#125; @Override public String toString() &#123; if (next == null) return name; return name + " --（" + (next.time - time) + "ms) --&gt; "; &#125; &#125;&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控** JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制** TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[状态判断-位运算]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2F%E7%8A%B6%E6%80%81%E5%88%A4%E6%96%AD-%E4%BD%8D%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通信模型与超时控制]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2F%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B6%85%E6%97%B6%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[线程池使用 线程池监控 线程池监控监控总收集器ResultCollector.java 监控信息收集器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.mpush.monitor.data;import com.mpush.monitor.quota.impl.*;import com.mpush.monitor.service.ThreadPoolManager;/** * Created by yxx on 2016/5/19. * * @author ohun@live.cn */public class ResultCollector &#123; private final JVMInfo jvmInfo; private final JVMGC jvmgc; private final JVMMemory jvmMemory; private final JVMThread jvmThread; private final JVMThreadPool jvmThreadPool; public ResultCollector(ThreadPoolManager threadPoolManager) &#123; this.jvmInfo = new JVMInfo(); this.jvmgc = new JVMGC(); this.jvmMemory = new JVMMemory(); this.jvmThread = new JVMThread(); this.jvmThreadPool = new JVMThreadPool(threadPoolManager); &#125; public MonitorResult collect() &#123; MonitorResult result = new MonitorResult(); result.addResult("jvm-info", jvmInfo.monitor()); result.addResult("jvm-gc", jvmgc.monitor()); result.addResult("jvm-memory", jvmMemory.monitor()); result.addResult("jvm-thread", jvmThread.monitor()); result.addResult("jvm-thread-pool", jvmThreadPool.monitor()); return result; &#125; public JVMInfo getJvmInfo() &#123; return jvmInfo; &#125; public JVMGC getJvmgc() &#123; return jvmgc; &#125; public JVMMemory getJvmMemory() &#123; return jvmMemory; &#125; public JVMThread getJvmThread() &#123; return jvmThread; &#125; public JVMThreadPool getJvmThreadPool() &#123; return jvmThreadPool; &#125;&#125; 线程池信息收集获取各个线程池的信息，如corePoolSize、maxPoolSize、activeCount(workingThread)、poolSize(workThread)、queueSize(blockedTask) 组装为map.put(“event-bus”,poolInfo); JVMThreadPool.java12345678910111213141516171819202122232425262728293031323334353637package com.mpush.monitor.quota.impl;import com.mpush.monitor.quota.ThreadPoolQuota;import com.mpush.monitor.service.ThreadPoolManager;import io.netty.channel.EventLoopGroup;import java.util.HashMap;import java.util.Map;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;import static com.mpush.tools.Utils.getPoolInfo;public class JVMThreadPool implements ThreadPoolQuota &#123; private final ThreadPoolManager threadPoolManager; public JVMThreadPool(ThreadPoolManager threadPoolManager) &#123; this.threadPoolManager = threadPoolManager; &#125; @Override public Object monitor(Object... args) &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(); Map&lt;String, Executor&gt; pools = threadPoolManager.getActivePools(); for (Map.Entry&lt;String, Executor&gt; entry : pools.entrySet()) &#123; String serviceName = entry.getKey(); Executor executor = entry.getValue(); if (executor instanceof ThreadPoolExecutor) &#123; result.put(serviceName, getPoolInfo((ThreadPoolExecutor) executor)); &#125; else if (executor instanceof EventLoopGroup) &#123; result.put(serviceName, getPoolInfo((EventLoopGroup) executor)); &#125; &#125; return result; &#125;&#125; 线程池使用ThreadPoolManager.java 线程池管理者，有5种线程池，包括MQ/event-bus/push-client-timer/push-task-timer/ack-timer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.mpush.monitor.service;import com.mpush.api.spi.common.ExecutorFactory;import com.mpush.tools.thread.NamedThreadFactory;import io.netty.channel.EventLoopGroup;import io.netty.channel.SingleThreadEventLoop;import io.netty.util.concurrent.EventExecutor;import io.netty.util.concurrent.ThreadProperties;import java.util.HashMap;import java.util.Map;import java.util.Objects;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.Executor;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.ThreadPoolExecutor;public final class ThreadPoolManager &#123; private final ExecutorFactory executorFactory = ExecutorFactory.create(); private final Map&lt;String, Executor&gt; pools = new ConcurrentHashMap&lt;&gt;(); public Executor getRedisExecutor() &#123; return pools.computeIfAbsent("mq", s -&gt; executorFactory.get(ExecutorFactory.MQ)); &#125; public Executor getEventBusExecutor() &#123; return pools.computeIfAbsent("event-bus", s -&gt; executorFactory.get(ExecutorFactory.EVENT_BUS)); &#125; public ScheduledExecutorService getPushClientTimer() &#123; return (ScheduledExecutorService) pools.computeIfAbsent("push-client-timer" , s -&gt; executorFactory.get(ExecutorFactory.PUSH_CLIENT)); &#125; public ScheduledExecutorService getPushTaskTimer() &#123; return (ScheduledExecutorService) pools.computeIfAbsent("push-task-timer" , s -&gt; executorFactory.get(ExecutorFactory.PUSH_TASK)); &#125; public ScheduledExecutorService getAckTimer() &#123; return (ScheduledExecutorService) pools.computeIfAbsent("ack-timer" , s -&gt; executorFactory.get(ExecutorFactory.ACK_TIMER)); &#125; public void register(String name, Executor executor) &#123; Objects.requireNonNull(name); Objects.requireNonNull(executor); pools.put(name, executor); &#125; public Map&lt;String, Executor&gt; getActivePools() &#123; return pools; &#125;&#125; ClientExecutorFactory.java 客户端线程池工厂 mpush-client/resources/META-INF/services/com.mpush.api.spi.common.ExecutorFactory1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.mpush.client;import com.mpush.api.spi.Spi;import com.mpush.common.CommonExecutorFactory;import com.mpush.tools.log.Logs;import com.mpush.tools.thread.NamedPoolThreadFactory;import java.util.concurrent.Executor;import java.util.concurrent.ScheduledThreadPoolExecutor;import static com.mpush.tools.config.CC.mp.thread.pool.ack_timer;import static com.mpush.tools.config.CC.mp.thread.pool.push_client;import static com.mpush.tools.thread.ThreadNames.T_ARK_REQ_TIMER;import static com.mpush.tools.thread.ThreadNames.T_PUSH_CLIENT_TIMER;/** * 此线程池可伸缩，线程空闲一定时间后回收，新请求重新创建线程 */@Spi(order = 1)public final class ClientExecutorFactory extends CommonExecutorFactory &#123; @Override public Executor get(String name) &#123; switch (name) &#123; case PUSH_CLIENT: &#123; ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(push_client , new NamedPoolThreadFactory(T_PUSH_CLIENT_TIMER), (r, e) -&gt; r.run() // run caller thread ); executor.setRemoveOnCancelPolicy(true); return executor; &#125; case ACK_TIMER: &#123; ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(ack_timer, new NamedPoolThreadFactory(T_ARK_REQ_TIMER), (r, e) -&gt; Logs.PUSH.error("one ack context was rejected, context=" + r) ); executor.setRemoveOnCancelPolicy(true); return executor; &#125; default: return super.get(name); &#125; &#125;&#125; ServerExecutorFactory 服务端线程池工厂 mpush-core/resources/META-INF/services/com.mpush.api.spi.common.ExecutorFactory12345678910111213141516171819202122232425262728293031323334353637383940/** * 此线程池可伸缩，线程空闲一定时间后回收，新请求重新创建线程 */@Spi(order = 1)public final class ServerExecutorFactory extends CommonExecutorFactory &#123; @Override public Executor get(String name) &#123; final ThreadPoolConfig config; switch (name) &#123; case MQ: config = ThreadPoolConfig .build(T_MQ) .setCorePoolSize(CC.mp.thread.pool.mq.min) .setMaxPoolSize(CC.mp.thread.pool.mq.max) .setKeepAliveSeconds(TimeUnit.SECONDS.toSeconds(10)) .setQueueCapacity(CC.mp.thread.pool.mq.queue_size) .setRejectedPolicy(ThreadPoolConfig.REJECTED_POLICY_CALLER_RUNS); break; case PUSH_TASK: return new ScheduledThreadPoolExecutor(push_task, new NamedPoolThreadFactory(T_PUSH_CENTER_TIMER), (r, e) -&gt; &#123; throw new PushException("one push task was rejected. task=" + r); &#125; ); case ACK_TIMER: &#123; ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(ack_timer, new NamedPoolThreadFactory(T_ARK_REQ_TIMER), (r, e) -&gt; Logs.PUSH.error("one ack context was rejected, context=" + r) ); executor.setRemoveOnCancelPolicy(true); return executor; &#125; default: return super.get(name); &#125; return get(config); &#125;&#125; ExecutorFactory.java 线程池工厂接口(SPI)123456789101112131415161718192021222324package com.mpush.api.spi.common;import com.mpush.api.spi.SpiLoader;import java.util.concurrent.Executor;/** * Created by yxx on 2016/5/20. * * @author ohun@live.cn */public interface ExecutorFactory &#123; String PUSH_CLIENT = "push-client"; String PUSH_TASK = "push-task"; String ACK_TIMER = "ack-timer"; String EVENT_BUS = "event-bus"; String MQ = "mq"; Executor get(String name); static ExecutorFactory create() &#123; return SpiLoader.load(ExecutorFactory.class); &#125;&#125; CommonExecutorFactory.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.mpush.common;import com.mpush.api.spi.common.ExecutorFactory;import com.mpush.tools.config.CC;import com.mpush.tools.log.Logs;import com.mpush.tools.thread.NamedPoolThreadFactory;import com.mpush.tools.thread.pool.DefaultExecutor;import com.mpush.tools.thread.pool.DumpThreadRejectedHandler;import com.mpush.tools.thread.pool.ThreadPoolConfig;import java.util.concurrent.BlockingQueue;import java.util.concurrent.Executor;import java.util.concurrent.ScheduledThreadPoolExecutor;import java.util.concurrent.TimeUnit;import static com.mpush.tools.config.CC.mp.thread.pool.ack_timer;import static com.mpush.tools.config.CC.mp.thread.pool.push_client;import static com.mpush.tools.thread.ThreadNames.T_ARK_REQ_TIMER;import static com.mpush.tools.thread.ThreadNames.T_EVENT_BUS;import static com.mpush.tools.thread.ThreadNames.T_PUSH_CLIENT_TIMER;/** * Created by ohun on 2017/7/15. * * @author ohun@live.cn (夜色) */public class CommonExecutorFactory implements ExecutorFactory &#123; protected Executor get(ThreadPoolConfig config) &#123; String name = config.getName(); int corePoolSize = config.getCorePoolSize(); int maxPoolSize = config.getMaxPoolSize(); int keepAliveSeconds = config.getKeepAliveSeconds(); BlockingQueue&lt;Runnable&gt; queue = config.getQueue(); return new DefaultExecutor(corePoolSize , maxPoolSize , keepAliveSeconds , TimeUnit.SECONDS , queue , new NamedPoolThreadFactory(name) , new DumpThreadRejectedHandler(config)); &#125; @Override public Executor get(String name) &#123; final ThreadPoolConfig config; switch (name) &#123; case EVENT_BUS: config = ThreadPoolConfig .build(T_EVENT_BUS) .setCorePoolSize(CC.mp.thread.pool.event_bus.min) .setMaxPoolSize(CC.mp.thread.pool.event_bus.max) .setKeepAliveSeconds(TimeUnit.SECONDS.toSeconds(10)) .setQueueCapacity(CC.mp.thread.pool.event_bus.queue_size) .setRejectedPolicy(ThreadPoolConfig.REJECTED_POLICY_CALLER_RUNS); break; case PUSH_CLIENT: &#123; ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(push_client , new NamedPoolThreadFactory(T_PUSH_CLIENT_TIMER), (r, e) -&gt; r.run() // run caller thread ); executor.setRemoveOnCancelPolicy(true); return executor; &#125; case ACK_TIMER: &#123; ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(ack_timer, new NamedPoolThreadFactory(T_ARK_REQ_TIMER), (r, e) -&gt; Logs.PUSH.error("one ack context was rejected, context=" + r) ); executor.setRemoveOnCancelPolicy(true); return executor; &#125; default: throw new IllegalArgumentException("no executor for " + name); &#125; return get(config); &#125;&#125; ThreadPoolConfig.java 线程池配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135/* * (C) Copyright 2015-2016 the original author or authors. * * Licensed under the Apache License, Version 2.0 (the "License"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. * * Contributors: * ohun@live.cn (夜色) */package com.mpush.tools.thread.pool;import java.util.concurrent.BlockingQueue;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.SynchronousQueue;public final class ThreadPoolConfig &#123; public static final int REJECTED_POLICY_ABORT = 0; public static final int REJECTED_POLICY_DISCARD = 1; public static final int REJECTED_POLICY_CALLER_RUNS = 2; private String name;//名字 private int corePoolSize; //最小线程大小 private int maxPoolSize; //最大线程大小 private int queueCapacity; // 允许缓冲在队列中的任务数 (0:不缓冲、负数：无限大、正数：缓冲的任务数) private int keepAliveSeconds;// 存活时间 private int rejectedPolicy = REJECTED_POLICY_ABORT; public ThreadPoolConfig(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public ThreadPoolConfig setName(String name) &#123; this.name = name; return this; &#125; public int getCorePoolSize() &#123; return corePoolSize; &#125; public ThreadPoolConfig setCorePoolSize(int corePoolSize) &#123; this.corePoolSize = corePoolSize; return this; &#125; public int getMaxPoolSize() &#123; return maxPoolSize; &#125; public ThreadPoolConfig setMaxPoolSize(int maxPoolSize) &#123; this.maxPoolSize = maxPoolSize; return this; &#125; public int getQueueCapacity() &#123; return queueCapacity; &#125; public ThreadPoolConfig setQueueCapacity(int queueCapacity) &#123; this.queueCapacity = queueCapacity; return this; &#125; public int getKeepAliveSeconds() &#123; return keepAliveSeconds; &#125; public ThreadPoolConfig setKeepAliveSeconds(long keepAliveSeconds) &#123; this.keepAliveSeconds = (int) keepAliveSeconds; return this; &#125; public int getRejectedPolicy() &#123; return rejectedPolicy; &#125; public ThreadPoolConfig setRejectedPolicy(int rejectedPolicy) &#123; this.rejectedPolicy = rejectedPolicy; return this; &#125; public static ThreadPoolConfig buildFixed(String name, int threads, int queueCapacity) &#123; return new ThreadPoolConfig(name) .setCorePoolSize(threads) .setMaxPoolSize(threads) .setQueueCapacity(queueCapacity) .setKeepAliveSeconds(0); &#125; public static ThreadPoolConfig buildCached(String name) &#123; return new ThreadPoolConfig(name) .setKeepAliveSeconds(0); &#125; public static ThreadPoolConfig build(String name) &#123; return new ThreadPoolConfig(name); &#125; public BlockingQueue&lt;Runnable&gt; getQueue() &#123; BlockingQueue&lt;Runnable&gt; blockingQueue; if (queueCapacity == 0) &#123; blockingQueue = new SynchronousQueue&lt;&gt;(); &#125; else if (queueCapacity &lt; 0) &#123; blockingQueue = new LinkedBlockingQueue&lt;&gt;(); &#125; else &#123; blockingQueue = new LinkedBlockingQueue&lt;&gt;(queueCapacity); &#125; return blockingQueue; &#125; @Override public String toString() &#123; return "ThreadPoolConfig&#123;" + "name='" + name + '\'' + ", corePoolSize=" + corePoolSize + ", maxPoolSize=" + maxPoolSize + ", queueCapacity=" + queueCapacity + ", keepAliveSeconds=" + keepAliveSeconds + '&#125;'; &#125;&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UserEventConsumer 用户事件订阅]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E8%B7%AF%E7%94%B1%E4%B8%AD%E5%BF%83%2FUserEventConsumer%20%E7%94%A8%E6%88%B7%E4%BA%8B%E4%BB%B6%E8%AE%A2%E9%98%85%2F</url>
    <content type="text"><![CDATA[用户在线列表的key为 mp:oul:127.0.0.11、初始化UserManager踢人、清空在线用户列表、将用户添加到在线列表中、从在线列表中删除用户；统计在线用户数量、获取在线用户列表；2、订阅用户在线事件UserOnlineEvent将用户添加到在线列表中；发布MQ在线消息ONLINE_CHANNEL给订阅方；3、订阅用户离线事件UserOfflineEvent从在线列表中删除用户；发布MQ离线消息OFFLINE_CHANNEL给订阅方； UserEventConsumer.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.mpush.core.router;import com.google.common.eventbus.AllowConcurrentEvents;import com.google.common.eventbus.Subscribe;import com.mpush.api.event.UserOfflineEvent;import com.mpush.api.event.UserOnlineEvent;import com.mpush.api.spi.common.MQClient;import com.mpush.api.spi.common.MQClientFactory;import com.mpush.common.router.RemoteRouterManager;import com.mpush.common.user.UserManager;import com.mpush.tools.event.EventConsumer;import static com.mpush.api.event.Topics.OFFLINE_CHANNEL;import static com.mpush.api.event.Topics.ONLINE_CHANNEL;/** * Created by ohun on 2015/12/23. * * @author ohun@live.cn */public final class UserEventConsumer extends EventConsumer &#123; private final MQClient mqClient = MQClientFactory.create(); private final UserManager userManager; public UserEventConsumer(RemoteRouterManager remoteRouterManager) &#123; this.userManager = new UserManager(remoteRouterManager); &#125; @Subscribe @AllowConcurrentEvents void on(UserOnlineEvent event) &#123; userManager.addToOnlineList(event.getUserId()); mqClient.publish(ONLINE_CHANNEL, event.getUserId()); &#125; @Subscribe @AllowConcurrentEvents void on(UserOfflineEvent event) &#123; userManager.remFormOnlineList(event.getUserId()); mqClient.publish(OFFLINE_CHANNEL, event.getUserId()); &#125; public UserManager getUserManager() &#123; return userManager; &#125;&#125; UserManager.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * 在线列表是存在redis里的，服务被kill -9的时候，无法修改redis。 * 查询全部在线列表的时候，要通过当前ZK里可用的机器来循环查询。 * 每台机器的在线列表是分开存的，如果都存储在一起，某台机器挂了，反而不好处理。 */public final class UserManager &#123; private static final Logger LOGGER = LoggerFactory.getLogger(UserManager.class); private final String onlineUserListKey = CacheKeys.getOnlineUserListKey(ConfigTools.getPublicIp()); private final CacheManager cacheManager = CacheManagerFactory.create(); private final MQClient mqClient = MQClientFactory.create(); private final RemoteRouterManager remoteRouterManager; public UserManager(RemoteRouterManager remoteRouterManager) &#123; this.remoteRouterManager = remoteRouterManager; &#125; public void kickUser(String userId) &#123; kickUser(userId, -1); &#125; public void kickUser(String userId, int clientType) &#123; Set&lt;RemoteRouter&gt; remoteRouters = remoteRouterManager.lookupAll(userId); if (remoteRouters != null) &#123; for (RemoteRouter remoteRouter : remoteRouters) &#123; ClientLocation location = remoteRouter.getRouteValue(); if (clientType == -1 || location.getClientType() == clientType) &#123; MQKickRemoteMsg message = new MQKickRemoteMsg() .setUserId(userId) .setClientType(location.getClientType()) .setConnId(location.getConnId()) .setDeviceId(location.getDeviceId()) .setTargetServer(location.getHost()) .setTargetPort(location.getPort()); mqClient.publish(Constants.getKickChannel(location.getHostAndPort()), message); &#125; &#125; &#125; &#125; public void clearOnlineUserList() &#123; cacheManager.del(onlineUserListKey); &#125; public void addToOnlineList(String userId) &#123; cacheManager.zAdd(onlineUserListKey, userId); LOGGER.info("user online &#123;&#125;", userId); &#125; public void remFormOnlineList(String userId) &#123; cacheManager.zRem(onlineUserListKey, userId); LOGGER.info("user offline &#123;&#125;", userId); &#125; //在线用户数量 public long getOnlineUserNum() &#123; Long value = cacheManager.zCard(onlineUserListKey); return value == null ? 0 : value; &#125; //在线用户数量 public long getOnlineUserNum(String publicIP) &#123; String online_key = CacheKeys.getOnlineUserListKey(publicIP); Long value = cacheManager.zCard(online_key); return value == null ? 0 : value; &#125; //在线用户列表 public List&lt;String&gt; getOnlineUserList(String publicIP, int start, int end) &#123; String key = CacheKeys.getOnlineUserListKey(publicIP); return cacheManager.zrange(key, start, end, String.class); &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>路由中心</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1-握手消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F1-%E6%8F%A1%E6%89%8B%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2-快速连接消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F2-%E5%BF%AB%E9%80%9F%E8%BF%9E%E6%8E%A5%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-踢人消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F3-%E8%B8%A2%E4%BA%BA%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6-心跳消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F6-%E5%BF%83%E8%B7%B3%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5-推送消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F5-%E6%8E%A8%E9%80%81%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7-成功消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F7-%E6%88%90%E5%8A%9F%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0-关于]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2Fmpush-client-java%2F0-%E5%85%B3%E4%BA%8E%2F</url>
    <content type="text"><![CDATA[mpush-client-java是一个纯java实现的一个MPUS客户端，不依赖其他任何第三方框架。主要用于android sdk底层通信，该工程本身不包含任何android相关代码。 参考：https://github.com/mpusher/mpush-client-java mpush-client-java文章目录: 0-关于 1-启动-建立连接 2-读数据 3-写数据 4-握手]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>mpush-client-java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[8-HTTP代理消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F8-HTTP%E4%BB%A3%E7%90%86%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4-错误消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F4-%E9%94%99%E8%AF%AF%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1-启动-建立连接]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2Fmpush-client-java%2F1-%E5%90%AF%E5%8A%A8-%E5%BB%BA%E7%AB%8B%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[mpush-client-java文章目录: 0-关于 1-启动-建立连接 2-读数据 3-写数据 4-握手]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>mpush-client-java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4-握手]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2Fmpush-client-java%2F4-%E6%8F%A1%E6%89%8B%2F</url>
    <content type="text"><![CDATA[mpush-client-java文章目录: 0-关于 1-启动-建立连接 2-读数据 3-写数据 4-握手]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>mpush-client-java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云到端-推送]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2FTCP%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F%E4%BA%91%E5%88%B0%E7%AB%AF-%E6%8E%A8%E9%80%81%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>TCP网关服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接收MPNS PUSH消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2FTCP%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F%E6%8E%A5%E6%94%B6MPNS%20PUSH%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[GatewayServer服务，接收MPNS的消息种类如下：12//GatewayServer#init()messageDispatcher.register(Command.GATEWAY_PUSH, () -&gt; new GatewayPushHandler(mPushServer.getPushCenter())); 处理MPNS来的消息：1234567891011121314public final class GatewayPushHandler extends BaseMessageHandler&lt;GatewayPushMessage&gt; &#123; private final PushCenter pushCenter; public GatewayPushHandler(PushCenter pushCenter) &#123; this.pushCenter = pushCenter; &#125; @Override public GatewayPushMessage decode(Packet packet, Connection connection) &#123; return new GatewayPushMessage(packet, connection); &#125; @Override public void handle(GatewayPushMessage message) &#123; pushCenter.push(message); &#125;&#125; 网关服务启动类 GatewayServer.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131public final class GatewayServer extends NettyTCPServer &#123; private ServerChannelHandler channelHandler; private ConnectionManager connectionManager; private MessageDispatcher messageDispatcher; private GlobalChannelTrafficShapingHandler trafficShapingHandler; private ScheduledExecutorService trafficShapingExecutor; private MPushServer mPushServer; public GatewayServer(MPushServer mPushServer) &#123; super(gateway_server_port, gateway_server_bind_ip); this.mPushServer = mPushServer; this.messageDispatcher = new MessageDispatcher(); this.connectionManager = new ServerConnectionManager(false); this.channelHandler = new ServerChannelHandler(false, connectionManager, messageDispatcher); &#125; @Override public void init() &#123; super.init(); messageDispatcher.register(Command.GATEWAY_PUSH, () -&gt; new GatewayPushHandler(mPushServer.getPushCenter())); if (CC.mp.net.traffic_shaping.gateway_server.enabled) &#123;//启用流量整形，限流 trafficShapingExecutor = Executors.newSingleThreadScheduledExecutor(new NamedPoolThreadFactory(T_TRAFFIC_SHAPING)); trafficShapingHandler = new GlobalChannelTrafficShapingHandler( trafficShapingExecutor, write_global_limit, read_global_limit, write_channel_limit, read_channel_limit, check_interval); &#125; &#125; @Override public void stop(Listener listener) &#123; super.stop(listener); if (trafficShapingHandler != null) &#123; trafficShapingHandler.release(); trafficShapingExecutor.shutdown(); &#125; if (connectionManager != null) &#123; connectionManager.destroy(); &#125; &#125; @Override protected String getBossThreadName() &#123; return ThreadNames.T_GATEWAY_BOSS; &#125; @Override protected String getWorkThreadName() &#123; return ThreadNames.T_GATEWAY_WORKER; &#125; @Override protected int getIoRate() &#123; return 100; &#125; @Override protected int getWorkThreadNum() &#123; return CC.mp.thread.pool.gateway_server_work; &#125; @Override protected void initPipeline(ChannelPipeline pipeline) &#123; super.initPipeline(pipeline); if (trafficShapingHandler != null) &#123; pipeline.addFirst(trafficShapingHandler); &#125; &#125; @Override protected void initOptions(ServerBootstrap b) &#123; super.initOptions(b); if (snd_buf.gateway_server &gt; 0) b.childOption(ChannelOption.SO_SNDBUF, snd_buf.gateway_server); if (rcv_buf.gateway_server &gt; 0) b.childOption(ChannelOption.SO_RCVBUF, rcv_buf.gateway_server); /** * 这个坑其实也不算坑，只是因为懒，该做的事情没做。一般来讲我们的业务如果比较小的时候我们用同步处理，等业务到一定规模的时候，一个优化手段就是异步化。 * 异步化是提高吞吐量的一个很好的手段。但是，与异步相比，同步有天然的负反馈机制，也就是如果后端慢了，前面也会跟着慢起来，可以自动的调节。 * 但是异步就不同了，异步就像决堤的大坝一样，洪水是畅通无阻。如果这个时候没有进行有效的限流措施就很容易把后端冲垮。 * 如果一下子把后端冲垮倒也不是最坏的情况，就怕把后端冲的要死不活。 * 这个时候，后端就会变得特别缓慢，如果这个时候前面的应用使用了一些无界的资源等，就有可能把自己弄死。 * 那么现在要介绍的这个坑就是关于Netty里的ChannelOutboundBuffer这个东西的。 * 这个buffer是用在netty向channel write数据的时候，有个buffer缓冲，这样可以提高网络的吞吐量(每个channel有一个这样的buffer)。 * 初始大小是32(32个元素，不是指字节)，但是如果超过32就会翻倍，一直增长。 * 大部分时候是没有什么问题的，但是在碰到对端非常慢(对端慢指的是对端处理TCP包的速度变慢，比如对端负载特别高的时候就有可能是这个情况)的时候就有问题了， * 这个时候如果还是不断地写数据，这个buffer就会不断地增长，最后就有可能出问题了(我们的情况是开始吃swap，最后进程被linux killer干掉了)。 * 为什么说这个地方是坑呢，因为大部分时候我们往一个channel写数据会判断channel是否active，但是往往忽略了这种慢的情况。 * * 那这个问题怎么解决呢？其实ChannelOutboundBuffer虽然无界，但是可以给它配置一个高水位线和低水位线， * 当buffer的大小超过高水位线的时候对应channel的isWritable就会变成false， * 当buffer的大小低于低水位线的时候，isWritable就会变成true。所以应用应该判断isWritable，如果是false就不要再写数据了。 * 高水位线和低水位线是字节数，默认高水位是64K，低水位是32K，我们可以根据我们的应用需要支持多少连接数和系统资源进行合理规划。 */ if (gateway_server_low &gt; 0 &amp;&amp; gateway_server_high &gt; 0) &#123; b.childOption(ChannelOption.WRITE_BUFFER_WATER_MARK, new WriteBufferWaterMark( gateway_server_low, gateway_server_high )); &#125; &#125; @Override public ChannelFactory&lt;? extends ServerChannel&gt; getChannelFactory() &#123; if (CC.mp.net.tcpGateway()) return super.getChannelFactory(); if (CC.mp.net.udtGateway()) return NioUdtProvider.BYTE_ACCEPTOR; if (CC.mp.net.sctpGateway()) return NioSctpServerChannel::new; return super.getChannelFactory(); &#125; @Override public SelectorProvider getSelectorProvider() &#123; if (CC.mp.net.tcpGateway()) return super.getSelectorProvider(); if (CC.mp.net.udtGateway()) return NioUdtProvider.BYTE_PROVIDER; if (CC.mp.net.sctpGateway()) return super.getSelectorProvider(); return super.getSelectorProvider(); &#125; @Override public ChannelHandler getChannelHandler() &#123; return channelHandler; &#125; public ConnectionManager getConnectionManager() &#123; return connectionManager; &#125; public MessageDispatcher getMessageDispatcher() &#123; return messageDispatcher; &#125;&#125; 网关服务channel处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@ChannelHandler.Sharablepublic final class ServerChannelHandler extends ChannelInboundHandlerAdapter &#123; private static final Logger LOGGER = LoggerFactory.getLogger(ServerChannelHandler.class); private static final long profile_slowly_limit = CC.mp.monitor.profile_slowly_duration.toMillis(); private final boolean security; //是否启用加密 private final ConnectionManager connectionManager; private final PacketReceiver receiver; public ServerChannelHandler(boolean security, ConnectionManager connectionManager, PacketReceiver receiver) &#123; this.security = security; this.connectionManager = connectionManager; this.receiver = receiver; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Packet packet = (Packet) msg; byte cmd = packet.cmd; try &#123; Profiler.start("time cost on [channel read]: ", packet.toString()); Connection connection = connectionManager.get(ctx.channel()); LOGGER.debug("channelRead conn=&#123;&#125;, packet=&#123;&#125;", ctx.channel(), connection.getSessionContext(), msg); connection.updateLastReadTime(); receiver.onReceive(packet, connection); &#125; finally &#123; Profiler.release(); if (Profiler.getDuration() &gt; profile_slowly_limit) &#123; Logs.PROFILE.info("Read Packet[cmd=&#123;&#125;] Slowly: \n&#123;&#125;", Command.toCMD(cmd), Profiler.dump()); &#125; Profiler.reset(); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; Connection connection = connectionManager.get(ctx.channel()); Logs.CONN.error("client caught ex, conn=&#123;&#125;", connection); LOGGER.error("caught an ex, channel=&#123;&#125;, conn=&#123;&#125;", ctx.channel(), connection, cause); ctx.close(); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Logs.CONN.info("client connected conn=&#123;&#125;", ctx.channel()); Connection connection = new NettyConnection(); connection.init(ctx.channel(), security); connectionManager.add(connection); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; Connection connection = connectionManager.removeAndClose(ctx.channel()); EventBus.post(new ConnectionCloseEvent(connection)); Logs.CONN.info("client disconnected conn=&#123;&#125;", connection); &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>TCP网关服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-写数据]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2Fmpush-client-java%2F3-%E5%86%99%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[mpush-client-java文章目录: 0-关于 1-启动-建立连接 2-读数据 3-写数据 4-握手]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>mpush-client-java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2-读数据]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2Fmpush-client-java%2F2-%E8%AF%BB%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[mpush-client-java文章目录: 0-关于 1-启动-建立连接 2-读数据 3-写数据 4-握手]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>mpush-client-java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[握手-绑定用户]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2FConnClient(%E6%A8%A1%E6%8B%9F%E5%AE%A2%E6%88%B7%E7%AB%AF)%2F%E6%8F%A1%E6%89%8B-%E7%BB%91%E5%AE%9A%E7%94%A8%E6%88%B7%2F</url>
    <content type="text"><![CDATA[ConnClient(模拟客户端)文章目录: 握手-绑定用户 1-握手消息 2-快速连接消息 3-踢人消息 4-错误消息 5-推送消息 6-心跳消息 7-成功消息 8-HTTP代理消息]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>ConnClient(模拟客户端)</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接收客户端消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2Fwebsocket%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F%E6%8E%A5%E6%94%B6%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[websocket服务，接收客户端的消息种类如下：123456//WebsocketServer#init()messageDispatcher.register(Command.HANDSHAKE, () -&gt; new HandshakeHandler(mPushServer));messageDispatcher.register(Command.BIND, () -&gt; new BindUserHandler(mPushServer));messageDispatcher.register(Command.UNBIND, () -&gt; new BindUserHandler(mPushServer));messageDispatcher.register(Command.PUSH, PushHandlerFactory::create);messageDispatcher.register(Command.ACK, () -&gt; new AckHandler(mPushServer)); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public final class WebsocketServer extends NettyTCPServer &#123; private final ChannelHandler channelHandler; private final MessageDispatcher messageDispatcher; private final ConnectionManager connectionManager; private final MPushServer mPushServer; public WebsocketServer(MPushServer mPushServer) &#123; super(CC.mp.net.ws_server_port); this.mPushServer = mPushServer; this.messageDispatcher = new MessageDispatcher(); this.connectionManager = new ServerConnectionManager(false); this.channelHandler = new WebSocketChannelHandler(connectionManager, messageDispatcher); &#125; @Override public void init() &#123; super.init(); connectionManager.init(); messageDispatcher.register(Command.HANDSHAKE, () -&gt; new HandshakeHandler(mPushServer)); messageDispatcher.register(Command.BIND, () -&gt; new BindUserHandler(mPushServer)); messageDispatcher.register(Command.UNBIND, () -&gt; new BindUserHandler(mPushServer)); messageDispatcher.register(Command.PUSH, PushHandlerFactory::create); messageDispatcher.register(Command.ACK, () -&gt; new AckHandler(mPushServer)); &#125; @Override public void stop(Listener listener) &#123; super.stop(listener); connectionManager.destroy(); &#125; @Override public EventLoopGroup getBossGroup() &#123; return mPushServer.getConnectionServer().getBossGroup(); &#125; @Override public EventLoopGroup getWorkerGroup() &#123; return mPushServer.getConnectionServer().getWorkerGroup(); &#125; @Override protected void initPipeline(ChannelPipeline pipeline) &#123; pipeline.addLast(new HttpServerCodec()); pipeline.addLast(new HttpObjectAggregator(65536)); pipeline.addLast(new WebSocketServerCompressionHandler()); pipeline.addLast(new WebSocketServerProtocolHandler(CC.mp.net.ws_path, null, true)); pipeline.addLast(new WebSocketIndexPageHandler()); pipeline.addLast(getChannelHandler()); &#125; @Override protected void initOptions(ServerBootstrap b) &#123; super.initOptions(b); b.option(ChannelOption.SO_BACKLOG, 1024); b.childOption(ChannelOption.SO_SNDBUF, 32 * 1024); b.childOption(ChannelOption.SO_RCVBUF, 32 * 1024); &#125; @Override public ChannelHandler getChannelHandler() &#123; return channelHandler; &#125; public ConnectionManager getConnectionManager() &#123; return connectionManager; &#125; public MessageDispatcher getMessageDispatcher() &#123; return messageDispatcher; &#125;&#125; WebSocketChannelHandler.java 处理socket消息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@ChannelHandler.Sharablepublic class WebSocketChannelHandler extends SimpleChannelInboundHandler&lt;WebSocketFrame&gt; &#123; private static final Logger LOGGER = LoggerFactory.getLogger(WebSocketChannelHandler.class); private final ConnectionManager connectionManager; private final PacketReceiver receiver; public WebSocketChannelHandler(ConnectionManager connectionManager, PacketReceiver receiver) &#123; this.connectionManager = connectionManager; this.receiver = receiver; &#125; //接收消息 @Override protected void channelRead0(ChannelHandlerContext ctx, WebSocketFrame frame) throws Exception &#123; if (frame instanceof TextWebSocketFrame) &#123; String text = ((TextWebSocketFrame) frame).text(); Connection connection = connectionManager.get(ctx.channel()); Packet packet = PacketDecoder.decodeFrame(text); LOGGER.debug("channelRead conn=&#123;&#125;, packet=&#123;&#125;", ctx.channel(), connection.getSessionContext(), packet); receiver.onReceive(packet, connection); &#125; else &#123; String message = "unsupported frame type: " + frame.getClass().getName(); throw new UnsupportedOperationException(message); &#125; &#125; //连接异常 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; Connection connection = connectionManager.get(ctx.channel()); Logs.CONN.error("client caught ex, conn=&#123;&#125;", connection); LOGGER.error("caught an ex, channel=&#123;&#125;, conn=&#123;&#125;", ctx.channel(), connection, cause); ctx.close(); &#125; //建立连接 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Logs.CONN.info("client connected conn=&#123;&#125;", ctx.channel()); Connection connection = new NettyConnection(); connection.init(ctx.channel(), false); connectionManager.add(connection); &#125; //断开连接 @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; Connection connection = connectionManager.removeAndClose(ctx.channel()); EventBus.post(new ConnectionCloseEvent(connection)); Logs.CONN.info("client disconnected conn=&#123;&#125;", connection); &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>websocket接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接收MPNS 广播踢人消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2FUDP%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F%E6%8E%A5%E6%94%B6MPNS%20%E5%B9%BF%E6%92%AD%E8%B8%A2%E4%BA%BA%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[GatewayUDPConnector服务，接收MPNS的消息种类如下：12345//GatewayUDPConnector#init()//PUSH消息messageDispatcher.register(Command.GATEWAY_PUSH, () -&gt; new GatewayPushHandler(mPushServer.getPushCenter()));//踢人消息messageDispatcher.register(Command.GATEWAY_KICK, () -&gt; new GatewayKickUserHandler(mPushServer.getRouterCenter())); 处理踢人消息：1234567891011121314public final class GatewayKickUserHandler extends BaseMessageHandler&lt;GatewayKickUserMessage&gt; &#123; private final RouterCenter routerCenter; public GatewayKickUserHandler(RouterCenter routerCenter) &#123; this.routerCenter = routerCenter; &#125; @Override public GatewayKickUserMessage decode(Packet packet, Connection connection) &#123; return new GatewayKickUserMessage(packet, connection); &#125; @Override public void handle(GatewayKickUserMessage message) &#123; routerCenter.getRouterChangeListener().onReceiveKickRemoteMsg(message); &#125;&#125; RouterChangeListener.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 处理远程机器发送的踢人广播. * &lt;p&gt; * 一台机器发送广播所有的机器都能收到， * 包括发送广播的机器，所有要做一次过滤 * * @param msg */public void onReceiveKickRemoteMsg(KickRemoteMsg msg) &#123; //1.如果当前机器不是目标机器，直接忽略 if (!mPushServer.isTargetMachine(msg.getTargetServer(), msg.getTargetPort())) &#123; Logs.CONN.error("receive kick remote msg, target server error, localIp=&#123;&#125;, msg=&#123;&#125;", ConfigTools.getLocalIp(), msg); return; &#125; //2.查询本地路由，找到要被踢下线的链接，并删除该本地路由 String userId = msg.getUserId(); int clientType = msg.getClientType(); LocalRouterManager localRouterManager = mPushServer.getRouterCenter().getLocalRouterManager(); LocalRouter localRouter = localRouterManager.lookup(userId, clientType); if (localRouter != null) &#123; Logs.CONN.info("receive kick remote msg, msg=&#123;&#125;", msg); if (localRouter.getRouteValue().getId().equals(msg.getConnId())) &#123;//二次校验，防止误杀 //fix 0.8.1 踢人的时候不再主动删除路由信息，只发踢人消息到客户端，路由信息有客户端主动解绑的时候再处理。 //2.1删除本地路由信息 //localRouterManager.unRegister(userId, clientType); //2.2发送踢人消息到客户端 sendKickUserMessage2Client(userId, localRouter); &#125; else &#123; Logs.CONN.warn("kick router failure target connId not match, localRouter=&#123;&#125;, msg=&#123;&#125;", localRouter, msg); &#125; &#125; else &#123; Logs.CONN.warn("kick router failure can't find local router, msg=&#123;&#125;", msg); &#125;&#125;/** * 发送踢人消息到客户端 * * @param userId 当前用户 * @param router 本地路由信息 */private void sendKickUserMessage2Client(final String userId, final LocalRouter router) &#123; Connection connection = router.getRouteValue(); SessionContext context = connection.getSessionContext(); KickUserMessage message = KickUserMessage.build(connection); message.deviceId = context.deviceId; message.userId = userId; message.send(future -&gt; &#123; if (future.isSuccess()) &#123; Logs.CONN.info("kick local connection success, userId=&#123;&#125;, router=&#123;&#125;, conn=&#123;&#125;", userId, router, connection); &#125; else &#123; Logs.CONN.warn("kick local connection failure, userId=&#123;&#125;, router=&#123;&#125;, conn=&#123;&#125;", userId, router, connection); &#125; &#125;);&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>UDP网关服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接收MPNS 广播PUSH消息]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2FUDP%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F%E6%8E%A5%E6%94%B6MPNS%20%E5%B9%BF%E6%92%ADPUSH%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[GatewayUDPConnector服务，接收MPNS的消息种类如下：12345//GatewayUDPConnector#init()//PUSH消息messageDispatcher.register(Command.GATEWAY_PUSH, () -&gt; new GatewayPushHandler(mPushServer.getPushCenter()));//踢人消息messageDispatcher.register(Command.GATEWAY_KICK, () -&gt; new GatewayKickUserHandler(mPushServer.getRouterCenter())); 处理PUSH消息：1234567891011121314public final class GatewayPushHandler extends BaseMessageHandler&lt;GatewayPushMessage&gt; &#123; private final PushCenter pushCenter; public GatewayPushHandler(PushCenter pushCenter) &#123; this.pushCenter = pushCenter; &#125; @Override public GatewayPushMessage decode(Packet packet, Connection connection) &#123; return new GatewayPushMessage(packet, connection); &#125; @Override public void handle(GatewayPushMessage message) &#123; pushCenter.push(message); &#125;&#125; 消息处理，走的是 BroadcastPushTask 任务1234567891011121314//PushCenter.java@Overridepublic void push(IPushMessage message) &#123; if (message.isBroadcast()) &#123; FlowControl flowControl = (message.getTaskId() == null) ? new FastFlowControl(limit, max, duration) : new RedisFlowControl(message.getTaskId(), max); //添加到自定义的PushTaskTimer线程池中执行该任务 addTask(new BroadcastPushTask(mPushServer, message, flowControl)); &#125; else &#123; //添加到GatewayUDPConnector的netty work 线程池中执行该任务 addTask(new SingleUserPushTask(mPushServer, message, globalFlowControl)); &#125;&#125; 自定义的PushTaskTimer线程池:123456789101112131415161718192021222324252627282930313233343536373839//PushCenter.java@Overrideprotected void doStart(Listener listener) throws Throwable &#123; this.pushListener = PushListenerFactory.create(); this.pushListener.init(mPushServer); if (CC.mp.net.udpGateway() || CC.mp.thread.pool.push_task &gt; 0) &#123; executor = new CustomJDKExecutor(mPushServer.getMonitor().getThreadPoolManager().getPushTaskTimer()); &#125; else &#123;//实际情况使用EventLoo并没有更快，还有待测试 executor = new NettyEventLoopExecutor(); &#125; MBeanRegistry.getInstance().register(new PushCenterBean(taskNum), null); ackTaskQueue.start(); logger.info("push center start success"); listener.onSuccess();&#125;/** * UDP 模式使用自定义线程池 */private static class CustomJDKExecutor implements PushTaskExecutor &#123; private final ScheduledExecutorService executorService; private CustomJDKExecutor(ScheduledExecutorService executorService) &#123; this.executorService = executorService; &#125; @Override public void shutdown() &#123; executorService.shutdown(); &#125; @Override public void addTask(PushTask task) &#123; executorService.execute(task); &#125; @Override public void delayTask(long delay, PushTask task) &#123; executorService.schedule(task, delay, TimeUnit.NANOSECONDS); &#125;&#125; 线程池调度任务时，执行 run() 方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public final class BroadcastPushTask implements PushTask &#123; private final long begin = System.currentTimeMillis(); private final AtomicInteger finishTasks = new AtomicInteger(0); private final TimeLine timeLine = new TimeLine(); private final Set&lt;String&gt; successUserIds = new HashSet&lt;&gt;(1024); private final FlowControl flowControl; private final IPushMessage message; private final Condition condition; private final MPushServer mPushServer; //使用Iterator, 记录任务遍历到的位置，因为有流控，一次任务可能会被分批发送，而且还有在推送过程中上/下线的用户 private final Iterator&lt;Map.Entry&lt;String, Map&lt;Integer, LocalRouter&gt;&gt;&gt; iterator; public BroadcastPushTask(MPushServer mPushServer, IPushMessage message, FlowControl flowControl) &#123; this.mPushServer = mPushServer; this.message = message; this.flowControl = flowControl; this.condition = message.getCondition(); this.iterator = mPushServer.getRouterCenter().getLocalRouterManager().routers().entrySet().iterator(); this.timeLine.begin("push-center-begin"); &#125; @Override public void run() &#123; flowControl.reset(); boolean done = broadcast(); if (done) &#123;//done 广播结束 if (finishTasks.addAndGet(flowControl.total()) == 0) &#123; report(); &#125; &#125; else &#123;//没有结束，就延时进行下次任务 TODO 考虑优先级问题 mPushServer.getPushCenter().delayTask(flowControl.getDelay(), this); &#125; flowControl.end(successUserIds.toArray(new String[successUserIds.size()])); &#125; private boolean broadcast() &#123; try &#123; iterator.forEachRemaining(entry -&gt; &#123; String userId = entry.getKey(); entry.getValue().forEach((clientType, router) -&gt; &#123; Connection connection = router.getRouteValue(); if (checkCondition(condition, connection)) &#123;//1.条件检测 if (connection.isConnected()) &#123; if (connection.getChannel().isWritable()) &#123; //检测TCP缓冲区是否已满且写队列超过最高阀值 PushMessage .build(connection) .setContent(message.getContent()) .send(future -&gt; operationComplete(future, userId)); //4. 检测qps, 是否超过流控限制，如果超过则结束当前循环直接进入catch if (!flowControl.checkQps()) &#123; throw new OverFlowException(false); &#125; &#125; &#125; else &#123; //2.如果链接失效，先删除本地失效的路由，再查下远程路由，看用户是否登陆到其他机器 Logs.PUSH.warn("[Broadcast] find router in local but conn disconnect, message=&#123;&#125;, conn=&#123;&#125;", message, connection); //删除已经失效的本地路由 mPushServer.getRouterCenter().getLocalRouterManager().unRegister(userId, clientType); &#125; &#125; &#125;); &#125;); &#125; catch (OverFlowException e) &#123; //超出最大限制，或者遍历完毕，结束广播 return e.isOverMaxLimit() || !iterator.hasNext(); &#125; return !iterator.hasNext();//遍历完毕, 广播结束 &#125; private void report() &#123; Logs.PUSH.info("[Broadcast] task finished, cost=&#123;&#125;, message=&#123;&#125;", (System.currentTimeMillis() - begin), message); mPushServer.getPushCenter().getPushListener().onBroadcastComplete(message, timeLine.end().getTimePoints());//通知发送方，广播推送完毕 &#125; private boolean checkCondition(Condition condition, Connection connection) &#123; if (condition == AwaysPassCondition.I) return true; SessionContext sessionContext = connection.getSessionContext(); Map&lt;String, Object&gt; env = new HashMap&lt;&gt;(); env.put("userId", sessionContext.userId); env.put("tags", sessionContext.tags); env.put("clientVersion", sessionContext.clientVersion); env.put("osName", sessionContext.osName); env.put("osVersion", sessionContext.osVersion); return condition.test(env); &#125; //@Override private void operationComplete(ChannelFuture future, String userId) throws Exception &#123; if (future.isSuccess()) &#123;//推送成功 successUserIds.add(userId); Logs.PUSH.info("[Broadcast] push message to client success, userId=&#123;&#125;, message=&#123;&#125;", message.getUserId(), message); &#125; else &#123;//推送失败 Logs.PUSH.warn("[Broadcast] push message to client failure, userId=&#123;&#125;, message=&#123;&#125;, conn=&#123;&#125;", message.getUserId(), message, future.channel()); &#125; if (finishTasks.decrementAndGet() == 0) &#123; report(); &#125; &#125; @Override public ScheduledExecutorService getExecutor() &#123; return ((Message) message).getConnection().getChannel().eventLoop(); &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>UDP网关服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UDP服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2FUDP%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2FUDP%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[UDP服务启动类12345678910111213141516171819202122232425262728293031323334353637383940414243444546public final class GatewayUDPConnector extends NettyUDPConnector &#123; private UDPChannelHandler channelHandler; private MessageDispatcher messageDispatcher; private MPushServer mPushServer; public GatewayUDPConnector(MPushServer mPushServer) &#123; super(CC.mp.net.gateway_server_port); this.mPushServer = mPushServer; this.messageDispatcher = new MessageDispatcher(POLICY_LOG); this.channelHandler = new UDPChannelHandler(messageDispatcher); &#125; @Override public void init() &#123; super.init(); messageDispatcher.register(Command.GATEWAY_PUSH, () -&gt; new GatewayPushHandler(mPushServer.getPushCenter())); messageDispatcher.register(Command.GATEWAY_KICK, () -&gt; new GatewayKickUserHandler(mPushServer.getRouterCenter())); channelHandler.setMulticastAddress(Utils.getInetAddress(CC.mp.net.gateway_server_multicast)); channelHandler.setNetworkInterface(Utils.getLocalNetworkInterface()); &#125; @Override protected void initOptions(Bootstrap b) &#123; super.initOptions(b); b.option(ChannelOption.IP_MULTICAST_LOOP_DISABLED, true);//默认情况下，当本机发送组播数据到某个网络接口时，在IP层，数据会回送到本地的回环接口，选项IP_MULTICAST_LOOP用于控制数据是否回送到本地的回环接口 b.option(ChannelOption.IP_MULTICAST_TTL, 255);//选项IP_MULTICAST_TTL允许设置超时TTL，范围为0～255之间的任何值 //b.option(ChannelOption.IP_MULTICAST_IF, null);//选项IP_MULTICAST_IF用于设置组播的默认网络接口，会从给定的网络接口发送，另一个网络接口会忽略此数据,参数addr是希望多播输出接口的IP地址，使用INADDR_ANY地址回送到默认接口。 //b.option(ChannelOption.WRITE_BUFFER_WATER_MARK, new WriteBufferWaterMark(32 * 1024, 1024 * 1024)); if (snd_buf.gateway_server &gt; 0) b.option(ChannelOption.SO_SNDBUF, snd_buf.gateway_server); if (rcv_buf.gateway_server &gt; 0) b.option(ChannelOption.SO_RCVBUF, rcv_buf.gateway_server); &#125; @Override public ChannelHandler getChannelHandler() &#123; return channelHandler; &#125; public Connection getConnection() &#123; return channelHandler.getConnection(); &#125; public MessageDispatcher getMessageDispatcher() &#123; return messageDispatcher; &#125;&#125; 广播消息处理：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@ChannelHandler.Sharablepublic final class UDPChannelHandler extends ChannelInboundHandlerAdapter &#123; private static final Logger LOGGER = LoggerFactory.getLogger(UDPChannelHandler.class); private final NettyConnection connection = new NettyConnection(); private final PacketReceiver receiver; private InetAddress multicastAddress; private NetworkInterface networkInterface; public UDPChannelHandler(PacketReceiver receiver) &#123; this.receiver = receiver; &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; connection.init(ctx.channel(), false); if (multicastAddress != null) &#123; ((DatagramChannel) ctx.channel()).joinGroup(multicastAddress, networkInterface, null).addListener(future -&gt; &#123; if (future.isSuccess()) &#123; LOGGER.info("join multicast group success, channel=&#123;&#125;, group=&#123;&#125;", ctx.channel(), multicastAddress); &#125; else &#123; LOGGER.error("join multicast group error, channel=&#123;&#125;, group=&#123;&#125;", ctx.channel(), multicastAddress, future.cause()); &#125; &#125;); &#125; LOGGER.info("init udp channel=&#123;&#125;", ctx.channel()); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; connection.close(); if (multicastAddress != null) &#123; ((DatagramChannel) ctx.channel()).leaveGroup(multicastAddress, networkInterface, null).addListener(future -&gt; &#123; if (future.isSuccess()) &#123; LOGGER.info("leave multicast group success, channel=&#123;&#125;, group=&#123;&#125;", ctx.channel(), multicastAddress); &#125; else &#123; LOGGER.error("leave multicast group error, channel=&#123;&#125;, group=&#123;&#125;", ctx.channel(), multicastAddress, future.cause()); &#125; &#125;); &#125; LOGGER.info("disconnect udp channel=&#123;&#125;, connection=&#123;&#125;", ctx.channel(), connection); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; DatagramPacket datagramPacket = (DatagramPacket) msg; Packet packet = PacketDecoder.decodeFrame(datagramPacket); receiver.onReceive(packet, connection); datagramPacket.release();//最后一个使用方要释放引用 &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; connection.close(); LOGGER.error("udp handler caught an exception, channel=&#123;&#125;, conn=&#123;&#125;", ctx.channel(), connection, cause); &#125; public UDPChannelHandler setMulticastAddress(InetAddress multicastAddress) &#123; if (!multicastAddress.isMulticastAddress()) &#123; throw new IllegalArgumentException(multicastAddress + "not a multicastAddress"); &#125; this.multicastAddress = multicastAddress; return this; &#125; public UDPChannelHandler setNetworkInterface(NetworkInterface networkInterface) &#123; this.networkInterface = networkInterface; return this; &#125; public Connection getConnection() &#123; return connection; &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>UDP网关服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[websocket客户端]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FSDK%2Fmpush-client-js%2Fwebsocket%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[web端，走websocket方式通信；源码：https://github.com/mpusher/mpush-client-js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;MPush WebSocket Client&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script type="text/javascript"&gt; (function (window) &#123; let socket, session = &#123;&#125;, ID_SEQ = 1; let config = &#123;listener: null, log: console&#125;; let listener = &#123; onOpened: function (event) &#123; if (config.listener != null) &#123; config.listener.onOpened(event); &#125; handshake(); &#125;, onClosed: function (event) &#123; if (config.listener != null) &#123; config.listener.onClosed(event); &#125; session = &#123;&#125;; ID_SEQ = 1; socket = null; &#125;, onHandshake: function () &#123; session.handshakeOk = true; if (config.listener != null) &#123; config.listener.onHandshake(); &#125; if (config.userId) &#123; bindUser(config.userId, config.tags); &#125; &#125;, onBindUser: function (success) &#123; if (config.listener != null) &#123; config.listener.onBindUser(success); &#125; &#125;, onReceivePush: function (message, messageId) &#123; if (config.listener != null) &#123; config.listener.onReceivePush(message, messageId); &#125; &#125;, onKickUser: function (userId, deviceId) &#123; if (config.listener != null) &#123; config.listener.onKickUser(userId, deviceId); &#125; doClose(-1, "kick user"); &#125; &#125;; const Command = &#123; HANDSHAKE: 2, BIND: 5, UNBIND: 6, ERROR: 10, OK: 11, KICK: 13, PUSH: 15, ACK: 23, UNKNOWN: -1 &#125;; function Packet(cmd, body, sessionId) &#123; return &#123; cmd: cmd, flags: 16, sessionId: sessionId || ID_SEQ++, body: body &#125; &#125; function handshake() &#123; send(Packet(Command.HANDSHAKE, &#123; deviceId: config.deviceId, osName: config.osName, osVersion: config.osVersion, clientVersion: config.clientVersion &#125;) ); &#125; function bindUser(userId, tags) &#123; if (userId &amp;&amp; userId != session.userId) &#123; session.userId = userId; session.tags = tags; send(Packet(Command.BIND, &#123;userId: userId, tags: tags&#125;)); &#125; &#125; function ack(sessionId) &#123; send(Packet(Command.ACK, null, sessionId)); &#125; function send(message) &#123; if (!socket) &#123; return; &#125; if (socket.readyState == WebSocket.OPEN) &#123; socket.send(JSON.stringify(message)); &#125; else &#123; config.log.error("The socket is not open."); &#125; &#125; function dispatch(packet) &#123; switch (packet.cmd) &#123; case Command.HANDSHAKE: &#123; config.log.debug("&gt;&gt;&gt; handshake ok."); listener.onHandshake(); break; &#125; case Command.OK: &#123; if (packet.body.cmd == Command.BIND) &#123; config.log.debug("&gt;&gt;&gt; bind user ok."); listener.onBindUser(true); &#125; break; &#125; case Command.ERROR: &#123; if (packet.body.cmd == Command.BIND) &#123; config.log.debug("&gt;&gt;&gt; bind user failure."); listener.onBindUser(false); &#125; break; &#125; case Command.KICK: &#123; if (session.userId == packet.body.userId &amp;&amp; config.deviceId == packet.body.deviceId) &#123; config.log.debug("&gt;&gt;&gt; receive kick user."); listener.onKickUser(packet.body.userId, packet.body.deviceId); &#125; break; &#125; case Command.PUSH: &#123; config.log.debug("&gt;&gt;&gt; receive push, content=" + packet.body.content); let sessionId; if ((packet.flags &amp; 8) != 0) &#123; ack(packet.sessionId); &#125; else &#123; sessionId = packet.sessionId &#125; listener.onReceivePush(packet.body.content, sessionId); break; &#125; &#125; &#125; function onReceive(event) &#123; config.log.debug("&gt;&gt;&gt; receive packet=" + event.data); dispatch(JSON.parse(event.data)) &#125; function onOpen(event) &#123; config.log.info("Web Socket opened!"); listener.onOpened(event); &#125; function onClose(event) &#123; config.log.info("Web Socket closed!"); listener.onClosed(event); &#125; function onError(event) &#123; config.log.info("Web Socket receive, error"); doClose(); &#125; function doClose(code, reason) &#123; if (socket) socket.close(); config.log.info("try close web socket client, reason=" + reason); &#125; function doConnect(cfg) &#123; config = copy(cfg); socket = new WebSocket(config.url); socket.onmessage = onReceive; socket.onopen = onOpen; socket.onclose = onClose; socket.onerror = onError; config.log.debug("try connect to web socket server, url=" + config.url); &#125; function copy(cfg) &#123; for (let p in cfg) &#123; if (cfg.hasOwnProperty(p)) &#123; config[p] = cfg[p]; &#125; &#125; return config; &#125; window.mpush = &#123; connect: doConnect, close: doClose, bindUser: bindUser &#125; &#125;)(window); function $(id) &#123; return document.getElementById(id); &#125; let log = &#123; log: function () &#123; $("responseText").value += (Array.prototype.join.call(arguments, "") + "\r\n"); &#125; &#125;; log.debug = log.info = log.warn = log.error = log.log; function connect() &#123; mpush.connect(&#123; url: $("url").value, userId: $("userId").value, deviceId: "test-1001", osName: "web " + navigator.userAgent, osVersion: "55.2", clientVersion: "1.0", log: log &#125;); &#125; function bind() &#123; mpush.bindUser($("userId").value) &#125;&lt;/script&gt;&lt;form onsubmit="return false;"&gt; &lt;label&gt; Server Url: &lt;input type="text" id="url" value="ws://127.0.0.1:8080/"&gt; &lt;/label&gt; &lt;input type="button" value="Connect" onclick="connect()"&gt; &lt;br&gt; &lt;label&gt; Bind User: &lt;input type="text" id="userId" value="user-0"&gt; &lt;/label&gt; &lt;input type="button" value="bind" onclick="bind()"&gt; &lt;h3&gt;&lt;label for="responseText"&gt;Output&lt;/label&gt;&lt;/h3&gt; &lt;textarea id="responseText" style="width:100%;height:500px;"&gt;&lt;/textarea&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>SDK</category>
        <category>mpush-client-js</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2-握手 HANDSHAKE]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F2-%E6%8F%A1%E6%89%8B%20HANDSHAKE%2F</url>
    <content type="text"><![CDATA[12//ConnectionServer#init()messageDispatcher.register(Command.HANDSHAKE, () -&gt; new HandshakeHandler(mPushServer)); 接收到客户端的Handshake请求，根据SessionContext中是否设置是否启用加密，分2种处理请求：123456789//HandshakeHandler.java@Overridepublic void handle(HandshakeMessage message) &#123; if (message.getConnection().getSessionContext().isSecurity()) &#123; doSecurity(message); &#125; else &#123; doInsecurity(message); &#125;&#125; 启用加密的处理：加解密的交互细节，站内搜索文章：深度进阶-加解密握手成功，生成session并保存到redis中，用于快速重连；123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private void doSecurity(HandshakeMessage message) &#123; byte[] iv = message.iv;//AES密钥向量16位 byte[] clientKey = message.clientKey;//客户端随机数16位 byte[] serverKey = CipherBox.I.randomAESKey();//服务端随机数16位 byte[] sessionKey = CipherBox.I.mixKey(clientKey, serverKey);//会话密钥16位 //1.校验客户端消息字段 if (Strings.isNullOrEmpty(message.deviceId) || iv.length != CipherBox.I.getAesKeyLength() || clientKey.length != CipherBox.I.getAesKeyLength()) &#123; ErrorMessage.from(message).setReason("Param invalid").close(); Logs.CONN.error("handshake failure, message=&#123;&#125;, conn=&#123;&#125;", message, message.getConnection()); return; &#125; //2.重复握手判断 SessionContext context = message.getConnection().getSessionContext(); if (message.deviceId.equals(context.deviceId)) &#123; ErrorMessage.from(message).setErrorCode(REPEAT_HANDSHAKE).send(); Logs.CONN.warn("handshake failure, repeat handshake, conn=&#123;&#125;", message.getConnection()); return; &#125; //3.更换会话密钥RSA=&gt;AES(clientKey) context.changeCipher(new AesCipher(clientKey, iv)); //4.生成可复用session, 用于快速重连 ReusableSession session = reusableSessionManager.genSession(context); //5.计算心跳时间 int heartbeat = ConfigTools.getHeartbeat(message.minHeartbeat, message.maxHeartbeat); //6.响应握手成功消息 HandshakeOkMessage .from(message) .setServerKey(serverKey) .setHeartbeat(heartbeat) .setSessionId(session.sessionId) .setExpireTime(session.expireTime) .send(f -&gt; &#123; if (f.isSuccess()) &#123; //7.更换会话密钥AES(clientKey)=&gt;AES(sessionKey) context.changeCipher(new AesCipher(sessionKey, iv)); //8.保存client信息到当前连接 context.setOsName(message.osName) .setOsVersion(message.osVersion) .setClientVersion(message.clientVersion) .setDeviceId(message.deviceId) .setHeartbeat(heartbeat); //9.保存可复用session到Redis, 用于快速重连 reusableSessionManager.cacheSession(session); Logs.CONN.info("handshake success, conn=&#123;&#125;", message.getConnection()); &#125; else &#123; Logs.CONN.info("handshake failure, conn=&#123;&#125;", message.getConnection(), f.cause()); &#125; &#125; );&#125; 没有启用加密的处理：123456789101112131415161718192021222324252627282930private void doInsecurity(HandshakeMessage message) &#123; //1.校验客户端消息字段 if (Strings.isNullOrEmpty(message.deviceId)) &#123; ErrorMessage.from(message).setReason("Param invalid").close(); Logs.CONN.error("handshake failure, message=&#123;&#125;, conn=&#123;&#125;", message, message.getConnection()); return; &#125; //2.重复握手判断 SessionContext context = message.getConnection().getSessionContext(); if (message.deviceId.equals(context.deviceId)) &#123; ErrorMessage.from(message).setErrorCode(REPEAT_HANDSHAKE).send(); Logs.CONN.warn("handshake failure, repeat handshake, conn=&#123;&#125;", message.getConnection()); return; &#125; //6.响应握手成功消息 HandshakeOkMessage.from(message).send(); //8.保存client信息到当前连接 context.setOsName(message.osName) .setOsVersion(message.osVersion) .setClientVersion(message.clientVersion) .setDeviceId(message.deviceId) .setHeartbeat(Integer.MAX_VALUE); Logs.CONN.info("handshake success, conn=&#123;&#125;", message.getConnection()); &#125;&#125; session管理1234567891011121314151617181920212223242526public final class ReusableSessionManager &#123; private final int expiredTime = CC.mp.core.session_expired_time; private final CacheManager cacheManager = CacheManagerFactory.create(); public boolean cacheSession(ReusableSession session) &#123; String key = CacheKeys.getSessionKey(session.sessionId); cacheManager.set(key, ReusableSession.encode(session.context), expiredTime); return true; &#125; public ReusableSession querySession(String sessionId) &#123; String key = CacheKeys.getSessionKey(sessionId); String value = cacheManager.get(key, String.class); if (Strings.isBlank(value)) return null; return ReusableSession.decode(value); &#125; public ReusableSession genSession(SessionContext context) &#123; long now = System.currentTimeMillis(); ReusableSession session = new ReusableSession(); session.context = context; session.sessionId = MD5Utils.encrypt(context.deviceId + now); session.expireTime = now + expiredTime * 1000; return session; &#125;&#125; 是否加密的设置代码：12345public final class ConnectionServer extends NettyTCPServer &#123; public ConnectionServer(MPushServer mPushServer) &#123; this.channelHandler = new ServerChannelHandler(true, connectionManager, messageDispatcher); &#125;&#125; 1234567891011121314@ChannelHandler.Sharablepublic final class ServerChannelHandler extends ChannelInboundHandlerAdapter &#123; private final boolean security; //是否启用加密 public ServerChannelHandler(boolean security, ConnectionManager connectionManager, PacketReceiver receiver) &#123; this.security = security; &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Logs.CONN.info("client connected conn=&#123;&#125;", ctx.channel()); Connection connection = new NettyConnection(); connection.init(ctx.channel(), security); connectionManager.add(connection); &#125;&#125; 12345678910111213141516171819public final class NettyConnection implements Connection, ChannelFutureListener &#123; private static final Logger LOGGER = LoggerFactory.getLogger(NettyConnection.class); private SessionContext context; private Channel channel; private volatile byte status = STATUS_NEW; private long lastReadTime; private long lastWriteTime; @Override public void init(Channel channel, boolean security) &#123; this.channel = channel; this.context = new SessionContext(); this.lastReadTime = System.currentTimeMillis(); this.status = STATUS_CONNECTED; if (security) &#123; this.context.changeCipher(RsaCipherFactory.create()); &#125; &#125;&#125; 接入服务文章目录: 1-心跳 HEARTBEAT 2-握手 HANDSHAKE 3-用户绑定-解绑 BIND-UNBIND 4-快速连接 FAST_CONNECT 5-客户端推送 PUSH 6-消息确认 ACK 7-HTTP代理 HTTP_PROXY]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1-心跳 HEARTBEAT]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F1-%E5%BF%83%E8%B7%B3%20HEARTBEAT%2F</url>
    <content type="text"><![CDATA[接收消息，解码协议、解码body内容，站内搜索文章: 编码解码 客户端发送心跳：场景一： 当与服务端建立连接时(ConnClientChannelHandler#channelActive)，发送握手或者快速连接请求；在接收到握手或者快速连接响应时(ConnClientChannelHandler#channelRead)，发送心跳请求； 场景二： 当网络断开时不主动关闭连接，而是尝试发送一次心跳检测，如果能收到响应，说明网络短时间内又恢复了，否则就断开连接，等待网络恢复并重建连接。见mpush-client-java工程 MPushClient#onNetStateChange()方法； 服务端接收处理心跳：12//ConnectionServer#init()messageDispatcher.register(Command.HEARTBEAT, HeartBeatHandler::new); 1234567public final class HeartBeatHandler implements MessageHandler &#123; @Override public void handle(Packet packet, Connection connection) &#123; connection.send(packet);//ping -&gt; pong Logs.HB.info("ping -&gt; pong, &#123;&#125;", connection); &#125;&#125; 接入服务文章目录: 1-心跳 HEARTBEAT 2-握手 HANDSHAKE 3-用户绑定-解绑 BIND-UNBIND 4-快速连接 FAST_CONNECT 5-客户端推送 PUSH 6-消息确认 ACK 7-HTTP代理 HTTP_PROXY]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-用户绑定-解绑 BIND-UNBIND]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F3-%E7%94%A8%E6%88%B7%E7%BB%91%E5%AE%9A-%E8%A7%A3%E7%BB%91%20BIND-UNBIND%2F</url>
    <content type="text"><![CDATA[接入服务文章目录: 1-心跳 HEARTBEAT 2-握手 HANDSHAKE 3-用户绑定-解绑 BIND-UNBIND 4-快速连接 FAST_CONNECT 5-客户端推送 PUSH 6-消息确认 ACK 7-HTTP代理 HTTP_PROXY]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4-快速连接 FAST_CONNECT]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F4-%E5%BF%AB%E9%80%9F%E8%BF%9E%E6%8E%A5%20FAST_CONNECT%2F</url>
    <content type="text"><![CDATA[12//ConnectionServer#init()messageDispatcher.register(Command.FAST_CONNECT, () -&gt; new FastConnectHandler(mPushServer)); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public final class FastConnectHandler extends BaseMessageHandler&lt;FastConnectMessage&gt; &#123; private final ReusableSessionManager reusableSessionManager; public FastConnectHandler(MPushServer mPushServer) &#123; this.reusableSessionManager = mPushServer.getReusableSessionManager(); &#125; @Override public FastConnectMessage decode(Packet packet, Connection connection) &#123; return new FastConnectMessage(packet, connection); &#125; @Override public void handle(FastConnectMessage message) &#123; //从缓存中心查询session Profiler.enter("time cost on [query session]"); ReusableSession session = reusableSessionManager.querySession(message.sessionId); Profiler.release(); if (session == null) &#123; //1.没查到说明session已经失效了 ErrorMessage.from(message).setErrorCode(SESSION_EXPIRED).send(); Logs.CONN.warn("fast connect failure, session is expired, sessionId=&#123;&#125;, deviceId=&#123;&#125;, conn=&#123;&#125;" , message.sessionId, message.deviceId, message.getConnection().getChannel()); &#125; else if (!session.context.deviceId.equals(message.deviceId)) &#123; //2.非法的设备, 当前设备不是上次生成session时的设备 ErrorMessage.from(message).setErrorCode(INVALID_DEVICE).send(); Logs.CONN.warn("fast connect failure, not the same device, deviceId=&#123;&#125;, session=&#123;&#125;, conn=&#123;&#125;" , message.deviceId, session.context, message.getConnection().getChannel()); &#125; else &#123; //3.校验成功，重新计算心跳，完成快速重连 int heartbeat = ConfigTools.getHeartbeat(message.minHeartbeat, message.maxHeartbeat); session.context.setHeartbeat(heartbeat); Profiler.enter("time cost on [send FastConnectOkMessage]"); FastConnectOkMessage .from(message) .setHeartbeat(heartbeat) .sendRaw(f -&gt; &#123; if (f.isSuccess()) &#123; //4. 恢复缓存的会话信息(包含会话密钥等) message.getConnection().setSessionContext(session.context); Logs.CONN.info("fast connect success, session=&#123;&#125;, conn=&#123;&#125;", session.context, message.getConnection().getChannel()); &#125; else &#123; Logs.CONN.info("fast connect failure, session=&#123;&#125;, conn=&#123;&#125;", session.context, message.getConnection().getChannel()); &#125; &#125;); Profiler.release(); &#125; &#125;&#125; 接入服务文章目录: 1-心跳 HEARTBEAT 2-握手 HANDSHAKE 3-用户绑定-解绑 BIND-UNBIND 4-快速连接 FAST_CONNECT 5-客户端推送 PUSH 6-消息确认 ACK 7-HTTP代理 HTTP_PROXY]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6-消息确认 ACK]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F6-%E6%B6%88%E6%81%AF%E7%A1%AE%E8%AE%A4%20ACK%2F</url>
    <content type="text"><![CDATA[接收到客户端的ACK响应12//ConnectionServer#init()messageDispatcher.register(Command.ACK, () -&gt; new AckHandler(mPushServer)); 12345678910111213141516171819public final class AckHandler extends BaseMessageHandler&lt;AckMessage&gt; &#123; private final AckTaskQueue ackTaskQueue; public AckHandler(MPushServer mPushServer) &#123; this.ackTaskQueue = mPushServer.getPushCenter().getAckTaskQueue(); &#125; @Override public AckMessage decode(Packet packet, Connection connection) &#123; return new AckMessage(packet, connection); &#125; @Override public void handle(AckMessage message) &#123; AckTask task = ackTaskQueue.getAndRemove(message.getSessionId()); if (task == null) &#123;//ack 超时了 Logs.PUSH.info("receive client ack, but task timeout message=&#123;&#125;", message); return; &#125; task.onResponse();//成功收到客户的ACK响应 &#125;&#125; 接入服务文章目录: 1-心跳 HEARTBEAT 2-握手 HANDSHAKE 3-用户绑定-解绑 BIND-UNBIND 4-快速连接 FAST_CONNECT 5-客户端推送 PUSH 6-消息确认 ACK 7-HTTP代理 HTTP_PROXY]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7-HTTP代理 HTTP_PROXY]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F7-HTTP%E4%BB%A3%E7%90%86%20HTTP_PROXY%2F</url>
    <content type="text"><![CDATA[接入服务文章目录: 1-心跳 HEARTBEAT 2-握手 HANDSHAKE 3-用户绑定-解绑 BIND-UNBIND 4-快速连接 FAST_CONNECT 5-客户端推送 PUSH 6-消息确认 ACK 7-HTTP代理 HTTP_PROXY]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5-客户端推送 PUSH]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F5-%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8E%A8%E9%80%81%20PUSH%2F</url>
    <content type="text"><![CDATA[接收客户端的PUSH请求，请求处理器走的SPI机制，业务实现由SPI扩展实现；12//ConnectionServer#init()messageDispatcher.register(Command.PUSH, PushHandlerFactory::create); 12345public interface PushHandlerFactory extends Factory&lt;MessageHandler&gt; &#123; static MessageHandler create() &#123; return SpiLoader.load(PushHandlerFactory.class).get(); &#125;&#125; 123456789101112131415161718192021@Spi(order = 1)public final class ClientPushHandler extends BaseMessageHandler&lt;PushMessage&gt; implements PushHandlerFactory &#123; @Override public PushMessage decode(Packet packet, Connection connection) &#123; return new PushMessage(packet, connection); &#125; @Override public void handle(PushMessage message) &#123; Logs.PUSH.info("receive client push message=&#123;&#125;", message); if (message.autoAck()) &#123; AckMessage.from(message).sendRaw(); Logs.PUSH.info("send ack for push message=&#123;&#125;", message); &#125; //biz code write here &#125; @Override public MessageHandler get() &#123; return this; &#125;&#125; 接入服务文章目录: 1-心跳 HEARTBEAT 2-握手 HANDSHAKE 3-用户绑定-解绑 BIND-UNBIND 4-快速连接 FAST_CONNECT 5-客户端推送 PUSH 6-消息确认 ACK 7-HTTP代理 HTTP_PROXY]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>接入服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1-入口]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F1-%E5%85%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2-MPushServer初始化]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F2-MPushServer%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-10-HTTP和DNS服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-10-HTTP%E5%92%8CDNS%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-3-接入服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-3-%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-2-服务注册与发现模块]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-2-%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-4-websocket接入服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-4-websocket%E6%8E%A5%E5%85%A5%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-1-缓存模块]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-1-%E7%BC%93%E5%AD%98%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-5-UDP网关服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-5-UDP%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-7-控制台服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-7-%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-6-TCP网关服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-6-TCP%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-8-路由服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-8-%E8%B7%AF%E7%94%B1%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-9-推送中心]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-9-%E6%8E%A8%E9%80%81%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[admin server测试]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%9C%8D%E5%8A%A1%2Fadmin%20server%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[启动mpush服务运行ServerTestMain#testServer()，控制台日志会打印如下：111:47:00.097 - [mp-admin-boss-6-1] INFO - com.mpush.core.server.AdminServer - server start success on:3002 telnet连接到mpush admin控制台123456789101112131415161718192021222324252627282930313233343536373839404142➜ ~ telnet 172.16.177.134 3002Trying 172.16.177.134...Connected to 172.16.177.134.Escape character is '^]'.Welcome to MPush Console [172.16.177.134]!since 2018-12-26T11:47:00.087 has running 0(h)It is Wed Dec 26 11:47:29 CST 2018 now.helpOption Description------ -----------help show helpquit exit console modeshutdown stop mpush serverrestart restart mpush serverzk:&lt;redis, cs ,gs&gt; query zk nodecount:&lt;conn, online&gt; count conn num or online user countroute:&lt;uid&gt; show user route infopush:&lt;uid&gt;, &lt;msg&gt; push test msg to clientconf:[key] show config infomonitor:[mxBean] show system monitorprofile:&lt;1,0&gt; enable/disable profilecount conn0pushunsupported optionroute user01user [user01] offline now.profile 1Profiler enabledquithave a good day!Connection closed by foreign host.➜ ~ mpush 后台输出日志：12345612:06:33.915 - [mp-admin-work-7-1] INFO - com.mpush.core.handler.AdminHandler - receive admin command=help12:06:45.343 - [mp-admin-work-7-1] INFO - com.mpush.core.handler.AdminHandler - receive admin command=count conn12:07:23.878 - [mp-admin-work-7-1] INFO - com.mpush.core.handler.AdminHandler - receive admin command=push12:08:14.383 - [mp-admin-work-7-1] INFO - com.mpush.core.handler.AdminHandler - receive admin command=route user0112:08:35.749 - [mp-admin-work-7-1] INFO - com.mpush.core.handler.AdminHandler - receive admin command=profile 112:10:26.446 - [mp-admin-work-7-1] INFO - com.mpush.core.handler.AdminHandler - receive admin command=quit]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>控制台服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdminHandler处理命令]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%9C%8D%E5%8A%A1%2FAdminHandler%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1、注册命令处理器2、建立连接，打印欢迎画面3、接收命令，执行命令处理器 注册命令处理器初始化AdminHandler时，调用init()注册所有的命令处理器到内存中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677private final Map&lt;String, OptionHandler&gt; optionHandlers = new HashMap&lt;&gt;();public AdminHandler(MPushServer mPushServer) &#123; this.mPushServer = mPushServer; init();&#125;public void init() &#123; register("help", (ctx, args) -&gt; "Option Description" + EOL + "------ -----------" + EOL + "help show help" + EOL + "quit exit console mode" + EOL + "shutdown stop mpush server" + EOL + "restart restart mpush server" + EOL + "zk:&lt;redis, cs ,gs&gt; query zk node" + EOL + "count:&lt;conn, online&gt; count conn num or online user count" + EOL + "route:&lt;uid&gt; show user route info" + EOL + "push:&lt;uid&gt;, &lt;msg&gt; push test msg to client" + EOL + "conf:[key] show config info" + EOL + "monitor:[mxBean] show system monitor" + EOL + "profile:&lt;1,0&gt; enable/disable profile" + EOL ); register("quit", (ctx, args) -&gt; "have a good day!"); register("shutdown", (ctx, args) -&gt; &#123; new Thread(() -&gt; System.exit(0)).start(); return "try close connect server..."; &#125;); register("count", (ctx, args) -&gt; &#123; switch (args) &#123; case "conn": return mPushServer.getConnectionServer().getConnectionManager().getConnNum(); case "online": &#123; return mPushServer.getRouterCenter().getUserEventConsumer().getUserManager().getOnlineUserNum(); &#125; &#125; return "[" + args + "] unsupported, try help."; &#125;); register("route", (ctx, args) -&gt; &#123; if (Strings.isNullOrEmpty(args)) return "please input userId"; Set&lt;RemoteRouter&gt; routers = mPushServer.getRouterCenter().getRemoteRouterManager().lookupAll(args); if (routers.isEmpty()) return "user [" + args + "] offline now."; return Jsons.toJson(routers); &#125;); register("conf", (ctx, args) -&gt; &#123; if (Strings.isNullOrEmpty(args)) &#123; return CC.cfg.root().render(ConfigRenderOptions.concise().setFormatted(true)); &#125; if (CC.cfg.hasPath(args)) &#123; return CC.cfg.getAnyRef(args).toString(); &#125; return "key [" + args + "] not find in config"; &#125;); register("profile", (ctx, args) -&gt; &#123; if (args == null || "0".equals(args)) &#123; Profiler.enable(false); return "Profiler disabled"; &#125; else &#123; Profiler.enable(true); return "Profiler enabled"; &#125; &#125;);&#125;private void register(String option, OptionHandler handler) &#123; optionHandlers.put(option, handler);&#125;public interface OptionHandler &#123; Object handle(ChannelHandlerContext ctx, String args) throws Exception;&#125; 建立连接，打印欢迎画面1234567@Overridepublic void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.write("Welcome to MPush Console [" + ConfigTools.getLocalIp() + "]!" + EOL); ctx.write("since " + startTime + " has running " + startTime.until(LocalDateTime.now(), ChronoUnit.HOURS) + "(h)" + EOL + EOL); ctx.write("It is " + new Date() + " now." + EOL + EOL); ctx.flush();&#125; 12345678➜ ~ telnet 172.16.177.134 3002Trying 172.16.177.134...Connected to 172.16.177.134.Escape character is '^]'.Welcome to MPush Console [172.16.177.134]!since 2018-12-26T11:47:00.087 has running 0(h)It is Wed Dec 26 11:47:29 CST 2018 now. 接收命令，执行命令处理器1234567891011121314151617181920212223242526272829303132333435363738//当读取到消息时，Netty触发channelRead()@Overrideprotected void channelRead0(ChannelHandlerContext ctx, String request) throws Exception &#123; String option = "help"; String arg = null; String[] args = null; if (!Strings.isNullOrEmpty(request)) &#123; String[] cmd_args = request.split(" "); option = cmd_args[0].trim().toLowerCase(); if (cmd_args.length == 2) &#123; arg = cmd_args[1]; &#125; else if (cmd_args.length &gt; 2) &#123; args = Arrays.copyOfRange(cmd_args, 1, cmd_args.length); &#125; &#125; try &#123; //接收命令，并处理 Object result = optionHandlers.getOrDefault(option, unsupported_handler).handle(ctx, arg); //将处理结果输出到控制台 ChannelFuture future = ctx.writeAndFlush(result + EOL + EOL); //如果当前操作命令是quit，则监听CLOSE事件，做关闭连接处理 if (option.equals("quit")) &#123; future.addListener(ChannelFutureListener.CLOSE); &#125; &#125; catch (Throwable throwable) &#123; ctx.writeAndFlush(throwable.getLocalizedMessage() + EOL + EOL); StringWriter writer = new StringWriter(1024); throwable.printStackTrace(new PrintWriter(writer)); ctx.writeAndFlush(writer.toString()); &#125; LOGGER.info("receive admin command=&#123;&#125;", request);&#125;//在通道读取完成后会在这个方法里通知，对应可以做刷新操作 ctx.flush()@Overridepublic void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush();&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>控制台服务</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RouterChangeListener路由变更监听]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E8%B7%AF%E7%94%B1%E4%B8%AD%E5%BF%83%2FRouterChangeListener%E8%B7%AF%E7%94%B1%E5%8F%98%E6%9B%B4%E7%9B%91%E5%90%AC%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>路由中心</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LocalRouterManager 本地路由管理器]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E8%B7%AF%E7%94%B1%E4%B8%AD%E5%BF%83%2FLocalRouterManager%20%E6%9C%AC%E5%9C%B0%E8%B7%AF%E7%94%B1%E7%AE%A1%E7%90%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1、本地路由信息的添加、删除、获取；2、订阅连接关闭事件ConnectionCloseEvent，删除本地路由，发布离线事件UserOfflineEvent； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package com.mpush.core.router;import com.google.common.eventbus.AllowConcurrentEvents;import com.google.common.eventbus.Subscribe;import com.mpush.api.connection.Connection;import com.mpush.api.connection.SessionContext;import com.mpush.api.event.ConnectionCloseEvent;import com.mpush.api.event.UserOfflineEvent;import com.mpush.api.router.RouterManager;import com.mpush.tools.event.EventBus;import com.mpush.tools.event.EventConsumer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.HashSet;import java.util.Map;import java.util.Set;import java.util.concurrent.ConcurrentHashMap;/** * Created by ohun on 2015/12/23. * * @author ohun@live.cn */public final class LocalRouterManager extends EventConsumer implements RouterManager&lt;LocalRouter&gt; &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LocalRouterManager.class); private static final Map&lt;Integer, LocalRouter&gt; EMPTY = new HashMap&lt;&gt;(0); /** * 本地路由表 */ private final Map&lt;String, Map&lt;Integer, LocalRouter&gt;&gt; routers = new ConcurrentHashMap&lt;&gt;(); //添加本地路由 @Override public LocalRouter register(String userId, LocalRouter router) &#123; LOGGER.info("register local router success userId=&#123;&#125;, router=&#123;&#125;", userId, router); return routers.computeIfAbsent(userId, s -&gt; new HashMap&lt;&gt;(1)).put(router.getClientType(), router); &#125; //删除本地路由 @Override public boolean unRegister(String userId, int clientType) &#123; LocalRouter router = routers.getOrDefault(userId, EMPTY).remove(clientType); LOGGER.info("unRegister local router success userId=&#123;&#125;, router=&#123;&#125;", userId, router); return true; &#125; //查找所有本地路由 @Override public Set&lt;LocalRouter&gt; lookupAll(String userId) &#123; return new HashSet&lt;&gt;(routers.getOrDefault(userId, EMPTY).values()); &#125; //查找userId和clientType对应的本地路由 @Override public LocalRouter lookup(String userId, int clientType) &#123; LocalRouter router = routers.getOrDefault(userId, EMPTY).get(clientType); LOGGER.info("lookup local router userId=&#123;&#125;, router=&#123;&#125;", userId, router); return router; &#125; //返回本地路由表 public Map&lt;String, Map&lt;Integer, LocalRouter&gt;&gt; routers() &#123; return routers; &#125; /** * 监听链接关闭事件，清理失效的路由 * * @param event */ @Subscribe @AllowConcurrentEvents void on(ConnectionCloseEvent event) &#123; Connection connection = event.connection; if (connection == null) return; SessionContext context = connection.getSessionContext(); String userId = context.userId; if (userId == null) return; int clientType = context.getClientType(); //获取userId和clientType对应的本地路由 LocalRouter localRouter = routers.getOrDefault(userId, EMPTY).get(clientType); if (localRouter == null) return; String connId = connection.getId(); //2.检测下，是否是同一个链接, 如果客户端重连，老的路由会被新的链接覆盖 if (connId.equals(localRouter.getRouteValue().getId())) &#123; //3. 删除路由 routers.getOrDefault(userId, EMPTY).remove(clientType); //4. 发送用户下线事件, 只有老的路由存在的情况下才发送，因为有可能又用户重连了，而老的链接又是在新连接之后才断开的 //这个时候就会有问题，会导致用户变成下线状态，实际用户应该是在线的。 EventBus.post(new UserOfflineEvent(event.connection, userId)); LOGGER.info("clean disconnected local route, userId=&#123;&#125;, route=&#123;&#125;", userId, localRouter); &#125; else &#123; //如果不相等，则log一下 LOGGER.info("clean disconnected local route, not clean:userId=&#123;&#125;, route=&#123;&#125;", userId, localRouter); &#125; &#125;&#125; LocalRouter.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.mpush.core.router;import com.mpush.api.connection.Connection;import com.mpush.api.router.Router;/** * Created by ohun on 2015/12/23. * * @author ohun@live.cn */public final class LocalRouter implements Router&lt;Connection&gt; &#123; private final Connection connection; public LocalRouter(Connection connection) &#123; this.connection = connection; &#125; public int getClientType() &#123; return connection.getSessionContext().getClientType(); &#125; @Override public Connection getRouteValue() &#123; return connection; &#125; @Override public RouterType getRouteType() &#123; return RouterType.LOCAL; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; LocalRouter that = (LocalRouter) o; return getClientType() == that.getClientType(); &#125; @Override public int hashCode() &#123; return Integer.hashCode(getClientType()); &#125; @Override public String toString() &#123; return "LocalRouter&#123;" + connection + '&#125;'; &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>路由中心</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RemoteRouterManager 远程路由管理器]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E8%B7%AF%E7%94%B1%E4%B8%AD%E5%BF%83%2FRemoteRouterManager%20%E8%BF%9C%E7%A8%8B%E8%B7%AF%E7%94%B1%E7%AE%A1%E7%90%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1、通过SPI，找到mpush-cache模块中CacheManagerFactory接口的实现类RedisCacheManagerFactory，得到RedisManager实例；2、远程路由信息的添加、删除、获取；3、订阅连接关闭事件ConnectionCloseEvent，将远程路由信息修改为离线(connId=null)； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class RemoteRouterManager extends EventConsumer implements RouterManager&lt;RemoteRouter&gt; &#123; public static final Logger LOGGER = LoggerFactory.getLogger(RemoteRouterManager.class); private final CacheManager cacheManager = CacheManagerFactory.create(); @Override public RemoteRouter register(String userId, RemoteRouter router) &#123; String key = CacheKeys.getUserRouteKey(userId); String field = Integer.toString(router.getRouteValue().getClientType()); ClientLocation old = cacheManager.hget(key, field, ClientLocation.class); cacheManager.hset(key, field, router.getRouteValue().toJson()); LOGGER.info("register remote router success userId=&#123;&#125;, newRouter=&#123;&#125;, oldRoute=&#123;&#125;", userId, router, old); return old == null ? null : new RemoteRouter(old); &#125; /** * 目前的实现方式是非原子操作(get:set)，可能会有并发问题，虽然概率很低 * 后续考虑采用lua脚本，实现原子操作 * * @param userId 用户ID * @param clientType 客户端类型 * @return 删除路由是否成功 */ @Override public boolean unRegister(String userId, int clientType) &#123; String key = CacheKeys.getUserRouteKey(userId); String field = Integer.toString(clientType); ClientLocation location = cacheManager.hget(key, field, ClientLocation.class); if (location == null || location.isOffline()) return true; cacheManager.hset(key, field, location.offline().toJson()); LOGGER.info("unRegister remote router success userId=&#123;&#125;, route=&#123;&#125;", userId, location); return true; &#125; @Override public Set&lt;RemoteRouter&gt; lookupAll(String userId) &#123; String key = CacheKeys.getUserRouteKey(userId); Map&lt;String, ClientLocation&gt; values = cacheManager.hgetAll(key, ClientLocation.class); if (values == null || values.isEmpty()) return Collections.emptySet(); return values.values().stream().map(RemoteRouter::new).collect(Collectors.toSet()); &#125; @Override public RemoteRouter lookup(String userId, int clientType) &#123; String key = CacheKeys.getUserRouteKey(userId); String field = Integer.toString(clientType); ClientLocation location = cacheManager.hget(key, field, ClientLocation.class); LOGGER.info("lookup remote router userId=&#123;&#125;, router=&#123;&#125;", userId, location); return location == null ? null : new RemoteRouter(location); &#125; /** * 监听链接关闭事件，清理失效的路由 * * @param event */ @Subscribe @AllowConcurrentEvents void on(ConnectionCloseEvent event) &#123; Connection connection = event.connection; if (connection == null) return; SessionContext context = connection.getSessionContext(); String userId = context.userId; if (userId == null) return; String key = CacheKeys.getUserRouteKey(userId); String field = Integer.toString(context.getClientType()); ClientLocation location = cacheManager.hget(key, field, ClientLocation.class); if (location == null || location.isOffline()) return; String connId = connection.getId(); //2.检测下，是否是同一个链接, 如果客户端重连，老的路由会被新的链接覆盖 if (connId.equals(location.getConnId())) &#123; cacheManager.hset(key, field, location.offline().toJson()); LOGGER.info("clean disconnected remote route, userId=&#123;&#125;, route=&#123;&#125;", userId, location); &#125; else &#123; LOGGER.info("clean disconnected remote route, not clean:userId=&#123;&#125;, route=&#123;&#125;", userId, location); &#125; &#125;&#125; RemoteRouter.java123456789101112131415161718192021222324public final class RemoteRouter implements Router&lt;ClientLocation&gt; &#123; private final ClientLocation clientLocation; public RemoteRouter(ClientLocation clientLocation) &#123; this.clientLocation = clientLocation; &#125; public boolean isOnline()&#123; return clientLocation.isOnline(); &#125; public boolean isOffline()&#123; return clientLocation.isOffline(); &#125; @Override public ClientLocation getRouteValue() &#123; return clientLocation; &#125; @Override public RouterType getRouteType() &#123; return RouterType.REMOTE; &#125; @Override public String toString() &#123; return "RemoteRouter&#123;" + clientLocation + '&#125;'; &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>路由中心</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[路由中心启动和初始化]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E8%B7%AF%E7%94%B1%E4%B8%AD%E5%BF%83%2F%E8%B7%AF%E7%94%B1%E4%B8%AD%E5%BF%83%E5%90%AF%E5%8A%A8%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>路由中心</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户路由信息注册]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E8%B7%AF%E7%94%B1%E4%B8%AD%E5%BF%83%2F%E7%94%A8%E6%88%B7%E8%B7%AF%E7%94%B1%E4%BF%A1%E6%81%AF%E6%B3%A8%E5%86%8C%2F</url>
    <content type="text"><![CDATA[BindUserHandler#bind()1234//2.如果握手成功，就把用户链接信息注册到路由中心，本地和远程各一份success = routerCenter.register(message.userId, message.getConnection());//3.注册失败再处理下，防止本地注册成功，远程注册失败的情况，只有都成功了才叫成功routerCenter.unRegister(message.userId, context.getClientType()); RouterCenter.java123456789101112131415161718192021222324252627282930313233343536//路由注册（本地、远程）public boolean register(String userId, Connection connection) &#123; ClientLocation location = ClientLocation .from(connection) .setHost(mPushServer.getGatewayServerNode().getHost()) .setPort(mPushServer.getGatewayServerNode().getPort()); LocalRouter localRouter = new LocalRouter(connection); RemoteRouter remoteRouter = new RemoteRouter(location); LocalRouter oldLocalRouter = null; RemoteRouter oldRemoteRouter = null; try &#123; oldLocalRouter = localRouterManager.register(userId, localRouter); oldRemoteRouter = remoteRouterManager.register(userId, remoteRouter); &#125; catch (Exception e) &#123; LOGGER.error("register router ex, userId=&#123;&#125;, connection=&#123;&#125;", userId, connection, e); &#125; //如果本地路由已经存在，则通过事件，发送踢人消息给客户端，由客户端发出解绑消息unbind if (oldLocalRouter != null) &#123; EventBus.post(new RouterChangeEvent(userId, oldLocalRouter)); LOGGER.info("register router success, find old local router=&#123;&#125;, userId=&#123;&#125;", oldLocalRouter, userId); &#125; //如果远程路由已经存在，并且是在线状态，则通过事件，发送踢人消息给客户端，由客户端发出解绑消息unbind if (oldRemoteRouter != null &amp;&amp; oldRemoteRouter.isOnline()) &#123; EventBus.post(new RouterChangeEvent(userId, oldRemoteRouter)); LOGGER.info("register router success, find old remote router=&#123;&#125;, userId=&#123;&#125;", oldRemoteRouter, userId); &#125; return true;&#125;//删除路由（本地路由、远程路由）public boolean unRegister(String userId, int clientType) &#123; localRouterManager.unRegister(userId, clientType); remoteRouterManager.unRegister(userId, clientType); return true;&#125; RouterChangeListener.java123456789101112131415@Subscribe@AllowConcurrentEventsvoid on(RouterChangeEvent event) &#123; String userId = event.userId; Router&lt;?&gt; r = event.router; if (r.getRouteType().equals(Router.RouterType.LOCAL)) &#123; //发送踢人消息到客户端 sendKickUserMessage2Client(userId, (LocalRouter) r); &#125; else &#123; //广播踢人消息到消息中心（MQ） //所有机器订阅MQ消息，如果当前机器不是目标机器，直接忽略， //否则查询本地路由，找到要被踢下线的链接，发送踢人消息到客户端 sendKickUserMessage2MQ(userId, (RemoteRouter) r); &#125;&#125;]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>路由中心</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-初始化和启动-11-监控服务]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2FMpush%20Server%2F%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%2F3-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%90%AF%E5%8A%A8-11-%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[服务启动-初始化文章目录: 1-入口 2-MPushServer初始化 3-初始化和启动-1-缓存模块 3-初始化和启动-2-服务注册与发现模块 3-初始化和启动-3-接入服务 3-初始化和启动-4-websocket接入服务 3-初始化和启动-5-UDP网关服务 3-初始化和启动-6-TCP网关服务 3-初始化和启动-7-控制台服务 3-初始化和启动-8-路由服务 3-初始化和启动-9-推送中心 3-初始化和启动-10-HTTP和DNS服务 3-初始化和启动-11-监控服务]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>Mpush Server</category>
        <category>服务启动-初始化</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[监控]]></title>
    <url>%2F%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2FMPush%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6%2F%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[监控服务目前看下来，只在MPushServer服务中启用； 监控服务 总收集器 线程池信息收集 …… 监控服务1、初始化总收集器；2、打印所有监控信息到控制台；3、根据系统负载将线程栈、堆栈输出到本地文件； MonitorService.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package com.mpush.monitor.service;import com.mpush.api.common.Monitor;import com.mpush.api.service.BaseService;import com.mpush.api.service.Listener;import com.mpush.monitor.data.MonitorResult;import com.mpush.monitor.data.ResultCollector;import com.mpush.tools.Utils;import com.mpush.tools.common.JVMUtil;import com.mpush.tools.config.CC;import com.mpush.tools.log.Logs;import com.mpush.tools.thread.ThreadNames;import java.util.concurrent.Executor;import java.util.concurrent.TimeUnit;public class MonitorService extends BaseService implements Monitor, Runnable &#123; private static final int FIRST_DUMP_JSTACK_LOAD_AVG = 2, SECOND_DUMP_JSTACK_LOAD_AVG = 4, THIRD_DUMP_JSTACK_LOAD_AVG = 6, FIRST_DUMP_JMAP_LOAD_AVG = 4; private static final String dumpLogDir = CC.mp.monitor.dump_dir; private static final boolean dumpEnabled = CC.mp.monitor.dump_stack; private static final boolean printLog = CC.mp.monitor.print_log; private static final long dumpPeriod = CC.mp.monitor.dump_period.getSeconds(); private volatile boolean dumpFirstJstack = false; private volatile boolean dumpSecondJstack = false; private volatile boolean dumpThirdJstack = false; private volatile boolean dumpJmap = false; private final ResultCollector collector; private final ThreadPoolManager threadPoolManager; public MonitorService() &#123; threadPoolManager = new ThreadPoolManager(); collector = new ResultCollector(threadPoolManager); &#125; private Thread thread; @Override public void run() &#123; while (isRunning()) &#123; MonitorResult result = collector.collect(); if (printLog) &#123; Logs.MONITOR.info(result.toJson()); &#125; if (dumpEnabled) &#123; dump(); &#125; try &#123; TimeUnit.SECONDS.sleep(dumpPeriod); &#125; catch (InterruptedException e) &#123; if (isRunning()) stop(); &#125; &#125; &#125; @Override protected void doStart(Listener listener) throws Throwable &#123; if (printLog || dumpEnabled) &#123; thread = Utils.newThread(ThreadNames.T_MONITOR, this); thread.setDaemon(true); thread.start(); &#125; listener.onSuccess(); &#125; @Override protected void doStop(Listener listener) throws Throwable &#123; if (thread != null &amp;&amp; thread.isAlive()) thread.interrupt(); listener.onSuccess(); &#125; private void dump() &#123; double load = collector.getJvmInfo().load(); if (load &gt; FIRST_DUMP_JSTACK_LOAD_AVG) &#123; if (!dumpFirstJstack) &#123; dumpFirstJstack = true; JVMUtil.dumpJstack(dumpLogDir); &#125; &#125; if (load &gt; SECOND_DUMP_JSTACK_LOAD_AVG) &#123; if (!dumpSecondJstack) &#123; dumpSecondJstack = true; JVMUtil.dumpJmap(dumpLogDir); &#125; &#125; if (load &gt; THIRD_DUMP_JSTACK_LOAD_AVG) &#123; if (!dumpThirdJstack) &#123; dumpThirdJstack = true; JVMUtil.dumpJmap(dumpLogDir); &#125; &#125; if (load &gt; FIRST_DUMP_JMAP_LOAD_AVG) &#123; if (!dumpJmap) &#123; dumpJmap = true; JVMUtil.dumpJmap(dumpLogDir); &#125; &#125; &#125; @Override public void monitor(String name, Thread thread) &#123; &#125; @Override public void monitor(String name, Executor executor) &#123; threadPoolManager.register(name, executor); &#125; public ThreadPoolManager getThreadPoolManager() &#123; return threadPoolManager; &#125;&#125; 总收集器收集类型： JVMInfo JVMGC 垃圾回收 JVMMemory 堆 JVMThread 线程 JVMThreadPool 线程池 ResultCollector.java 监控信息收集器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.mpush.monitor.data;import com.mpush.monitor.quota.impl.*;import com.mpush.monitor.service.ThreadPoolManager;/** * Created by yxx on 2016/5/19. * * @author ohun@live.cn */public class ResultCollector &#123; private final JVMInfo jvmInfo; private final JVMGC jvmgc; private final JVMMemory jvmMemory; private final JVMThread jvmThread; private final JVMThreadPool jvmThreadPool; public ResultCollector(ThreadPoolManager threadPoolManager) &#123; this.jvmInfo = new JVMInfo(); this.jvmgc = new JVMGC(); this.jvmMemory = new JVMMemory(); this.jvmThread = new JVMThread(); this.jvmThreadPool = new JVMThreadPool(threadPoolManager); &#125; public MonitorResult collect() &#123; MonitorResult result = new MonitorResult(); result.addResult("jvm-info", jvmInfo.monitor()); result.addResult("jvm-gc", jvmgc.monitor()); result.addResult("jvm-memory", jvmMemory.monitor()); result.addResult("jvm-thread", jvmThread.monitor()); result.addResult("jvm-thread-pool", jvmThreadPool.monitor()); return result; &#125; public JVMInfo getJvmInfo() &#123; return jvmInfo; &#125; public JVMGC getJvmgc() &#123; return jvmgc; &#125; public JVMMemory getJvmMemory() &#123; return jvmMemory; &#125; public JVMThread getJvmThread() &#123; return jvmThread; &#125; public JVMThreadPool getJvmThreadPool() &#123; return jvmThreadPool; &#125;&#125; 线程池信息收集获取各个线程池的信息，如corePoolSize、maxPoolSize、activeCount(workingThread)、poolSize(workThread)、queueSize(blockedTask) 组装为map.put(“event-bus”,poolInfo); JVMThreadPool.java12345678910111213141516171819202122232425262728293031323334353637package com.mpush.monitor.quota.impl;import com.mpush.monitor.quota.ThreadPoolQuota;import com.mpush.monitor.service.ThreadPoolManager;import io.netty.channel.EventLoopGroup;import java.util.HashMap;import java.util.Map;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;import static com.mpush.tools.Utils.getPoolInfo;public class JVMThreadPool implements ThreadPoolQuota &#123; private final ThreadPoolManager threadPoolManager; public JVMThreadPool(ThreadPoolManager threadPoolManager) &#123; this.threadPoolManager = threadPoolManager; &#125; @Override public Object monitor(Object... args) &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(); Map&lt;String, Executor&gt; pools = threadPoolManager.getActivePools(); for (Map.Entry&lt;String, Executor&gt; entry : pools.entrySet()) &#123; String serviceName = entry.getKey(); Executor executor = entry.getValue(); if (executor instanceof ThreadPoolExecutor) &#123; result.put(serviceName, getPoolInfo((ThreadPoolExecutor) executor)); &#125; else if (executor instanceof EventLoopGroup) &#123; result.put(serviceName, getPoolInfo((EventLoopGroup) executor)); &#125; &#125; return result; &#125;&#125; 功能组件文章目录: CC-配置中心 EventBus-事件总线 FlowControl-流控 JVMUtil Logs Profiler-统计方法或者线程执行时间 Profiler入门 SPI机制 TimeLine-时间线 服务启动监听 监控 通信模型与超时控制 线程池 状态判断-位运算]]></content>
      <categories>
        <category>源码学习</category>
        <category>MPush</category>
        <category>源码分析</category>
        <category>功能组件</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>消息推送</tag>
      </tags>
  </entry>
</search>
